{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1045872c-9dcf-4e8d-bc55-b6dee2b0e2b1",
   "metadata": {},
   "source": [
    "# 📄 Smart CLM 문서 처리 파이프라인\n",
    "\n",
    "Smart CLM 시스템의 문서 처리 및 RAG 평가를 위한 통합 노트북입니다.\n",
    "\n",
    "## 🏗️ 시스템 구성\n",
    "- **문서 처리 엔진**: PDF 문서 분석 및 구조화\n",
    "- **로컬 스토리지**: 파일 시스템 기반 데이터 저장\n",
    "- **벡터 저장소**: 로컬 파일 기반 벡터 데이터 관리\n",
    "- **Jupyter Notebook**: 문서 처리 및 평가 도구\n",
    "\n",
    "## 📋 주요 기능\n",
    "1. **docs 폴더 스캔**: PDF 파일 자동 탐색\n",
    "2. **문서 분석**: 내장 PDF 처리 엔진으로 분석\n",
    "3. **로컬 저장**: 파일 시스템에 결과 저장\n",
    "4. **RAG 평가**: 문서 처리 품질 및 검색 성능 평가\n",
    "5. **결과 시각화**: 처리 결과 및 성능 지표 대시보드\n",
    "\n",
    "## 🚀 사용 방법\n",
    "1. `docs/` 폴더에 처리할 PDF 파일 배치\n",
    "2. 노트북 셀 순서대로 실행\n",
    "3. 결과 확인 및 분석\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e21772a-49a2-46e7-a331-d65ffda5db93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T08:19:38.671478Z",
     "iopub.status.busy": "2025-07-14T08:19:38.671091Z",
     "iopub.status.idle": "2025-07-14T08:19:54.064772Z",
     "shell.execute_reply": "2025-07-14T08:19:54.063968Z",
     "shell.execute_reply.started": "2025-07-14T08:19:38.671445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ragas\n",
      "  Using cached ragas-0.2.15-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting docling\n",
      "  Using cached docling-2.41.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting streamlit\n",
      "  Using cached streamlit-1.46.1-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from ragas) (1.26.4)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.12/site-packages (from ragas) (2.2.1)\n",
      "Collecting tiktoken (from ragas)\n",
      "  Using cached tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.12/site-packages (from ragas) (0.3.25)\n",
      "Requirement already satisfied: langchain-core in /opt/conda/lib/python3.12/site-packages (from ragas) (0.3.63)\n",
      "Requirement already satisfied: langchain-community in /opt/conda/lib/python3.12/site-packages (from ragas) (0.3.24)\n",
      "Collecting langchain_openai (from ragas)\n",
      "  Using cached langchain_openai-0.3.27-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.12/site-packages (from ragas) (1.6.0)\n",
      "Requirement already satisfied: appdirs in /opt/conda/lib/python3.12/site-packages (from ragas) (1.4.4)\n",
      "Requirement already satisfied: pydantic>=2 in /opt/conda/lib/python3.12/site-packages (from ragas) (2.11.4)\n",
      "Collecting openai>1 (from ragas)\n",
      "  Using cached openai-1.95.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: diskcache>=5.6.3 in /opt/conda/lib/python3.12/site-packages (from ragas) (5.6.3)\n",
      "Collecting docling-core<3.0.0,>=2.42.0 (from docling-core[chunking]<3.0.0,>=2.42.0->docling)\n",
      "  Using cached docling_core-2.42.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting docling-parse<5.0.0,>=4.0.0 (from docling)\n",
      "  Using cached docling_parse-4.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting docling-ibm-models<4,>=3.6.0 (from docling)\n",
      "  Using cached docling_ibm_models-3.8.1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from docling)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting pypdfium2<5.0.0,>=4.30.0 (from docling)\n",
      "  Using cached pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.3.0 in /opt/conda/lib/python3.12/site-packages (from docling) (2.9.1)\n",
      "Requirement already satisfied: huggingface_hub<1,>=0.23 in /opt/conda/lib/python3.12/site-packages (from docling) (0.32.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.2 in /opt/conda/lib/python3.12/site-packages (from docling) (2.32.3)\n",
      "Collecting easyocr<2.0,>=1.7 (from docling)\n",
      "  Using cached easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in /opt/conda/lib/python3.12/site-packages (from docling) (2025.4.26)\n",
      "Collecting rtree<2.0.0,>=1.3.0 (from docling)\n",
      "  Using cached rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typer<0.17.0,>=0.12.5 in /opt/conda/lib/python3.12/site-packages (from docling) (0.16.0)\n",
      "Collecting python-docx<2.0.0,>=1.1.2 (from docling)\n",
      "  Using cached python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting python-pptx<2.0.0,>=1.0.2 (from docling)\n",
      "  Using cached python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/conda/lib/python3.12/site-packages (from docling) (4.13.4)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.1.4 in /opt/conda/lib/python3.12/site-packages (from docling) (2.2.3)\n",
      "Collecting marko<3.0.0,>=2.1.2 (from docling)\n",
      "  Using cached marko-2.1.4-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting openpyxl<4.0.0,>=3.1.5 (from docling)\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: lxml<6.0.0,>=4.0.0 in /opt/conda/lib/python3.12/site-packages (from docling) (5.4.0)\n",
      "Requirement already satisfied: pillow<12.0.0,>=10.0.0 in /opt/conda/lib/python3.12/site-packages (from docling) (11.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.65.0 in /opt/conda/lib/python3.12/site-packages (from docling) (4.67.1)\n",
      "Requirement already satisfied: pluggy<2.0.0,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from docling) (1.6.0)\n",
      "Collecting pylatexenc<3.0,>=2.10 (from docling)\n",
      "  Using cached pylatexenc-2.10-py3-none-any.whl\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from docling) (1.15.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.12/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.12/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (4.13.2)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in /opt/conda/lib/python3.12/site-packages (from docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (4.23.0)\n",
      "Collecting jsonref<2.0.0,>=1.1.0 (from docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling)\n",
      "  Using cached jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /opt/conda/lib/python3.12/site-packages (from docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (0.9.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.1 in /opt/conda/lib/python3.12/site-packages (from docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (6.0.2)\n",
      "Collecting latex2mathml<4.0.0,>=3.77.0 (from docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling)\n",
      "  Using cached latex2mathml-3.78.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting semchunk<3.0.0,>=2.2.0 (from docling-core[chunking]<3.0.0,>=2.42.0->docling)\n",
      "  Using cached semchunk-2.2.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/conda/lib/python3.12/site-packages (from docling-core[chunking]<3.0.0,>=2.42.0->docling) (4.52.4)\n",
      "Requirement already satisfied: torch<3.0.0,>=2.2.2 in /opt/conda/lib/python3.12/site-packages (from docling-ibm-models<4,>=3.6.0->docling) (2.6.0)\n",
      "Requirement already satisfied: torchvision<1,>=0 in /opt/conda/lib/python3.12/site-packages (from docling-ibm-models<4,>=3.6.0->docling) (0.21.0)\n",
      "Collecting jsonlines<4.0.0,>=3.1.0 (from docling-ibm-models<4,>=3.6.0->docling)\n",
      "  Using cached jsonlines-3.1.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting opencv-python-headless<5.0.0.0,>=4.6.0.66 (from docling-ibm-models<4,>=3.6.0->docling)\n",
      "  Using cached opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: safetensors<1,>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from safetensors[torch]<1,>=0.4.3->docling-ibm-models<4,>=3.6.0->docling) (0.5.3)\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.12/site-packages (from easyocr<2.0,>=1.7->docling) (0.25.2)\n",
      "Collecting python-bidi (from easyocr<2.0,>=1.7->docling)\n",
      "  Using cached python_bidi-0.6.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting Shapely (from easyocr<2.0,>=1.7->docling)\n",
      "  Using cached shapely-2.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting pyclipper (from easyocr<2.0,>=1.7->docling)\n",
      "  Using cached pyclipper-1.3.0.post6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting ninja (from easyocr<2.0,>=1.7->docling)\n",
      "  Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface_hub<1,>=0.23->docling) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub<1,>=0.23->docling) (2024.12.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub<1,>=0.23->docling) (1.1.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub<1,>=0.23->docling) (24.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.12/site-packages (from jsonlines<4.0.0,>=3.1.0->docling-ibm-models<4,>=3.6.0->docling) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (0.25.1)\n",
      "Collecting numpy (from ragas)\n",
      "  Using cached numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting et-xmlfile (from openpyxl<4.0.0,>=3.1.5->docling)\n",
      "  Using cached et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas<3.0.0,>=2.1.4->docling) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas<3.0.0,>=2.1.4->docling) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic>=2->ragas) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.12/site-packages (from pydantic>=2->ragas) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from pydantic>=2->ragas) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/conda/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.3.0->docling) (1.1.0)\n",
      "Collecting XlsxWriter>=0.5.7 (from python-pptx<2.0.0,>=1.0.2->docling)\n",
      "  Using cached xlsxwriter-3.2.5-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.2->docling) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.2->docling) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.2->docling) (1.26.19)\n",
      "Collecting mpire[dill] (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.42.0->docling)\n",
      "  Using cached mpire-2.10.2-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4,>=3.6.0->docling) (80.8.0)\n",
      "Requirement already satisfied: sympy!=1.13.2,>=1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4,>=3.6.0->docling) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4,>=3.6.0->docling) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4,>=3.6.0->docling) (3.1.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (0.21.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.12/site-packages (from typer<0.17.0,>=0.12.5->docling) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.12/site-packages (from typer<0.17.0,>=0.12.5->docling) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.12/site-packages (from typer<0.17.0,>=0.12.5->docling) (14.0.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /opt/conda/lib/python3.12/site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in /opt/conda/lib/python3.12/site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in /opt/conda/lib/python3.12/site-packages (from streamlit) (5.5.2)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in /opt/conda/lib/python3.12/site-packages (from streamlit) (5.28.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /opt/conda/lib/python3.12/site-packages (from streamlit) (19.0.1)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /opt/conda/lib/python3.12/site-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /opt/conda/lib/python3.12/site-packages (from streamlit) (0.10.2)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
      "  Using cached watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/conda/lib/python3.12/site-packages (from streamlit) (3.1.44)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /opt/conda/lib/python3.12/site-packages (from streamlit) (6.5.1)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /opt/conda/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit) (1.41.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.12/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch<3.0.0,>=2.2.2->docling-ibm-models<4,>=3.6.0->docling) (3.0.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.12/site-packages (from openai>1->ragas) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.12/site-packages (from openai>1->ragas) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from openai>1->ragas) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>1->ragas)\n",
      "  Using cached jiter-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.12/site-packages (from openai>1->ragas) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas) (0.16.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.1.4->docling) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich>=10.11.0->typer<0.17.0,>=0.12.5->docling) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich>=10.11.0->typer<0.17.0,>=0.12.5->docling) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.17.0,>=0.12.5->docling) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy!=1.13.2,>=1.13.1->torch<3.0.0,>=2.2.2->docling-ibm-models<4,>=3.6.0->docling) (1.3.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.12/site-packages (from datasets->ragas) (0.4.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.12/site-packages (from datasets->ragas) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.12/site-packages (from datasets->ragas) (0.70.18)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.12/site-packages (from datasets->ragas) (3.9.5)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.12/site-packages (from datasets->ragas) (0.18.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets->ragas) (1.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets->ragas) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets->ragas) (6.4.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets->ragas) (1.20.0)\n",
      "Requirement already satisfied: propcache>=0.2.1 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.0->aiohttp->datasets->ragas) (0.3.1)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/conda/lib/python3.12/site-packages (from langchain->ragas) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /opt/conda/lib/python3.12/site-packages (from langchain->ragas) (0.2.11)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.12/site-packages (from langchain->ragas) (2.0.41)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.12/site-packages (from langchain-core->ragas) (1.33)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain->ragas) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain->ragas) (1.0.0)\n",
      "Requirement already satisfied: greenlet>=1 in /opt/conda/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain->ragas) (3.2.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.12/site-packages (from langchain-community->ragas) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from langchain-community->ragas) (0.4.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (0.9.0)\n",
      "Requirement already satisfied: mypy_extensions>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (1.1.0)\n",
      "Collecting langchain-core (from ragas)\n",
      "  Using cached langchain_core-0.3.68-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain->ragas)\n",
      "  Using cached langsmith-0.3.45-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain->ragas) (0.23.0)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /opt/conda/lib/python3.12/site-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.12/site-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (2025.5.26)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /opt/conda/lib/python3.12/site-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (0.4)\n",
      "Using cached ragas-0.2.15-py3-none-any.whl (190 kB)\n",
      "Using cached docling-2.41.0-py3-none-any.whl (187 kB)\n",
      "Using cached docling_core-2.42.0-py3-none-any.whl (158 kB)\n",
      "Using cached docling_ibm_models-3.8.1-py3-none-any.whl (86 kB)\n",
      "Using cached docling_parse-4.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.1 MB)\n",
      "Using cached easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
      "Using cached jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
      "Using cached latex2mathml-3.78.0-py3-none-any.whl (73 kB)\n",
      "Using cached marko-2.1.4-py3-none-any.whl (42 kB)\n",
      "Using cached opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (54.0 MB)\n",
      "Using cached numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
      "Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Using cached pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "Using cached python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
      "Using cached python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
      "Using cached rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (541 kB)\n",
      "Using cached semchunk-2.2.2-py3-none-any.whl (10 kB)\n",
      "Using cached streamlit-1.46.1-py3-none-any.whl (10.1 MB)\n",
      "Using cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "Using cached watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
      "Using cached openai-1.95.1-py3-none-any.whl (755 kB)\n",
      "Using cached jiter-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Using cached xlsxwriter-3.2.5-py3-none-any.whl (172 kB)\n",
      "Using cached et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Using cached langchain_openai-0.3.27-py3-none-any.whl (70 kB)\n",
      "Using cached langchain_core-0.3.68-py3-none-any.whl (441 kB)\n",
      "Using cached langsmith-0.3.45-py3-none-any.whl (363 kB)\n",
      "Using cached tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached mpire-2.10.2-py3-none-any.whl (272 kB)\n",
      "Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "Using cached pyclipper-1.3.0.post6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (963 kB)\n",
      "Using cached python_bidi-0.6.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (292 kB)\n",
      "Using cached shapely-2.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Installing collected packages: python-bidi, pylatexenc, pyclipper, filetype, XlsxWriter, watchdog, rtree, python-docx, pypdfium2, numpy, ninja, mpire, marko, latex2mathml, jsonref, jsonlines, jiter, et-xmlfile, tiktoken, Shapely, python-pptx, pydeck, openpyxl, opencv-python-headless, semchunk, openai, langsmith, langchain-core, easyocr, docling-core, streamlit, langchain_openai, docling-parse, docling-ibm-models, docling, ragas\n",
      "\u001b[2K  Attempting uninstall: numpym\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/36\u001b[0m [pypdfium2]x]\n",
      "\u001b[2K    Found existing installation: numpy 1.26.4━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/36\u001b[0m [pypdfium2]\n",
      "\u001b[2K    Uninstalling numpy-1.26.4:m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/36\u001b[0m [pypdfium2]\n",
      "\u001b[2K      Successfully uninstalled numpy-1.26.4━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/36\u001b[0m [pypdfium2]\n",
      "\u001b[2K  Attempting uninstall: langsmith━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m25/36\u001b[0m [openai]python-headless]\n",
      "\u001b[2K    Found existing installation: langsmith 0.2.11m━━━━━━━━━━━━\u001b[0m \u001b[32m25/36\u001b[0m [openai]\n",
      "\u001b[2K    Uninstalling langsmith-0.2.11:m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m25/36\u001b[0m [openai]\n",
      "\u001b[2K      Successfully uninstalled langsmith-0.2.1190m━━━━━━━━━━━━\u001b[0m \u001b[32m25/36\u001b[0m [openai]\n",
      "\u001b[2K  Attempting uninstall: langchain-core0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m26/36\u001b[0m [langsmith]\n",
      "\u001b[2K    Found existing installation: langchain-core 0.3.63━━━━━━━━\u001b[0m \u001b[32m26/36\u001b[0m [langsmith]\n",
      "\u001b[2K    Uninstalling langchain-core-0.3.63:1m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m26/36\u001b[0m [langsmith]\n",
      "\u001b[2K      Successfully uninstalled langchain-core-0.3.63━━━━━━━━━━\u001b[0m \u001b[32m26/36\u001b[0m [langsmith]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36/36\u001b[0m [ragas]m35/36\u001b[0m [ragas]g]parse]]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 1.3.1 requires nvidia-ml-py3<8.0,>=7.352.0, which is not installed.\n",
      "amazon-sagemaker-jupyter-ai-q-developer 1.2.4 requires numpy<=2.0.1, but you have numpy 2.2.6 which is incompatible.\n",
      "amazon-sagemaker-sql-magic 0.1.4 requires numpy<2, but you have numpy 2.2.6 which is incompatible.\n",
      "autogluon-multimodal 1.3.1 requires transformers[sentencepiece]<4.50,>=4.38.0, but you have transformers 4.52.4 which is incompatible.\n",
      "autogluon-timeseries 1.3.1 requires transformers[sentencepiece]<4.50,>=4.38.0, but you have transformers 4.52.4 which is incompatible.\n",
      "catboost 1.2.7 requires numpy<2.0,>=1.16.0, but you have numpy 2.2.6 which is incompatible.\n",
      "gluonts 0.16.1 requires numpy<2.2,>=1.16, but you have numpy 2.2.6 which is incompatible.\n",
      "langchain-aws 0.2.19 requires boto3>=1.37.24, but you have boto3 1.37.1 which is incompatible.\n",
      "sagemaker 2.245.0 requires numpy==1.26.4, but you have numpy 2.2.6 which is incompatible.\n",
      "spacy 3.8.6 requires thinc<8.4.0,>=8.3.4, but you have thinc 8.3.2 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Shapely-2.1.1 XlsxWriter-3.2.5 docling-2.41.0 docling-core-2.42.0 docling-ibm-models-3.8.1 docling-parse-4.1.0 easyocr-1.7.2 et-xmlfile-2.0.0 filetype-1.2.0 jiter-0.10.0 jsonlines-3.1.0 jsonref-1.1.0 langchain-core-0.3.68 langchain_openai-0.3.27 langsmith-0.3.45 latex2mathml-3.78.0 marko-2.1.4 mpire-2.10.2 ninja-1.11.1.4 numpy-2.2.6 openai-1.95.1 opencv-python-headless-4.12.0.88 openpyxl-3.1.5 pyclipper-1.3.0.post6 pydeck-0.9.1 pylatexenc-2.10 pypdfium2-4.30.1 python-bidi-0.6.6 python-docx-1.2.0 python-pptx-1.0.2 ragas-0.2.15 rtree-1.4.0 semchunk-2.2.2 streamlit-1.46.1 tiktoken-0.9.0 watchdog-6.0.0\n"
     ]
    }
   ],
   "source": [
    "# 0. 의존성 설치\n",
    "!pip install ragas docling streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbef1828-5feb-471b-977d-a0348667e76d",
   "metadata": {},
   "source": [
    "## 🛠️ 1. 환경 설정 및 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ca8cdc8-de93-4b98-8970-14bb46cee581",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T00:39:45.437371Z",
     "iopub.status.busy": "2025-07-14T00:39:45.437078Z",
     "iopub.status.idle": "2025-07-14T00:39:47.166637Z",
     "shell.execute_reply": "2025-07-14T00:39:47.165781Z",
     "shell.execute_reply.started": "2025-07-14T00:39:45.437345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 환경 설정 완료\n",
      "📁 문서 폴더: docs\n",
      "📊 결과 폴더: results\n",
      "💾 처리 폴더: processed\n",
      "🔢 벡터 폴더: vectors\n",
      "📑 청크 폴더: chunks\n",
      "\n",
      "📄 docs 폴더 내 PDF 파일: 0개\n"
     ]
    }
   ],
   "source": [
    "# 필수 라이브러리 import\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 시각화 스타일 설정\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# 로깅 설정\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 기본 디렉토리 설정\n",
    "ROOT_DIR = Path(\".\")\n",
    "DOCS_DIR = ROOT_DIR / \"docs\"\n",
    "RESULTS_DIR = ROOT_DIR / \"results\"\n",
    "PROCESSED_DIR = ROOT_DIR / \"processed\"\n",
    "VECTORS_DIR = ROOT_DIR / \"vectors\"\n",
    "CHUNKS_DIR = ROOT_DIR / \"chunks\"\n",
    "\n",
    "# 디렉토리 생성\n",
    "for dir_path in [DOCS_DIR, RESULTS_DIR, PROCESSED_DIR, VECTORS_DIR, CHUNKS_DIR]:\n",
    "    dir_path.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"✅ 환경 설정 완료\")\n",
    "print(f\"📁 문서 폴더: {DOCS_DIR}\")\n",
    "print(f\"📊 결과 폴더: {RESULTS_DIR}\")\n",
    "print(f\"💾 처리 폴더: {PROCESSED_DIR}\")\n",
    "print(f\"🔢 벡터 폴더: {VECTORS_DIR}\")\n",
    "print(f\"📑 청크 폴더: {CHUNKS_DIR}\")\n",
    "\n",
    "# docs 폴더 상태 확인\n",
    "pdf_files = list(DOCS_DIR.glob(\"*.pdf\"))\n",
    "print(f\"\\n📄 docs 폴더 내 PDF 파일: {len(pdf_files)}개\")\n",
    "for pdf_file in pdf_files:\n",
    "    file_size = pdf_file.stat().st_size\n",
    "    print(f\"  - {pdf_file.name}: {file_size:,} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65872c1-02ad-4c53-97c4-636ba77c3af6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T03:45:24.411303Z",
     "iopub.status.busy": "2025-07-11T03:45:24.410916Z",
     "iopub.status.idle": "2025-07-11T03:45:24.416966Z",
     "shell.execute_reply": "2025-07-11T03:45:24.416144Z",
     "shell.execute_reply.started": "2025-07-11T03:45:24.411276Z"
    }
   },
   "source": [
    "## 📄 2. PDF 처리 엔진 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979898fc-e3ed-4d9d-9ba5-0402e82fec88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T00:39:51.959057Z",
     "iopub.status.busy": "2025-07-14T00:39:51.958418Z",
     "iopub.status.idle": "2025-07-14T00:39:51.988677Z",
     "shell.execute_reply": "2025-07-14T00:39:51.988033Z",
     "shell.execute_reply.started": "2025-07-14T00:39:51.959026Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:🔍 OCR 활성화\n",
      "INFO:__main__:📊 테이블 구조 인식 활성화\n",
      "INFO:__main__:🖼️ 이미지 처리 활성화\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 문서 처리기가 준비되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# docling 라이브러리 import\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "class DocumentProcessingConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        use_ocr: bool = True,\n",
    "        extract_tables: bool = True,\n",
    "        extract_images: bool = True,\n",
    "        extract_outline: bool = True,\n",
    "        min_chunk_size: int = 100,\n",
    "        max_chunk_size: int = 1000,\n",
    "        overlap_size: int = 50,\n",
    "        language: str = \"kor\",\n",
    "    ):\n",
    "        self.use_ocr = use_ocr\n",
    "        self.extract_tables = extract_tables\n",
    "        self.extract_images = extract_images\n",
    "        self.extract_outline = extract_outline\n",
    "        self.min_chunk_size = min_chunk_size\n",
    "        self.max_chunk_size = max_chunk_size\n",
    "        self.overlap_size = overlap_size\n",
    "        self.language = language\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            \"use_ocr\": self.use_ocr,\n",
    "            \"extract_tables\": self.extract_tables,\n",
    "            \"extract_images\": self.extract_images,\n",
    "            \"extract_outline\": self.extract_outline,\n",
    "            \"min_chunk_size\": self.min_chunk_size,\n",
    "            \"max_chunk_size\": self.max_chunk_size,\n",
    "            \"overlap_size\": self.overlap_size,\n",
    "            \"language\": self.language,\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, config_dict: Dict) -> \"DocumentProcessingConfig\":\n",
    "        return cls(**config_dict)\n",
    "\n",
    "def quick_layout_analysis(doc_analysis) -> dict:\n",
    "    \"\"\"\n",
    "    빠른 레이아웃 분석으로 문서 구조 파악\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 문서 특성 분석\n",
    "        text_content = doc_analysis.export_to_text()\n",
    "        text_length = len(text_content.strip())\n",
    "\n",
    "        analysis = {\n",
    "            \"has_tables\": len(doc_analysis.tables) > 0,\n",
    "            \"table_count\": len(doc_analysis.tables),\n",
    "            \"has_images\": len(doc_analysis.pictures) > 0,\n",
    "            \"image_count\": len(doc_analysis.pictures),\n",
    "            \"is_scanned\": text_length < 100,  # 텍스트가 거의 없으면 스캔 문서\n",
    "            \"text_length\": text_length,\n",
    "            \"is_text_heavy\": text_length > 5000,  # 텍스트 위주 문서\n",
    "            \"page_count\": len(doc_analysis.pages),\n",
    "        }\n",
    "\n",
    "        logger.info(\"📊 분석 결과:\")\n",
    "        logger.info(f\"  - 페이지: {analysis['page_count']}페이지\")\n",
    "        logger.info(\n",
    "            f\"  - 텍스트: {analysis['text_length']}자 ({'스캔' if analysis['is_scanned'] else '디지털'})\"\n",
    "        )\n",
    "        logger.info(f\"  - 테이블: {analysis['table_count']}개\")\n",
    "        logger.info(f\"  - 이미지: {analysis['image_count']}개\")\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"레이아웃 분석 실패: {str(e)}\")\n",
    "        return {\n",
    "            \"has_tables\": True,  # 안전을 위해 True\n",
    "            \"table_count\": 0,\n",
    "            \"has_images\": True,  # 안전을 위해 True\n",
    "            \"image_count\": 0,\n",
    "            \"is_scanned\": False,\n",
    "            \"text_length\": 0,\n",
    "            \"is_text_heavy\": False,\n",
    "            \"page_count\": 1,\n",
    "        }\n",
    "\n",
    "def create_optimized_pipeline(config: DocumentProcessingConfig) -> PdfPipelineOptions:\n",
    "    \"\"\"\n",
    "    설정에 따라 최적의 파이프라인 옵션 생성\n",
    "    \"\"\"\n",
    "    options = PdfPipelineOptions()\n",
    "\n",
    "    # OCR 설정\n",
    "    options.do_ocr = config.use_ocr\n",
    "    if config.use_ocr:\n",
    "        options.ocr_options.lang = [\"ko\", \"en\"]\n",
    "        logger.info(\"🔍 OCR 활성화\")\n",
    "    else:\n",
    "        logger.info(\"📄 OCR 비활성화\")\n",
    "\n",
    "    # 테이블 처리 설정\n",
    "    options.do_table_structure = config.extract_tables\n",
    "    if config.extract_tables:\n",
    "        options.table_structure_options.do_cell_matching = True\n",
    "        logger.info(\"📊 테이블 구조 인식 활성화\")\n",
    "    else:\n",
    "        logger.info(\"📝 테이블 구조 인식 비활성화\")\n",
    "\n",
    "    # 이미지 처리 설정\n",
    "    options.generate_picture_images = config.extract_images\n",
    "    if config.extract_images:\n",
    "        options.images_scale = 2.0\n",
    "        logger.info(\"🖼️ 이미지 처리 활성화\")\n",
    "    else:\n",
    "        logger.info(\"📝 이미지 처리 비활성화\")\n",
    "\n",
    "    # 페이지 이미지는 기본적으로 비활성화 (용량 절약)\n",
    "    options.generate_page_images = False\n",
    "\n",
    "    return options\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, config: DocumentProcessingConfig):\n",
    "        self.config = config\n",
    "        self.pipeline = create_optimized_pipeline(config)\n",
    "        self.converter = DocumentConverter(\n",
    "            format_options={\n",
    "                InputFormat.PDF: PdfFormatOption(pipeline_options=self.pipeline)\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def process_document(self, pdf_path: Path) -> Dict:\n",
    "        \"\"\"문서를 처리하고 결과를 반환합니다.\"\"\"\n",
    "        results = {\n",
    "            \"metadata\": {},\n",
    "            \"tables\": [],\n",
    "            \"images\": [],\n",
    "            \"outline\": [],\n",
    "            \"chunks\": [],\n",
    "            \"errors\": []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # 1. PDF 파일 처리\n",
    "            start_time = time.time()\n",
    "            doc_analysis = self.converter.convert(str(pdf_path)).document\n",
    "            \n",
    "            # 2. 레이아웃 분석\n",
    "            layout_info = quick_layout_analysis(doc_analysis)\n",
    "            results[\"metadata\"][\"layout\"] = layout_info\n",
    "            \n",
    "            # 3. 테이블 추출\n",
    "            if self.config.extract_tables:\n",
    "                tables = self._extract_tables(doc_analysis)\n",
    "                results[\"tables\"] = tables\n",
    "            \n",
    "            # 4. 이미지 추출\n",
    "            if self.config.extract_images:\n",
    "                images = self._extract_images(doc_analysis)\n",
    "                results[\"images\"] = images\n",
    "            \n",
    "            # 5. 아웃라인 추출\n",
    "            if self.config.extract_outline:\n",
    "                outline = self._extract_outline(doc_analysis)\n",
    "                results[\"outline\"] = outline\n",
    "            \n",
    "            # 6. 청크 생성\n",
    "            chunks = self._create_chunks(doc_analysis)\n",
    "            results[\"chunks\"] = chunks\n",
    "            \n",
    "            # 7. 메타데이터 추가\n",
    "            processing_time = time.time() - start_time\n",
    "            results[\"metadata\"].update({\n",
    "                \"filename\": pdf_path.name,\n",
    "                \"page_count\": len(doc_analysis.pages),\n",
    "                \"processing_time\": f\"{processing_time:.2f}초\",\n",
    "                \"processing_config\": self.config.to_dict()\n",
    "            })\n",
    "            \n",
    "            logger.info(f\"✅ 문서 처리 완료 ({processing_time:.2f}초)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"문서 처리 실패: {str(e)}\"\n",
    "            logger.error(error_msg)\n",
    "            results[\"errors\"].append(error_msg)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _extract_tables(self, doc_analysis) -> List[Dict]:\n",
    "        \"\"\"테이블 정보를 추출합니다.\"\"\"\n",
    "        tables = []\n",
    "        for page_idx, page in enumerate(doc_analysis.pages):\n",
    "            page_tables = page.find_tables()\n",
    "            for table_idx, table in enumerate(page_tables):\n",
    "                table_info = {\n",
    "                    \"page\": page_idx + 1,\n",
    "                    \"table_idx\": table_idx,\n",
    "                    \"content\": table.extract(),\n",
    "                    \"bbox\": table.bbox.to_dict()\n",
    "                }\n",
    "                tables.append(table_info)\n",
    "        return tables\n",
    "    \n",
    "    def _extract_images(self, doc_analysis) -> List[Dict]:\n",
    "        \"\"\"이미지 정보를 추출합니다.\"\"\"\n",
    "        images = []\n",
    "        for page_idx, page in enumerate(doc_analysis.pages):\n",
    "            page_images = page.find_images()\n",
    "            for img_idx, img in enumerate(page_images):\n",
    "                img_info = {\n",
    "                    \"page\": page_idx + 1,\n",
    "                    \"image_idx\": img_idx,\n",
    "                    \"bbox\": img.bbox.to_dict()\n",
    "                }\n",
    "                images.append(img_info)\n",
    "        return images\n",
    "    \n",
    "    def _extract_outline(self, doc_analysis) -> List[Dict]:\n",
    "        \"\"\"문서 아웃라인을 추출합니다.\"\"\"\n",
    "        outline = []\n",
    "        if hasattr(doc_analysis, \"outline\"):\n",
    "            for item in doc_analysis.outline:\n",
    "                outline_item = {\n",
    "                    \"title\": item.title,\n",
    "                    \"level\": item.level,\n",
    "                    \"page\": item.page\n",
    "                }\n",
    "                outline.append(outline_item)\n",
    "        return outline\n",
    "    \n",
    "    def _create_chunks(self, doc_analysis) -> List[Dict]:\n",
    "        \"\"\"텍스트를 청크로 분할합니다.\"\"\"\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        current_page = 1\n",
    "        \n",
    "        for page_idx, page in enumerate(doc_analysis.pages):\n",
    "            page_text = page.extract_text()\n",
    "            words = page_text.split()\n",
    "            \n",
    "            for word in words:\n",
    "                if len(current_chunk) + len(word) + 1 <= self.config.max_chunk_size:\n",
    "                    current_chunk += word + \" \"\n",
    "                else:\n",
    "                    if len(current_chunk) >= self.config.min_chunk_size:\n",
    "                        chunk_info = {\n",
    "                            \"text\": current_chunk.strip(),\n",
    "                            \"page\": current_page,\n",
    "                            \"size\": len(current_chunk)\n",
    "                        }\n",
    "                        chunks.append(chunk_info)\n",
    "                    \n",
    "                    current_chunk = word + \" \"\n",
    "                    current_page = page_idx + 1\n",
    "            \n",
    "            # 페이지 끝에서 청크 저장\n",
    "            if len(current_chunk) >= self.config.min_chunk_size:\n",
    "                chunk_info = {\n",
    "                    \"text\": current_chunk.strip(),\n",
    "                    \"page\": current_page,\n",
    "                    \"size\": len(current_chunk)\n",
    "                }\n",
    "                chunks.append(chunk_info)\n",
    "                current_chunk = \"\"\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# 기본 설정으로 프로세서 생성\n",
    "default_config = DocumentProcessingConfig()\n",
    "processor = DocumentProcessor(default_config)\n",
    "print(\"✅ 문서 처리기가 준비되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "286a5d0d-a1e2-4901-ae38-97028c5edaf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T00:39:53.863580Z",
     "iopub.status.busy": "2025-07-14T00:39:53.863253Z",
     "iopub.status.idle": "2025-07-14T00:39:53.871161Z",
     "shell.execute_reply": "2025-07-14T00:39:53.870136Z",
     "shell.execute_reply.started": "2025-07-14T00:39:53.863552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "문서 처리를 시작하려면 다음과 같이 실행하세요:\n",
      "results = process_documents(config=custom_config)\n",
      "또는 기본 설정으로 실행:\n",
      "results = process_documents()\n"
     ]
    }
   ],
   "source": [
    "def process_documents(\n",
    "    config: Optional[Dict] = None,\n",
    "    input_dir: Optional[Path] = None,\n",
    "    output_dir: Optional[Path] = None\n",
    ") -> List[Dict]:\n",
    "    \"\"\"여러 문서를 일괄 처리합니다.\"\"\"\n",
    "    # 설정 로드\n",
    "    if config is None:\n",
    "        processing_config = default_config\n",
    "    else:\n",
    "        processing_config = DocumentProcessingConfig.from_dict(config)\n",
    "    \n",
    "    # 입/출력 경로 설정\n",
    "    input_dir = input_dir or DOCS_DIR\n",
    "    output_dir = output_dir or PROCESSED_DIR\n",
    "    \n",
    "    # PDF 파일 목록 가져오기\n",
    "    pdf_files = list(input_dir.glob(\"*.pdf\"))\n",
    "    print(f\"총 {len(pdf_files)}개의 PDF 파일을 처리합니다.\")\n",
    "    \n",
    "    # 결과 저장용 리스트\n",
    "    results = []\n",
    "    \n",
    "    # 각 파일 처리\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\n{pdf_file.name} 처리 중...\")\n",
    "        \n",
    "        try:\n",
    "            # 문서 처리\n",
    "            result = processor.process_document(pdf_file)\n",
    "            \n",
    "            # 결과 저장\n",
    "            output_file = output_dir / f\"{pdf_file.stem}_processed.json\"\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            print(f\"처리 완료: {output_file}\")\n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"처리 실패: {str(e)}\")\n",
    "            results.append({\n",
    "                \"filename\": pdf_file.name,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 사용 예시\n",
    "custom_config = {\n",
    "    \"use_ocr\": True,\n",
    "    \"extract_tables\": True,\n",
    "    \"extract_images\": True,\n",
    "    \"extract_outline\": True,\n",
    "    \"min_chunk_size\": 200,\n",
    "    \"max_chunk_size\": 800,\n",
    "    \"overlap_size\": 100,\n",
    "    \"language\": \"kor\"\n",
    "}\n",
    "\n",
    "print(\"\\n문서 처리를 시작하려면 다음과 같이 실행하세요:\")\n",
    "print(\"results = process_documents(config=custom_config)\")\n",
    "print(\"또는 기본 설정으로 실행:\")\n",
    "print(\"results = process_documents()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b66e05f2-a17f-4303-9557-352adf200d85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T00:39:57.440119Z",
     "iopub.status.busy": "2025-07-14T00:39:57.439842Z",
     "iopub.status.idle": "2025-07-14T00:39:57.455791Z",
     "shell.execute_reply": "2025-07-14T00:39:57.455095Z",
     "shell.execute_reply.started": "2025-07-14T00:39:57.440098Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_tables_info(document) -> Dict[str, Any]:\n",
    "    \"\"\"문서에서 테이블 정보를 추출합니다.\"\"\"\n",
    "    tables_info = []\n",
    "\n",
    "    if hasattr(document, \"tables\") and document.tables:\n",
    "        for idx, table in enumerate(document.tables):\n",
    "            try:\n",
    "                table_data = {\n",
    "                    \"table_index\": idx + 1,\n",
    "                    \"rows\": getattr(table, \"num_rows\", 0),\n",
    "                    \"cols\": getattr(table, \"num_cols\", 0),\n",
    "                    \"markdown\": table.export_to_markdown()\n",
    "                    if hasattr(table, \"export_to_markdown\")\n",
    "                    else \"\",\n",
    "                    \"html\": table.export_to_html()\n",
    "                    if hasattr(table, \"export_to_html\")\n",
    "                    else \"\",\n",
    "                }\n",
    "                # 페이지 번호 추출\n",
    "                if hasattr(table, \"prov\") and table.prov:\n",
    "                    for prov in table.prov:\n",
    "                        if hasattr(prov, \"page_no\"):\n",
    "                            table_data[\"page\"] = prov.page_no\n",
    "                            break\n",
    "                tables_info.append(table_data)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"테이블 {idx} 처리 오류: {e}\")\n",
    "\n",
    "    return {\"count\": len(tables_info), \"data\": tables_info}\n",
    "\n",
    "\n",
    "def extract_images_info(document) -> Dict[str, Any]:\n",
    "    \"\"\"문서에서 이미지 정보를 추출합니다.\"\"\"\n",
    "    images_info = []\n",
    "\n",
    "    if hasattr(document, \"pictures\") and document.pictures:\n",
    "        for idx, picture in enumerate(document.pictures):\n",
    "            try:\n",
    "                image_data = {\n",
    "                    \"image_index\": idx + 1,\n",
    "                    \"caption\": picture.caption_text(doc=document)\n",
    "                    if hasattr(picture, \"caption_text\")\n",
    "                    else \"\",\n",
    "                    \"has_image\": hasattr(picture, \"image\")\n",
    "                    and picture.image is not None,\n",
    "                }\n",
    "                # 페이지 번호 추출\n",
    "                if hasattr(picture, \"prov\") and picture.prov:\n",
    "                    for prov in picture.prov:\n",
    "                        if hasattr(prov, \"page_no\"):\n",
    "                            image_data[\"page\"] = prov.page_no\n",
    "                            break\n",
    "                images_info.append(image_data)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"이미지 {idx} 처리 오류: {e}\")\n",
    "\n",
    "    return {\n",
    "        \"count\": len(images_info),\n",
    "        \"data\": images_info,\n",
    "        \"processing\": {\"advanced_enabled\": False, \"ocr_enabled\": False},\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_outline_info(result) -> Dict[str, Any]:\n",
    "    \"\"\"모든 문서에서 아웃라인 정보를 추출합니다.\"\"\"\n",
    "    outline_data = _extract_outline_from_text(result)\n",
    "    logger.info(f\"아웃라인 {len(outline_data)}개 추출 완료\")\n",
    "\n",
    "    return {\n",
    "        \"enabled\": len(outline_data) > 0,\n",
    "        \"reason\": \"전체 문서 아웃라인 추출\",\n",
    "        \"data\": outline_data,\n",
    "    }\n",
    "\n",
    "\n",
    "def _extract_outline_from_text(result) -> List[Dict[str, Any]]:\n",
    "    \"\"\"section_header 라벨만 필터링해서 아웃라인 추출 (내부 함수)\"\"\"\n",
    "    outline_data = []\n",
    "\n",
    "    try:\n",
    "        document = result.document\n",
    "\n",
    "        logger.info(\"=== section_header 라벨 기반 아웃라인 추출 ===\")\n",
    "        logger.info(f\"document.pages 타입: {type(document.pages)}\")\n",
    "        logger.info(\n",
    "            f\"document.pages 길이: {len(document.pages) if hasattr(document.pages, '__len__') else 'N/A'}\"\n",
    "        )\n",
    "\n",
    "        if hasattr(document, \"texts\"):\n",
    "            for item in document.texts:\n",
    "                # section_header 라벨만 찾기\n",
    "                if hasattr(item, \"label\") and item.label == \"section_header\":\n",
    "                    text_content = \"\"\n",
    "                    if hasattr(item, \"text\") and item.text:\n",
    "                        text_content = item.text.strip()\n",
    "\n",
    "                    if text_content:\n",
    "                        logger.info(f\"제목 발견: '{text_content}'\")\n",
    "\n",
    "                        # 위치 정보 추출\n",
    "                        page_idx = 0\n",
    "                        y_position = 0.0\n",
    "                        bbox_info = {}\n",
    "\n",
    "                        if hasattr(item, \"prov\") and item.prov:\n",
    "                            for prov in item.prov:\n",
    "                                if hasattr(prov, \"page_no\"):\n",
    "                                    page_idx = prov.page_no - 1  # 1-based to 0-based\n",
    "\n",
    "                                if hasattr(prov, \"bbox\"):\n",
    "                                    bbox_info, y_position = _extract_bbox_info(\n",
    "                                        prov.bbox, document, page_idx\n",
    "                                    )\n",
    "                                    break\n",
    "\n",
    "                        # 첫 번째면 introduction, 나머지는 main body\n",
    "                        element_type = (\n",
    "                            \"introduction\" if len(outline_data) == 0 else \"main body\"\n",
    "                        )\n",
    "\n",
    "                        outline_item = {\n",
    "                            \"title\": text_content,\n",
    "                            \"pageIndex\": page_idx,\n",
    "                            \"y\": y_position,\n",
    "                            \"bbox\": bbox_info,\n",
    "                            \"dest\": f\"Section_{len(outline_data) + 2}\",\n",
    "                            \"size\": 23.3333,\n",
    "                            \"type\": element_type,\n",
    "                            \"items\": [],\n",
    "                        }\n",
    "                        outline_data.append(outline_item)\n",
    "\n",
    "        logger.info(f\"section_header로 찾은 아웃라인: {len(outline_data)}개\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"아웃라인 추출 에러: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    return outline_data\n",
    "\n",
    "\n",
    "def _extract_bbox_info(bbox, document, page_idx: int) -> tuple[Dict[str, Any], float]:\n",
    "    \"\"\"bbox 정보를 추출하고 정규화합니다.\"\"\"\n",
    "    # 페이지 크기 구하기 (안전하게)\n",
    "    page_width = 595  # A4 기본 너비\n",
    "    page_height = 842  # A4 기본 높이\n",
    "    y_position = 0.0\n",
    "\n",
    "    try:\n",
    "        if hasattr(document, \"pages\") and page_idx < len(document.pages):\n",
    "            # 다양한 접근 방식 시도\n",
    "            if isinstance(document.pages, list):\n",
    "                page = document.pages[page_idx]\n",
    "            elif isinstance(document.pages, dict):\n",
    "                page = document.pages.get(page_idx) or document.pages.get(str(page_idx))\n",
    "            else:\n",
    "                page = None\n",
    "\n",
    "            if page and hasattr(page, \"size\"):\n",
    "                if hasattr(page.size, \"height\"):\n",
    "                    page_height = page.size.height\n",
    "                if hasattr(page.size, \"width\"):\n",
    "                    page_width = page.size.width\n",
    "    except (KeyError, IndexError, AttributeError) as e:\n",
    "        logger.warning(f\"페이지 정보 접근 오류: {e}, 기본값 사용\")\n",
    "\n",
    "    # bbox 전체 정보 수집\n",
    "    bbox_info = {}\n",
    "\n",
    "    # 다양한 bbox 속성 확인\n",
    "    if (\n",
    "        hasattr(bbox, \"l\")\n",
    "        and hasattr(bbox, \"r\")\n",
    "        and hasattr(bbox, \"t\")\n",
    "        and hasattr(bbox, \"b\")\n",
    "    ):\n",
    "        # l, r, t, b 형식\n",
    "        bbox_info = {\n",
    "            \"left\": bbox.l / page_width,\n",
    "            \"right\": bbox.r / page_width,\n",
    "            \"top\": bbox.t / page_height,\n",
    "            \"bottom\": bbox.b / page_height,\n",
    "            \"width\": (bbox.r - bbox.l) / page_width,\n",
    "            \"height\": (bbox.b - bbox.t) / page_height,\n",
    "        }\n",
    "        y_position = bbox.t / page_height\n",
    "\n",
    "    return bbox_info, y_position\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de33736-f2ff-48f9-8886-7f941105645b",
   "metadata": {},
   "source": [
    "## 📋 3. 문서 처리 파이프라인 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0028bc0-c2cc-436c-955d-a07cfcd5af32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T00:39:59.437447Z",
     "iopub.status.busy": "2025-07-14T00:39:59.436762Z",
     "iopub.status.idle": "2025-07-14T00:39:59.448080Z",
     "shell.execute_reply": "2025-07-14T00:39:59.447326Z",
     "shell.execute_reply.started": "2025-07-14T00:39:59.437414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 파이프라인 준비 완료\n"
     ]
    }
   ],
   "source": [
    "def process_documents():\n",
    "    \"\"\"문서 처리 파이프라인 실행\"\"\"\n",
    "    logger.info(\"🚀 문서 처리 파이프라인 시작\")\n",
    "    \n",
    "    # 1. docs 폴더의 PDF 파일 목록 가져오기\n",
    "    pdf_files = list(DOCS_DIR.glob(\"*.pdf\"))\n",
    "    logger.info(f\"📁 처리할 PDF 파일: {len(pdf_files)}개\")\n",
    "    \n",
    "    # 2. 각 PDF 파일 처리\n",
    "    for pdf_file in pdf_files:\n",
    "        try:\n",
    "            logger.info(f\"\\n{'='*50}\")\n",
    "            logger.info(f\"📄 처리 시작: {pdf_file.name}\")\n",
    "            \n",
    "            # 2.1. PDF 파일 분석\n",
    "            result = pdf_service.analyze_pdf(str(pdf_file))\n",
    "            \n",
    "            # 2.2. 청크 생성\n",
    "            chunks = []\n",
    "            \n",
    "            # 2.2.1. 텍스트 기반 청크\n",
    "            text_content = result[\"markdown_content\"]\n",
    "            if text_content:\n",
    "                # 섹션 단위로 분할\n",
    "                sections = text_content.split(\"\\n## \")\n",
    "                for section in sections:\n",
    "                    if section.strip():\n",
    "                        chunks.append({\n",
    "                            \"type\": \"text\",\n",
    "                            \"content\": section.strip(),\n",
    "                            \"metadata\": {\n",
    "                                \"source\": pdf_file.name,\n",
    "                                \"type\": \"text_section\"\n",
    "                            }\n",
    "                        })\n",
    "            \n",
    "            # 2.2.2. 테이블 기반 청크\n",
    "            for table in result[\"tables\"][\"data\"]:\n",
    "                if table[\"markdown\"]:\n",
    "                    chunks.append({\n",
    "                        \"type\": \"table\",\n",
    "                        \"content\": table[\"markdown\"],\n",
    "                        \"metadata\": {\n",
    "                            \"source\": pdf_file.name,\n",
    "                            \"type\": \"table\",\n",
    "                            \"page\": table.get(\"page\", 1),\n",
    "                            \"rows\": table[\"rows\"],\n",
    "                            \"cols\": table[\"cols\"]\n",
    "                        }\n",
    "                    })\n",
    "            \n",
    "            # 2.2.3. 이미지 기반 청크 (캡션이 있는 경우)\n",
    "            for image in result[\"images\"][\"data\"]:\n",
    "                if image[\"caption\"]:\n",
    "                    chunks.append({\n",
    "                        \"type\": \"image\",\n",
    "                        \"content\": image[\"caption\"],\n",
    "                        \"metadata\": {\n",
    "                            \"source\": pdf_file.name,\n",
    "                            \"type\": \"image_caption\",\n",
    "                            \"page\": image.get(\"page\", 1)\n",
    "                        }\n",
    "                    })\n",
    "            \n",
    "            # 2.3. 청크 저장\n",
    "            chunk_file = CHUNKS_DIR / f\"chunks_{pdf_file.stem}.json\"\n",
    "            with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            logger.info(f\"✅ 청크 생성 완료: {len(chunks)}개\")\n",
    "            logger.info(f\"  - 저장 위치: {chunk_file}\")\n",
    "            \n",
    "            # 2.4. 벡터 저장 (향후 구현)\n",
    "            # TODO: 벡터 저장 로직 구현\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ 처리 실패 ({pdf_file.name}): {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    logger.info(\"\\n🎉 모든 문서 처리 완료\")\n",
    "\n",
    "# 파이프라인 준비\n",
    "print(\"✅ 파이프라인 준비 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1ac0b8-f0fd-4053-a8ad-8f8c7d5ca06c",
   "metadata": {},
   "source": [
    "## 🚀 4. 파이프라인 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5c5d612-e50d-4a69-9205-95b73f565ea5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T00:40:01.408816Z",
     "iopub.status.busy": "2025-07-14T00:40:01.407990Z",
     "iopub.status.idle": "2025-07-14T00:40:01.427001Z",
     "shell.execute_reply": "2025-07-14T00:40:01.417474Z",
     "shell.execute_reply.started": "2025-07-14T00:40:01.408788Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:🚀 문서 처리 파이프라인 시작\n",
      "INFO:__main__:📁 처리할 PDF 파일: 0개\n",
      "INFO:__main__:\n",
      "🎉 모든 문서 처리 완료\n"
     ]
    }
   ],
   "source": [
    "# 파이프라인 실행\n",
    "process_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508a33bd-ff18-4a00-899d-9731e7ef37cc",
   "metadata": {},
   "source": [
    "## 🧪 3. RAG 평가 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6632c47c-0664-4e32-92df-538ddf0ba4f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T00:42:20.958785Z",
     "iopub.status.busy": "2025-07-14T00:42:20.958392Z",
     "iopub.status.idle": "2025-07-14T00:42:20.981213Z",
     "shell.execute_reply": "2025-07-14T00:42:20.980402Z",
     "shell.execute_reply.started": "2025-07-14T00:42:20.958760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SmartCLMRAGEvaluator 클래스 정의 완료\n"
     ]
    }
   ],
   "source": [
    "# Smart CLM 청킹 로직\n",
    "class HierarchicalChunker:\n",
    "    \"\"\"\n",
    "    Docling에서 추출한 마크다운을 기반으로 계층적 청킹을 수행하는 클래스\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # 헤더 정의 (헤더 레벨과 메타데이터 키 이름)\n",
    "        self.headers_to_split_on = [\n",
    "            (\"#\", \"Header 1\"),\n",
    "            (\"##\", \"Header 2\"),\n",
    "            (\"###\", \"Header 3\"),\n",
    "            (\"####\", \"Header 4\"),\n",
    "        ]\n",
    "        \n",
    "        # Child 문서용 텍스트 분할기 설정\n",
    "        self.chunk_size = 500\n",
    "        self.chunk_overlap = 50\n",
    "\n",
    "    def chunk_markdown(self, markdown_content: str, filename: str = \"document\") -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        마크다운 콘텐츠를 계층적으로 청킹합니다.\n",
    "        \"\"\"\n",
    "        # 단순화된 청킹 로직 (실제 구현을 기반으로 간소화)\n",
    "        lines = markdown_content.split('\\n')\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_headers = {}\n",
    "        \n",
    "        for line in lines:\n",
    "            # 헤더 감지\n",
    "            if line.startswith('#'):\n",
    "                if current_chunk:\n",
    "                    # 이전 청크 저장\n",
    "                    chunk_content = '\\n'.join(current_chunk)\n",
    "                    if len(chunk_content.strip()) > 0:\n",
    "                        chunks.append({\n",
    "                            'content': chunk_content,\n",
    "                            'headers': current_headers.copy(),\n",
    "                            'char_count': len(chunk_content)\n",
    "                        })\n",
    "                    current_chunk = []\n",
    "                \n",
    "                # 헤더 레벨 파악\n",
    "                level = len(line) - len(line.lstrip('#'))\n",
    "                header_text = line.lstrip('#').strip()\n",
    "                current_headers[f'header_{level}'] = header_text\n",
    "                \n",
    "                # 하위 레벨 헤더 초기화\n",
    "                for i in range(level + 1, 5):\n",
    "                    if f'header_{i}' in current_headers:\n",
    "                        del current_headers[f'header_{i}']\n",
    "            \n",
    "            current_chunk.append(line)\n",
    "        \n",
    "        # 마지막 청크 저장\n",
    "        if current_chunk:\n",
    "            chunk_content = '\\n'.join(current_chunk)\n",
    "            if len(chunk_content.strip()) > 0:\n",
    "                chunks.append({\n",
    "                    'content': chunk_content,\n",
    "                    'headers': current_headers.copy(),\n",
    "                    'char_count': len(chunk_content)\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'filename': filename,\n",
    "            'chunks': chunks,\n",
    "            'summary': {\n",
    "                'total_chunks': len(chunks),\n",
    "                'average_size': sum(c['char_count'] for c in chunks) // len(chunks) if chunks else 0\n",
    "            }\n",
    "        }\n",
    "\n",
    "class SmartCLMRAGEvaluator:\n",
    "    \"\"\"\n",
    "    Smart CLM RAG 시스템 전용 평가기\n",
    "    \n",
    "    계약서 도메인에 특화된 RAG 평가 메트릭과 방법론을 제공합니다.\n",
    "    \"\"\"\n",
    "    \n",
    "    # def __init__(self, api_client: SmartCLMAPIClient):\n",
    "    #     self.api_client = api_client\n",
    "    #     self.chunker = HierarchicalChunker()\n",
    "    #     self.evaluation_results = {}\n",
    "        \n",
    "    def evaluate_chunking_quality(self, contract_id: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        청킹 품질 평가\n",
    "        \n",
    "        Args:\n",
    "            contract_id: 평가할 계약서 ID\n",
    "            \n",
    "        Returns:\n",
    "            청킹 품질 메트릭 딕셔너리\n",
    "        \"\"\"\n",
    "        print(f\"📊 계약서 {contract_id} 청킹 품질 평가 시작\")\n",
    "        \n",
    "        try:\n",
    "            # API를 통해 청크 데이터 조회\n",
    "            chunks_data = self.api_client.get_contract_chunks(contract_id)\n",
    "            \n",
    "            if not chunks_data:\n",
    "                print(f\"⚠️ API에서 데이터를 가져올 수 없습니다. 목업 데이터를 사용합니다.\")\n",
    "                chunks_data = self._create_mock_chunks_data(contract_id)\n",
    "            \n",
    "            if not chunks_data:\n",
    "                print(f\"⚠️ 계약서 {contract_id}의 청크 데이터가 없습니다\")\n",
    "                return {\"error\": \"No chunks found\"}\n",
    "            \n",
    "            # 청킹 메트릭 계산\n",
    "            char_counts = [chunk.get('char_count', 500) for chunk in chunks_data]\n",
    "            \n",
    "            metrics = {\n",
    "                \"total_chunks\": len(chunks_data),\n",
    "                \"parent_chunks\": len([c for c in chunks_data if c.get('chunk_type') == \"parent\"]),\n",
    "                \"child_chunks\": len([c for c in chunks_data if c.get('chunk_type') == \"child\"]),\n",
    "                \"avg_chunk_length\": float(np.mean(char_counts)) if char_counts else 0,\n",
    "                \"std_chunk_length\": float(np.std(char_counts)) if char_counts else 0,\n",
    "                \"chunks_with_embeddings\": len([c for c in chunks_data if c.get('embedding') is not None]),\n",
    "                \"hierarchical_coverage\": self._calculate_hierarchical_coverage_from_data(chunks_data)\n",
    "            }\n",
    "            \n",
    "            print(f\"✅ 청킹 평가 완료: {metrics['total_chunks']}개 청크, 임베딩 {metrics['chunks_with_embeddings']}개\")\n",
    "            return metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 청킹 평가 오류: {e}\")\n",
    "            return {\"error\": str(e)}\n",
    "    \n",
    "    def _create_mock_chunks_data(self, contract_id: int) -> List[Dict[str, Any]]:\n",
    "        \"\"\"목업 청크 데이터 생성 (딕셔너리 형태)\"\"\"\n",
    "        mock_chunks = []\n",
    "        for i in range(15):  # 15개 청크 생성\n",
    "            chunk_data = {\n",
    "                'id': i + 1,\n",
    "                'contract_id': contract_id,\n",
    "                'chunk_type': 'parent' if i < 3 else 'child',\n",
    "                'char_count': int(np.random.randint(300, 800)),\n",
    "                'embedding': [0.1] * 1024 if i < 10 else None,\n",
    "                'header_1': f\"제{i+1}조\" if i < 5 else None,\n",
    "                'header_2': f\"항목 {i+1}\" if i < 8 else None,\n",
    "                'header_3': None,\n",
    "                'header_4': None,\n",
    "                'content': f\"계약서 {contract_id} 청크 {i+1} 내용입니다. 이것은 목업 데이터입니다.\"\n",
    "            }\n",
    "            mock_chunks.append(chunk_data)\n",
    "        return mock_chunks\n",
    "    \n",
    "    def _calculate_hierarchical_coverage_from_data(self, chunks_data: List[Dict[str, Any]]) -> float:\n",
    "        \"\"\"계층 구조 커버리지 계산 (딕셔너리 데이터용)\"\"\"\n",
    "        headers = [\"header_1\", \"header_2\", \"header_3\", \"header_4\"]\n",
    "        coverage_scores = []\n",
    "        \n",
    "        for header in headers:\n",
    "            chunks_with_header = len([c for c in chunks_data if c.get(header) is not None])\n",
    "            coverage = chunks_with_header / len(chunks_data) if chunks_data else 0\n",
    "            coverage_scores.append(coverage)\n",
    "        \n",
    "        return float(np.mean(coverage_scores))\n",
    "    \n",
    "    def evaluate_retrieval_accuracy(self, test_queries: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        검색 정확도 평가\n",
    "        \n",
    "        Args:\n",
    "            test_queries: 테스트 쿼리 리스트 [{\"query\": \"...\", \"expected_chunks\": [...], \"contract_id\": ...}]\n",
    "            \n",
    "        Returns:\n",
    "            검색 정확도 메트릭\n",
    "        \"\"\"\n",
    "        print(f\"🔍 {len(test_queries)}개 쿼리로 검색 정확도 평가 시작\")\n",
    "        \n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        f1_scores = []\n",
    "        \n",
    "        for i, test_case in enumerate(test_queries):\n",
    "            query = test_case[\"query\"]\n",
    "            expected_chunks = set(test_case.get(\"expected_chunks\", []))\n",
    "            contract_id = test_case.get(\"contract_id\", 1)\n",
    "            \n",
    "            # API 또는 시뮬레이션을 통한 벡터 검색\n",
    "            retrieved_chunks = self._perform_vector_search(query, contract_id)\n",
    "            retrieved_chunk_ids = set([str(chunk.get('id', i)) for chunk in retrieved_chunks])\n",
    "            \n",
    "            # 정밀도, 재현율, F1 계산\n",
    "            precision = 0.0\n",
    "            recall = 0.0\n",
    "            f1 = 0.0\n",
    "            \n",
    "            if retrieved_chunk_ids:\n",
    "                precision = len(expected_chunks & retrieved_chunk_ids) / len(retrieved_chunk_ids)\n",
    "                precision_scores.append(precision)\n",
    "            \n",
    "            if expected_chunks:\n",
    "                recall = len(expected_chunks & retrieved_chunk_ids) / len(expected_chunks)\n",
    "                recall_scores.append(recall)\n",
    "            \n",
    "            if precision > 0 or recall > 0:\n",
    "                f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "                f1_scores.append(f1)\n",
    "            \n",
    "            print(f\"  쿼리 {i+1}: P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}\")\n",
    "        \n",
    "        results = {\n",
    "            \"avg_precision\": float(np.mean(precision_scores)) if precision_scores else 0.0,\n",
    "            \"avg_recall\": float(np.mean(recall_scores)) if recall_scores else 0.0,\n",
    "            \"avg_f1\": float(np.mean(f1_scores)) if f1_scores else 0.0,\n",
    "            \"total_queries\": len(test_queries)\n",
    "        }\n",
    "        \n",
    "        print(f\"✅ 검색 평가 완료: P={results['avg_precision']:.3f}, R={results['avg_recall']:.3f}, F1={results['avg_f1']:.3f}\")\n",
    "        return results\n",
    "    \n",
    "    def _perform_vector_search(self, query: str, contract_id: int, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"벡터 검색 수행 (API 우선, 실패시 목업 데이터)\"\"\"\n",
    "        # 먼저 API를 통한 실제 검색 시도\n",
    "        search_results = self.api_client.search_chunks(query, contract_id, top_k)\n",
    "        \n",
    "        if search_results:\n",
    "            return search_results\n",
    "        else:\n",
    "            # API 실패시 목업 데이터 반환\n",
    "            print(f\"  ⚠️ API 검색 실패, 목업 데이터 사용\")\n",
    "            mock_chunks = self._create_mock_chunks_data(contract_id)\n",
    "            return mock_chunks[:top_k]\n",
    "\n",
    "print(\"✅ SmartCLMRAGEvaluator 클래스 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe70428-49de-45c2-a694-98008d72777c",
   "metadata": {},
   "source": [
    "## 📝 4. 테스트 데이터 생성 및 평가 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "548bfe28-442c-46d6-964d-4d572e2231ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T00:42:33.587105Z",
     "iopub.status.busy": "2025-07-14T00:42:33.586789Z",
     "iopub.status.idle": "2025-07-14T00:42:33.593339Z",
     "shell.execute_reply": "2025-07-14T00:42:33.592639Z",
     "shell.execute_reply.started": "2025-07-14T00:42:33.587082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 생성된 테스트 쿼리: 5개\n",
      "  1. [기본정보] 계약 기간은 언제까지인가요?...\n",
      "  2. [리스크] 위약금 조항이 있나요?...\n",
      "  3. [재무] 지급 조건은 어떻게 되나요?...\n",
      "  4. [해지] 계약 해지 사유는 무엇인가요?...\n",
      "  5. [법적책임] 손해배상 책임은 누구에게 있나요?...\n"
     ]
    }
   ],
   "source": [
    "# RAG 평가기 인스턴스 생성 (API 클라이언트와 함께)\n",
    "# evaluator = SmartCLMRAGEvaluator(api_client)\n",
    "\n",
    "# 계약서 도메인 특화 테스트 쿼리 생성\n",
    "test_queries = [\n",
    "    {\n",
    "        \"query\": \"계약 기간은 언제까지인가요?\",\n",
    "        \"expected_chunks\": [\"1\", \"2\", \"3\"],\n",
    "        \"contract_id\": 1,\n",
    "        \"category\": \"기본정보\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"위약금 조항이 있나요?\",\n",
    "        \"expected_chunks\": [\"5\", \"6\"],\n",
    "        \"contract_id\": 1,\n",
    "        \"category\": \"리스크\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"지급 조건은 어떻게 되나요?\",\n",
    "        \"expected_chunks\": [\"4\", \"7\", \"8\"],\n",
    "        \"contract_id\": 1,\n",
    "        \"category\": \"재무\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"계약 해지 사유는 무엇인가요?\",\n",
    "        \"expected_chunks\": [\"9\", \"10\"],\n",
    "        \"contract_id\": 1,\n",
    "        \"category\": \"해지\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"손해배상 책임은 누구에게 있나요?\",\n",
    "        \"expected_chunks\": [\"11\", \"12\"],\n",
    "        \"contract_id\": 1,\n",
    "        \"category\": \"법적책임\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"📊 생성된 테스트 쿼리: {len(test_queries)}개\")\n",
    "for i, query in enumerate(test_queries):\n",
    "    print(f\"  {i+1}. [{query['category']}] {query['query'][:30]}...\")\n",
    "\n",
    "# 평가 결과 저장용 딕셔너리\n",
    "evaluation_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73e2a426-8b47-4d80-b124-6841b80ea594",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T00:42:44.176285Z",
     "iopub.status.busy": "2025-07-14T00:42:44.175803Z",
     "iopub.status.idle": "2025-07-14T00:42:44.198500Z",
     "shell.execute_reply": "2025-07-14T00:42:44.197390Z",
     "shell.execute_reply.started": "2025-07-14T00:42:44.176259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 === 청킹 품질 평가 시작 ===\n",
      "\n",
      "📊 청킹 평가 결과:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'chunking_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# chunking_results = evaluator.evaluate_chunking_quality(contract_id=1)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# evaluation_results['chunking'] = chunking_results\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m📊 청킹 평가 결과:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - 총 청크 수: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mchunking_results\u001b[49m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_chunks\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - Parent 청크: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunking_results\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparent_chunks\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - Child 청크: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunking_results\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchild_chunks\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chunking_results' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. 청킹 품질 평가 실행\n",
    "print(\"🔍 === 청킹 품질 평가 시작 ===\")\n",
    "chunking_results = evaluator.evaluate_chunking_quality(contract_id=1)\n",
    "evaluation_results['chunking'] = chunking_results\n",
    "\n",
    "print(\"\\n📊 청킹 평가 결과:\")\n",
    "print(f\"  - 총 청크 수: {chunking_results.get('total_chunks', 0)}\")\n",
    "print(f\"  - Parent 청크: {chunking_results.get('parent_chunks', 0)}\")\n",
    "print(f\"  - Child 청크: {chunking_results.get('child_chunks', 0)}\")\n",
    "print(f\"  - 평균 청크 길이: {chunking_results.get('avg_chunk_length', 0):.1f}자\")\n",
    "print(f\"  - 임베딩 보유 청크: {chunking_results.get('chunks_with_embeddings', 0)}\")\n",
    "print(f\"  - 계층 구조 커버리지: {chunking_results.get('hierarchical_coverage', 0):.3f}\")\n",
    "\n",
    "# 청킹 품질 점수 계산\n",
    "chunk_quality_score = 0\n",
    "if chunking_results.get('total_chunks', 0) > 0:\n",
    "    embedding_ratio = chunking_results.get('chunks_with_embeddings', 0) / chunking_results.get('total_chunks', 1)\n",
    "    hierarchy_score = chunking_results.get('hierarchical_coverage', 0)\n",
    "    chunk_quality_score = (embedding_ratio * 0.6 + hierarchy_score * 0.4) * 100\n",
    "\n",
    "print(f\"\\n✅ 청킹 품질 종합 점수: {chunk_quality_score:.1f}/100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3dc428b-0541-408a-b52f-57097ea2ba9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T03:59:01.652314Z",
     "iopub.status.busy": "2025-07-11T03:59:01.651848Z",
     "iopub.status.idle": "2025-07-11T03:59:01.851701Z",
     "shell.execute_reply": "2025-07-11T03:59:01.850822Z",
     "shell.execute_reply.started": "2025-07-11T03:59:01.652255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?.\n",
      "See 'docker run --help'.\n"
     ]
    }
   ],
   "source": [
    "!docker run pgvector/pgvector:0.8.0-pg16 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afa7a910-a338-440e-b780-229cfbf6dda0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T03:58:02.080939Z",
     "iopub.status.busy": "2025-07-11T03:58:02.080543Z",
     "iopub.status.idle": "2025-07-11T03:58:02.096387Z",
     "shell.execute_reply": "2025-07-11T03:58:02.095674Z",
     "shell.execute_reply.started": "2025-07-11T03:58:02.080902Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sagemaker-user'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096811e2-f938-4c5a-8473-efa80abdd3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
