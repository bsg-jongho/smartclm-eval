{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1045872c-9dcf-4e8d-bc55-b6dee2b0e2b1",
   "metadata": {},
   "source": [
    "# ğŸ“„ Smart CLM ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
    "\n",
    "Smart CLM ì‹œìŠ¤í…œì˜ ë¬¸ì„œ ì²˜ë¦¬ ë° RAG í‰ê°€ë¥¼ ìœ„í•œ í†µí•© ë…¸íŠ¸ë¶ì…ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ—ï¸ ì‹œìŠ¤í…œ êµ¬ì„±\n",
    "- **ë¬¸ì„œ ì²˜ë¦¬ ì—”ì§„**: PDF ë¬¸ì„œ ë¶„ì„ ë° êµ¬ì¡°í™”\n",
    "- **ë¡œì»¬ ìŠ¤í† ë¦¬ì§€**: íŒŒì¼ ì‹œìŠ¤í…œ ê¸°ë°˜ ë°ì´í„° ì €ì¥\n",
    "- **ë²¡í„° ì €ì¥ì†Œ**: ë¡œì»¬ íŒŒì¼ ê¸°ë°˜ ë²¡í„° ë°ì´í„° ê´€ë¦¬\n",
    "- **Jupyter Notebook**: ë¬¸ì„œ ì²˜ë¦¬ ë° í‰ê°€ ë„êµ¬\n",
    "\n",
    "## ğŸ“‹ ì£¼ìš” ê¸°ëŠ¥\n",
    "1. **docs í´ë” ìŠ¤ìº”**: PDF íŒŒì¼ ìë™ íƒìƒ‰\n",
    "2. **ë¬¸ì„œ ë¶„ì„**: ë‚´ì¥ PDF ì²˜ë¦¬ ì—”ì§„ìœ¼ë¡œ ë¶„ì„\n",
    "3. **ë¡œì»¬ ì €ì¥**: íŒŒì¼ ì‹œìŠ¤í…œì— ê²°ê³¼ ì €ì¥\n",
    "4. **RAG í‰ê°€**: ë¬¸ì„œ ì²˜ë¦¬ í’ˆì§ˆ ë° ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€\n",
    "5. **ê²°ê³¼ ì‹œê°í™”**: ì²˜ë¦¬ ê²°ê³¼ ë° ì„±ëŠ¥ ì§€í‘œ ëŒ€ì‹œë³´ë“œ\n",
    "\n",
    "## ğŸš€ ì‚¬ìš© ë°©ë²•\n",
    "1. `docs/` í´ë”ì— ì²˜ë¦¬í•  PDF íŒŒì¼ ë°°ì¹˜\n",
    "2. ë…¸íŠ¸ë¶ ì…€ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰\n",
    "3. ê²°ê³¼ í™•ì¸ ë° ë¶„ì„\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e21772a-49a2-46e7-a331-d65ffda5db93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T08:19:38.671478Z",
     "iopub.status.busy": "2025-07-14T08:19:38.671091Z",
     "iopub.status.idle": "2025-07-14T08:19:54.064772Z",
     "shell.execute_reply": "2025-07-14T08:19:54.063968Z",
     "shell.execute_reply.started": "2025-07-14T08:19:38.671445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ragas\n",
      "  Using cached ragas-0.2.15-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting docling\n",
      "  Using cached docling-2.41.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting streamlit\n",
      "  Using cached streamlit-1.46.1-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from ragas) (1.26.4)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.12/site-packages (from ragas) (2.2.1)\n",
      "Collecting tiktoken (from ragas)\n",
      "  Using cached tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.12/site-packages (from ragas) (0.3.25)\n",
      "Requirement already satisfied: langchain-core in /opt/conda/lib/python3.12/site-packages (from ragas) (0.3.63)\n",
      "Requirement already satisfied: langchain-community in /opt/conda/lib/python3.12/site-packages (from ragas) (0.3.24)\n",
      "Collecting langchain_openai (from ragas)\n",
      "  Using cached langchain_openai-0.3.27-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.12/site-packages (from ragas) (1.6.0)\n",
      "Requirement already satisfied: appdirs in /opt/conda/lib/python3.12/site-packages (from ragas) (1.4.4)\n",
      "Requirement already satisfied: pydantic>=2 in /opt/conda/lib/python3.12/site-packages (from ragas) (2.11.4)\n",
      "Collecting openai>1 (from ragas)\n",
      "  Using cached openai-1.95.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: diskcache>=5.6.3 in /opt/conda/lib/python3.12/site-packages (from ragas) (5.6.3)\n",
      "Collecting docling-core<3.0.0,>=2.42.0 (from docling-core[chunking]<3.0.0,>=2.42.0->docling)\n",
      "  Using cached docling_core-2.42.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting docling-parse<5.0.0,>=4.0.0 (from docling)\n",
      "  Using cached docling_parse-4.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting docling-ibm-models<4,>=3.6.0 (from docling)\n",
      "  Using cached docling_ibm_models-3.8.1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from docling)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting pypdfium2<5.0.0,>=4.30.0 (from docling)\n",
      "  Using cached pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.3.0 in /opt/conda/lib/python3.12/site-packages (from docling) (2.9.1)\n",
      "Requirement already satisfied: huggingface_hub<1,>=0.23 in /opt/conda/lib/python3.12/site-packages (from docling) (0.32.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.2 in /opt/conda/lib/python3.12/site-packages (from docling) (2.32.3)\n",
      "Collecting easyocr<2.0,>=1.7 (from docling)\n",
      "  Using cached easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in /opt/conda/lib/python3.12/site-packages (from docling) (2025.4.26)\n",
      "Collecting rtree<2.0.0,>=1.3.0 (from docling)\n",
      "  Using cached rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typer<0.17.0,>=0.12.5 in /opt/conda/lib/python3.12/site-packages (from docling) (0.16.0)\n",
      "Collecting python-docx<2.0.0,>=1.1.2 (from docling)\n",
      "  Using cached python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting python-pptx<2.0.0,>=1.0.2 (from docling)\n",
      "  Using cached python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/conda/lib/python3.12/site-packages (from docling) (4.13.4)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.1.4 in /opt/conda/lib/python3.12/site-packages (from docling) (2.2.3)\n",
      "Collecting marko<3.0.0,>=2.1.2 (from docling)\n",
      "  Using cached marko-2.1.4-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting openpyxl<4.0.0,>=3.1.5 (from docling)\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: lxml<6.0.0,>=4.0.0 in /opt/conda/lib/python3.12/site-packages (from docling) (5.4.0)\n",
      "Requirement already satisfied: pillow<12.0.0,>=10.0.0 in /opt/conda/lib/python3.12/site-packages (from docling) (11.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.65.0 in /opt/conda/lib/python3.12/site-packages (from docling) (4.67.1)\n",
      "Requirement already satisfied: pluggy<2.0.0,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from docling) (1.6.0)\n",
      "Collecting pylatexenc<3.0,>=2.10 (from docling)\n",
      "  Using cached pylatexenc-2.10-py3-none-any.whl\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from docling) (1.15.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.12/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.12/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (4.13.2)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in /opt/conda/lib/python3.12/site-packages (from docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (4.23.0)\n",
      "Collecting jsonref<2.0.0,>=1.1.0 (from docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling)\n",
      "  Using cached jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /opt/conda/lib/python3.12/site-packages (from docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (0.9.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.1 in /opt/conda/lib/python3.12/site-packages (from docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (6.0.2)\n",
      "Collecting latex2mathml<4.0.0,>=3.77.0 (from docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling)\n",
      "  Using cached latex2mathml-3.78.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting semchunk<3.0.0,>=2.2.0 (from docling-core[chunking]<3.0.0,>=2.42.0->docling)\n",
      "  Using cached semchunk-2.2.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/conda/lib/python3.12/site-packages (from docling-core[chunking]<3.0.0,>=2.42.0->docling) (4.52.4)\n",
      "Requirement already satisfied: torch<3.0.0,>=2.2.2 in /opt/conda/lib/python3.12/site-packages (from docling-ibm-models<4,>=3.6.0->docling) (2.6.0)\n",
      "Requirement already satisfied: torchvision<1,>=0 in /opt/conda/lib/python3.12/site-packages (from docling-ibm-models<4,>=3.6.0->docling) (0.21.0)\n",
      "Collecting jsonlines<4.0.0,>=3.1.0 (from docling-ibm-models<4,>=3.6.0->docling)\n",
      "  Using cached jsonlines-3.1.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting opencv-python-headless<5.0.0.0,>=4.6.0.66 (from docling-ibm-models<4,>=3.6.0->docling)\n",
      "  Using cached opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: safetensors<1,>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from safetensors[torch]<1,>=0.4.3->docling-ibm-models<4,>=3.6.0->docling) (0.5.3)\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.12/site-packages (from easyocr<2.0,>=1.7->docling) (0.25.2)\n",
      "Collecting python-bidi (from easyocr<2.0,>=1.7->docling)\n",
      "  Using cached python_bidi-0.6.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting Shapely (from easyocr<2.0,>=1.7->docling)\n",
      "  Using cached shapely-2.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting pyclipper (from easyocr<2.0,>=1.7->docling)\n",
      "  Using cached pyclipper-1.3.0.post6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting ninja (from easyocr<2.0,>=1.7->docling)\n",
      "  Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface_hub<1,>=0.23->docling) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub<1,>=0.23->docling) (2024.12.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub<1,>=0.23->docling) (1.1.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub<1,>=0.23->docling) (24.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.12/site-packages (from jsonlines<4.0.0,>=3.1.0->docling-ibm-models<4,>=3.6.0->docling) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (0.25.1)\n",
      "Collecting numpy (from ragas)\n",
      "  Using cached numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting et-xmlfile (from openpyxl<4.0.0,>=3.1.5->docling)\n",
      "  Using cached et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas<3.0.0,>=2.1.4->docling) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas<3.0.0,>=2.1.4->docling) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic>=2->ragas) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.12/site-packages (from pydantic>=2->ragas) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from pydantic>=2->ragas) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/conda/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.3.0->docling) (1.1.0)\n",
      "Collecting XlsxWriter>=0.5.7 (from python-pptx<2.0.0,>=1.0.2->docling)\n",
      "  Using cached xlsxwriter-3.2.5-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.2->docling) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.2->docling) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.2->docling) (1.26.19)\n",
      "Collecting mpire[dill] (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.42.0->docling)\n",
      "  Using cached mpire-2.10.2-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4,>=3.6.0->docling) (80.8.0)\n",
      "Requirement already satisfied: sympy!=1.13.2,>=1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4,>=3.6.0->docling) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4,>=3.6.0->docling) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4,>=3.6.0->docling) (3.1.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (0.21.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.12/site-packages (from typer<0.17.0,>=0.12.5->docling) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.12/site-packages (from typer<0.17.0,>=0.12.5->docling) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.12/site-packages (from typer<0.17.0,>=0.12.5->docling) (14.0.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /opt/conda/lib/python3.12/site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in /opt/conda/lib/python3.12/site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in /opt/conda/lib/python3.12/site-packages (from streamlit) (5.5.2)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in /opt/conda/lib/python3.12/site-packages (from streamlit) (5.28.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /opt/conda/lib/python3.12/site-packages (from streamlit) (19.0.1)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /opt/conda/lib/python3.12/site-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /opt/conda/lib/python3.12/site-packages (from streamlit) (0.10.2)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
      "  Using cached watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/conda/lib/python3.12/site-packages (from streamlit) (3.1.44)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /opt/conda/lib/python3.12/site-packages (from streamlit) (6.5.1)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /opt/conda/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit) (1.41.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.12/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch<3.0.0,>=2.2.2->docling-ibm-models<4,>=3.6.0->docling) (3.0.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.12/site-packages (from openai>1->ragas) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.12/site-packages (from openai>1->ragas) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from openai>1->ragas) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>1->ragas)\n",
      "  Using cached jiter-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.12/site-packages (from openai>1->ragas) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas) (0.16.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.1.4->docling) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich>=10.11.0->typer<0.17.0,>=0.12.5->docling) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich>=10.11.0->typer<0.17.0,>=0.12.5->docling) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.17.0,>=0.12.5->docling) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy!=1.13.2,>=1.13.1->torch<3.0.0,>=2.2.2->docling-ibm-models<4,>=3.6.0->docling) (1.3.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.12/site-packages (from datasets->ragas) (0.4.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.12/site-packages (from datasets->ragas) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.12/site-packages (from datasets->ragas) (0.70.18)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.12/site-packages (from datasets->ragas) (3.9.5)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.12/site-packages (from datasets->ragas) (0.18.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets->ragas) (1.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets->ragas) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets->ragas) (6.4.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets->ragas) (1.20.0)\n",
      "Requirement already satisfied: propcache>=0.2.1 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.0->aiohttp->datasets->ragas) (0.3.1)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/conda/lib/python3.12/site-packages (from langchain->ragas) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /opt/conda/lib/python3.12/site-packages (from langchain->ragas) (0.2.11)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.12/site-packages (from langchain->ragas) (2.0.41)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.12/site-packages (from langchain-core->ragas) (1.33)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain->ragas) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain->ragas) (1.0.0)\n",
      "Requirement already satisfied: greenlet>=1 in /opt/conda/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain->ragas) (3.2.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.12/site-packages (from langchain-community->ragas) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from langchain-community->ragas) (0.4.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (0.9.0)\n",
      "Requirement already satisfied: mypy_extensions>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (1.1.0)\n",
      "Collecting langchain-core (from ragas)\n",
      "  Using cached langchain_core-0.3.68-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain->ragas)\n",
      "  Using cached langsmith-0.3.45-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain->ragas) (0.23.0)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /opt/conda/lib/python3.12/site-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.12/site-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (2025.5.26)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /opt/conda/lib/python3.12/site-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (0.4)\n",
      "Using cached ragas-0.2.15-py3-none-any.whl (190 kB)\n",
      "Using cached docling-2.41.0-py3-none-any.whl (187 kB)\n",
      "Using cached docling_core-2.42.0-py3-none-any.whl (158 kB)\n",
      "Using cached docling_ibm_models-3.8.1-py3-none-any.whl (86 kB)\n",
      "Using cached docling_parse-4.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.1 MB)\n",
      "Using cached easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
      "Using cached jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
      "Using cached latex2mathml-3.78.0-py3-none-any.whl (73 kB)\n",
      "Using cached marko-2.1.4-py3-none-any.whl (42 kB)\n",
      "Using cached opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (54.0 MB)\n",
      "Using cached numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
      "Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Using cached pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "Using cached python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
      "Using cached python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
      "Using cached rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (541 kB)\n",
      "Using cached semchunk-2.2.2-py3-none-any.whl (10 kB)\n",
      "Using cached streamlit-1.46.1-py3-none-any.whl (10.1 MB)\n",
      "Using cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "Using cached watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
      "Using cached openai-1.95.1-py3-none-any.whl (755 kB)\n",
      "Using cached jiter-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Using cached xlsxwriter-3.2.5-py3-none-any.whl (172 kB)\n",
      "Using cached et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Using cached langchain_openai-0.3.27-py3-none-any.whl (70 kB)\n",
      "Using cached langchain_core-0.3.68-py3-none-any.whl (441 kB)\n",
      "Using cached langsmith-0.3.45-py3-none-any.whl (363 kB)\n",
      "Using cached tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached mpire-2.10.2-py3-none-any.whl (272 kB)\n",
      "Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "Using cached pyclipper-1.3.0.post6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (963 kB)\n",
      "Using cached python_bidi-0.6.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (292 kB)\n",
      "Using cached shapely-2.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Installing collected packages: python-bidi, pylatexenc, pyclipper, filetype, XlsxWriter, watchdog, rtree, python-docx, pypdfium2, numpy, ninja, mpire, marko, latex2mathml, jsonref, jsonlines, jiter, et-xmlfile, tiktoken, Shapely, python-pptx, pydeck, openpyxl, opencv-python-headless, semchunk, openai, langsmith, langchain-core, easyocr, docling-core, streamlit, langchain_openai, docling-parse, docling-ibm-models, docling, ragas\n",
      "\u001b[2K  Attempting uninstall: numpym\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 8/36\u001b[0m [pypdfium2]x]\n",
      "\u001b[2K    Found existing installation: numpy 1.26.4â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 8/36\u001b[0m [pypdfium2]\n",
      "\u001b[2K    Uninstalling numpy-1.26.4:mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 8/36\u001b[0m [pypdfium2]\n",
      "\u001b[2K      Successfully uninstalled numpy-1.26.4â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 8/36\u001b[0m [pypdfium2]\n",
      "\u001b[2K  Attempting uninstall: langsmithâ”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m25/36\u001b[0m [openai]python-headless]\n",
      "\u001b[2K    Found existing installation: langsmith 0.2.11mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m25/36\u001b[0m [openai]\n",
      "\u001b[2K    Uninstalling langsmith-0.2.11:m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m25/36\u001b[0m [openai]\n",
      "\u001b[2K      Successfully uninstalled langsmith-0.2.1190mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m25/36\u001b[0m [openai]\n",
      "\u001b[2K  Attempting uninstall: langchain-core0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m26/36\u001b[0m [langsmith]\n",
      "\u001b[2K    Found existing installation: langchain-core 0.3.63â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m26/36\u001b[0m [langsmith]\n",
      "\u001b[2K    Uninstalling langchain-core-0.3.63:1mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m26/36\u001b[0m [langsmith]\n",
      "\u001b[2K      Successfully uninstalled langchain-core-0.3.63â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m26/36\u001b[0m [langsmith]\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m36/36\u001b[0m [ragas]m35/36\u001b[0m [ragas]g]parse]]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 1.3.1 requires nvidia-ml-py3<8.0,>=7.352.0, which is not installed.\n",
      "amazon-sagemaker-jupyter-ai-q-developer 1.2.4 requires numpy<=2.0.1, but you have numpy 2.2.6 which is incompatible.\n",
      "amazon-sagemaker-sql-magic 0.1.4 requires numpy<2, but you have numpy 2.2.6 which is incompatible.\n",
      "autogluon-multimodal 1.3.1 requires transformers[sentencepiece]<4.50,>=4.38.0, but you have transformers 4.52.4 which is incompatible.\n",
      "autogluon-timeseries 1.3.1 requires transformers[sentencepiece]<4.50,>=4.38.0, but you have transformers 4.52.4 which is incompatible.\n",
      "catboost 1.2.7 requires numpy<2.0,>=1.16.0, but you have numpy 2.2.6 which is incompatible.\n",
      "gluonts 0.16.1 requires numpy<2.2,>=1.16, but you have numpy 2.2.6 which is incompatible.\n",
      "langchain-aws 0.2.19 requires boto3>=1.37.24, but you have boto3 1.37.1 which is incompatible.\n",
      "sagemaker 2.245.0 requires numpy==1.26.4, but you have numpy 2.2.6 which is incompatible.\n",
      "spacy 3.8.6 requires thinc<8.4.0,>=8.3.4, but you have thinc 8.3.2 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Shapely-2.1.1 XlsxWriter-3.2.5 docling-2.41.0 docling-core-2.42.0 docling-ibm-models-3.8.1 docling-parse-4.1.0 easyocr-1.7.2 et-xmlfile-2.0.0 filetype-1.2.0 jiter-0.10.0 jsonlines-3.1.0 jsonref-1.1.0 langchain-core-0.3.68 langchain_openai-0.3.27 langsmith-0.3.45 latex2mathml-3.78.0 marko-2.1.4 mpire-2.10.2 ninja-1.11.1.4 numpy-2.2.6 openai-1.95.1 opencv-python-headless-4.12.0.88 openpyxl-3.1.5 pyclipper-1.3.0.post6 pydeck-0.9.1 pylatexenc-2.10 pypdfium2-4.30.1 python-bidi-0.6.6 python-docx-1.2.0 python-pptx-1.0.2 ragas-0.2.15 rtree-1.4.0 semchunk-2.2.2 streamlit-1.46.1 tiktoken-0.9.0 watchdog-6.0.0\n"
     ]
    }
   ],
   "source": [
    "# 0. ì˜ì¡´ì„± ì„¤ì¹˜\n",
    "!pip install ragas docling streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbef1828-5feb-471b-977d-a0348667e76d",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ca8cdc8-de93-4b98-8970-14bb46cee581",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T00:39:45.437371Z",
     "iopub.status.busy": "2025-07-14T00:39:45.437078Z",
     "iopub.status.idle": "2025-07-14T00:39:47.166637Z",
     "shell.execute_reply": "2025-07-14T00:39:47.165781Z",
     "shell.execute_reply.started": "2025-07-14T00:39:45.437345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ\n",
      "ğŸ“ ë¬¸ì„œ í´ë”: docs\n",
      "ğŸ“Š ê²°ê³¼ í´ë”: results\n",
      "ğŸ’¾ ì²˜ë¦¬ í´ë”: processed\n",
      "ğŸ”¢ ë²¡í„° í´ë”: vectors\n",
      "ğŸ“‘ ì²­í¬ í´ë”: chunks\n",
      "\n",
      "ğŸ“„ docs í´ë” ë‚´ PDF íŒŒì¼: 0ê°œ\n"
     ]
    }
   ],
   "source": [
    "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ì‹œê°í™” ìŠ¤íƒ€ì¼ ì„¤ì •\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# ë¡œê¹… ì„¤ì •\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ê¸°ë³¸ ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "ROOT_DIR = Path(\".\")\n",
    "DOCS_DIR = ROOT_DIR / \"docs\"\n",
    "RESULTS_DIR = ROOT_DIR / \"results\"\n",
    "PROCESSED_DIR = ROOT_DIR / \"processed\"\n",
    "VECTORS_DIR = ROOT_DIR / \"vectors\"\n",
    "CHUNKS_DIR = ROOT_DIR / \"chunks\"\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "for dir_path in [DOCS_DIR, RESULTS_DIR, PROCESSED_DIR, VECTORS_DIR, CHUNKS_DIR]:\n",
    "    dir_path.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ\")\n",
    "print(f\"ğŸ“ ë¬¸ì„œ í´ë”: {DOCS_DIR}\")\n",
    "print(f\"ğŸ“Š ê²°ê³¼ í´ë”: {RESULTS_DIR}\")\n",
    "print(f\"ğŸ’¾ ì²˜ë¦¬ í´ë”: {PROCESSED_DIR}\")\n",
    "print(f\"ğŸ”¢ ë²¡í„° í´ë”: {VECTORS_DIR}\")\n",
    "print(f\"ğŸ“‘ ì²­í¬ í´ë”: {CHUNKS_DIR}\")\n",
    "\n",
    "# docs í´ë” ìƒíƒœ í™•ì¸\n",
    "pdf_files = list(DOCS_DIR.glob(\"*.pdf\"))\n",
    "print(f\"\\nğŸ“„ docs í´ë” ë‚´ PDF íŒŒì¼: {len(pdf_files)}ê°œ\")\n",
    "for pdf_file in pdf_files:\n",
    "    file_size = pdf_file.stat().st_size\n",
    "    print(f\"  - {pdf_file.name}: {file_size:,} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65872c1-02ad-4c53-97c4-636ba77c3af6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T03:45:24.411303Z",
     "iopub.status.busy": "2025-07-11T03:45:24.410916Z",
     "iopub.status.idle": "2025-07-11T03:45:24.416966Z",
     "shell.execute_reply": "2025-07-11T03:45:24.416144Z",
     "shell.execute_reply.started": "2025-07-11T03:45:24.411276Z"
    }
   },
   "source": [
    "## ğŸ“„ 2. PDF ì²˜ë¦¬ ì—”ì§„ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979898fc-e3ed-4d9d-9ba5-0402e82fec88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T00:39:51.959057Z",
     "iopub.status.busy": "2025-07-14T00:39:51.958418Z",
     "iopub.status.idle": "2025-07-14T00:39:51.988677Z",
     "shell.execute_reply": "2025-07-14T00:39:51.988033Z",
     "shell.execute_reply.started": "2025-07-14T00:39:51.959026Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:ğŸ” OCR í™œì„±í™”\n",
      "INFO:__main__:ğŸ“Š í…Œì´ë¸” êµ¬ì¡° ì¸ì‹ í™œì„±í™”\n",
      "INFO:__main__:ğŸ–¼ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ í™œì„±í™”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë¬¸ì„œ ì²˜ë¦¬ê¸°ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# docling ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "class DocumentProcessingConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        use_ocr: bool = True,\n",
    "        extract_tables: bool = True,\n",
    "        extract_images: bool = True,\n",
    "        extract_outline: bool = True,\n",
    "        min_chunk_size: int = 100,\n",
    "        max_chunk_size: int = 1000,\n",
    "        overlap_size: int = 50,\n",
    "        language: str = \"kor\",\n",
    "    ):\n",
    "        self.use_ocr = use_ocr\n",
    "        self.extract_tables = extract_tables\n",
    "        self.extract_images = extract_images\n",
    "        self.extract_outline = extract_outline\n",
    "        self.min_chunk_size = min_chunk_size\n",
    "        self.max_chunk_size = max_chunk_size\n",
    "        self.overlap_size = overlap_size\n",
    "        self.language = language\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            \"use_ocr\": self.use_ocr,\n",
    "            \"extract_tables\": self.extract_tables,\n",
    "            \"extract_images\": self.extract_images,\n",
    "            \"extract_outline\": self.extract_outline,\n",
    "            \"min_chunk_size\": self.min_chunk_size,\n",
    "            \"max_chunk_size\": self.max_chunk_size,\n",
    "            \"overlap_size\": self.overlap_size,\n",
    "            \"language\": self.language,\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, config_dict: Dict) -> \"DocumentProcessingConfig\":\n",
    "        return cls(**config_dict)\n",
    "\n",
    "def quick_layout_analysis(doc_analysis) -> dict:\n",
    "    \"\"\"\n",
    "    ë¹ ë¥¸ ë ˆì´ì•„ì›ƒ ë¶„ì„ìœ¼ë¡œ ë¬¸ì„œ êµ¬ì¡° íŒŒì•…\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # ë¬¸ì„œ íŠ¹ì„± ë¶„ì„\n",
    "        text_content = doc_analysis.export_to_text()\n",
    "        text_length = len(text_content.strip())\n",
    "\n",
    "        analysis = {\n",
    "            \"has_tables\": len(doc_analysis.tables) > 0,\n",
    "            \"table_count\": len(doc_analysis.tables),\n",
    "            \"has_images\": len(doc_analysis.pictures) > 0,\n",
    "            \"image_count\": len(doc_analysis.pictures),\n",
    "            \"is_scanned\": text_length < 100,  # í…ìŠ¤íŠ¸ê°€ ê±°ì˜ ì—†ìœ¼ë©´ ìŠ¤ìº” ë¬¸ì„œ\n",
    "            \"text_length\": text_length,\n",
    "            \"is_text_heavy\": text_length > 5000,  # í…ìŠ¤íŠ¸ ìœ„ì£¼ ë¬¸ì„œ\n",
    "            \"page_count\": len(doc_analysis.pages),\n",
    "        }\n",
    "\n",
    "        logger.info(\"ğŸ“Š ë¶„ì„ ê²°ê³¼:\")\n",
    "        logger.info(f\"  - í˜ì´ì§€: {analysis['page_count']}í˜ì´ì§€\")\n",
    "        logger.info(\n",
    "            f\"  - í…ìŠ¤íŠ¸: {analysis['text_length']}ì ({'ìŠ¤ìº”' if analysis['is_scanned'] else 'ë””ì§€í„¸'})\"\n",
    "        )\n",
    "        logger.info(f\"  - í…Œì´ë¸”: {analysis['table_count']}ê°œ\")\n",
    "        logger.info(f\"  - ì´ë¯¸ì§€: {analysis['image_count']}ê°œ\")\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"ë ˆì´ì•„ì›ƒ ë¶„ì„ ì‹¤íŒ¨: {str(e)}\")\n",
    "        return {\n",
    "            \"has_tables\": True,  # ì•ˆì „ì„ ìœ„í•´ True\n",
    "            \"table_count\": 0,\n",
    "            \"has_images\": True,  # ì•ˆì „ì„ ìœ„í•´ True\n",
    "            \"image_count\": 0,\n",
    "            \"is_scanned\": False,\n",
    "            \"text_length\": 0,\n",
    "            \"is_text_heavy\": False,\n",
    "            \"page_count\": 1,\n",
    "        }\n",
    "\n",
    "def create_optimized_pipeline(config: DocumentProcessingConfig) -> PdfPipelineOptions:\n",
    "    \"\"\"\n",
    "    ì„¤ì •ì— ë”°ë¼ ìµœì ì˜ íŒŒì´í”„ë¼ì¸ ì˜µì…˜ ìƒì„±\n",
    "    \"\"\"\n",
    "    options = PdfPipelineOptions()\n",
    "\n",
    "    # OCR ì„¤ì •\n",
    "    options.do_ocr = config.use_ocr\n",
    "    if config.use_ocr:\n",
    "        options.ocr_options.lang = [\"ko\", \"en\"]\n",
    "        logger.info(\"ğŸ” OCR í™œì„±í™”\")\n",
    "    else:\n",
    "        logger.info(\"ğŸ“„ OCR ë¹„í™œì„±í™”\")\n",
    "\n",
    "    # í…Œì´ë¸” ì²˜ë¦¬ ì„¤ì •\n",
    "    options.do_table_structure = config.extract_tables\n",
    "    if config.extract_tables:\n",
    "        options.table_structure_options.do_cell_matching = True\n",
    "        logger.info(\"ğŸ“Š í…Œì´ë¸” êµ¬ì¡° ì¸ì‹ í™œì„±í™”\")\n",
    "    else:\n",
    "        logger.info(\"ğŸ“ í…Œì´ë¸” êµ¬ì¡° ì¸ì‹ ë¹„í™œì„±í™”\")\n",
    "\n",
    "    # ì´ë¯¸ì§€ ì²˜ë¦¬ ì„¤ì •\n",
    "    options.generate_picture_images = config.extract_images\n",
    "    if config.extract_images:\n",
    "        options.images_scale = 2.0\n",
    "        logger.info(\"ğŸ–¼ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ í™œì„±í™”\")\n",
    "    else:\n",
    "        logger.info(\"ğŸ“ ì´ë¯¸ì§€ ì²˜ë¦¬ ë¹„í™œì„±í™”\")\n",
    "\n",
    "    # í˜ì´ì§€ ì´ë¯¸ì§€ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë¹„í™œì„±í™” (ìš©ëŸ‰ ì ˆì•½)\n",
    "    options.generate_page_images = False\n",
    "\n",
    "    return options\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, config: DocumentProcessingConfig):\n",
    "        self.config = config\n",
    "        self.pipeline = create_optimized_pipeline(config)\n",
    "        self.converter = DocumentConverter(\n",
    "            format_options={\n",
    "                InputFormat.PDF: PdfFormatOption(pipeline_options=self.pipeline)\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def process_document(self, pdf_path: Path) -> Dict:\n",
    "        \"\"\"ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "        results = {\n",
    "            \"metadata\": {},\n",
    "            \"tables\": [],\n",
    "            \"images\": [],\n",
    "            \"outline\": [],\n",
    "            \"chunks\": [],\n",
    "            \"errors\": []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # 1. PDF íŒŒì¼ ì²˜ë¦¬\n",
    "            start_time = time.time()\n",
    "            doc_analysis = self.converter.convert(str(pdf_path)).document\n",
    "            \n",
    "            # 2. ë ˆì´ì•„ì›ƒ ë¶„ì„\n",
    "            layout_info = quick_layout_analysis(doc_analysis)\n",
    "            results[\"metadata\"][\"layout\"] = layout_info\n",
    "            \n",
    "            # 3. í…Œì´ë¸” ì¶”ì¶œ\n",
    "            if self.config.extract_tables:\n",
    "                tables = self._extract_tables(doc_analysis)\n",
    "                results[\"tables\"] = tables\n",
    "            \n",
    "            # 4. ì´ë¯¸ì§€ ì¶”ì¶œ\n",
    "            if self.config.extract_images:\n",
    "                images = self._extract_images(doc_analysis)\n",
    "                results[\"images\"] = images\n",
    "            \n",
    "            # 5. ì•„ì›ƒë¼ì¸ ì¶”ì¶œ\n",
    "            if self.config.extract_outline:\n",
    "                outline = self._extract_outline(doc_analysis)\n",
    "                results[\"outline\"] = outline\n",
    "            \n",
    "            # 6. ì²­í¬ ìƒì„±\n",
    "            chunks = self._create_chunks(doc_analysis)\n",
    "            results[\"chunks\"] = chunks\n",
    "            \n",
    "            # 7. ë©”íƒ€ë°ì´í„° ì¶”ê°€\n",
    "            processing_time = time.time() - start_time\n",
    "            results[\"metadata\"].update({\n",
    "                \"filename\": pdf_path.name,\n",
    "                \"page_count\": len(doc_analysis.pages),\n",
    "                \"processing_time\": f\"{processing_time:.2f}ì´ˆ\",\n",
    "                \"processing_config\": self.config.to_dict()\n",
    "            })\n",
    "            \n",
    "            logger.info(f\"âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ ({processing_time:.2f}ì´ˆ)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}\"\n",
    "            logger.error(error_msg)\n",
    "            results[\"errors\"].append(error_msg)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _extract_tables(self, doc_analysis) -> List[Dict]:\n",
    "        \"\"\"í…Œì´ë¸” ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\"\"\"\n",
    "        tables = []\n",
    "        for page_idx, page in enumerate(doc_analysis.pages):\n",
    "            page_tables = page.find_tables()\n",
    "            for table_idx, table in enumerate(page_tables):\n",
    "                table_info = {\n",
    "                    \"page\": page_idx + 1,\n",
    "                    \"table_idx\": table_idx,\n",
    "                    \"content\": table.extract(),\n",
    "                    \"bbox\": table.bbox.to_dict()\n",
    "                }\n",
    "                tables.append(table_info)\n",
    "        return tables\n",
    "    \n",
    "    def _extract_images(self, doc_analysis) -> List[Dict]:\n",
    "        \"\"\"ì´ë¯¸ì§€ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\"\"\"\n",
    "        images = []\n",
    "        for page_idx, page in enumerate(doc_analysis.pages):\n",
    "            page_images = page.find_images()\n",
    "            for img_idx, img in enumerate(page_images):\n",
    "                img_info = {\n",
    "                    \"page\": page_idx + 1,\n",
    "                    \"image_idx\": img_idx,\n",
    "                    \"bbox\": img.bbox.to_dict()\n",
    "                }\n",
    "                images.append(img_info)\n",
    "        return images\n",
    "    \n",
    "    def _extract_outline(self, doc_analysis) -> List[Dict]:\n",
    "        \"\"\"ë¬¸ì„œ ì•„ì›ƒë¼ì¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.\"\"\"\n",
    "        outline = []\n",
    "        if hasattr(doc_analysis, \"outline\"):\n",
    "            for item in doc_analysis.outline:\n",
    "                outline_item = {\n",
    "                    \"title\": item.title,\n",
    "                    \"level\": item.level,\n",
    "                    \"page\": item.page\n",
    "                }\n",
    "                outline.append(outline_item)\n",
    "        return outline\n",
    "    \n",
    "    def _create_chunks(self, doc_analysis) -> List[Dict]:\n",
    "        \"\"\"í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.\"\"\"\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        current_page = 1\n",
    "        \n",
    "        for page_idx, page in enumerate(doc_analysis.pages):\n",
    "            page_text = page.extract_text()\n",
    "            words = page_text.split()\n",
    "            \n",
    "            for word in words:\n",
    "                if len(current_chunk) + len(word) + 1 <= self.config.max_chunk_size:\n",
    "                    current_chunk += word + \" \"\n",
    "                else:\n",
    "                    if len(current_chunk) >= self.config.min_chunk_size:\n",
    "                        chunk_info = {\n",
    "                            \"text\": current_chunk.strip(),\n",
    "                            \"page\": current_page,\n",
    "                            \"size\": len(current_chunk)\n",
    "                        }\n",
    "                        chunks.append(chunk_info)\n",
    "                    \n",
    "                    current_chunk = word + \" \"\n",
    "                    current_page = page_idx + 1\n",
    "            \n",
    "            # í˜ì´ì§€ ëì—ì„œ ì²­í¬ ì €ì¥\n",
    "            if len(current_chunk) >= self.config.min_chunk_size:\n",
    "                chunk_info = {\n",
    "                    \"text\": current_chunk.strip(),\n",
    "                    \"page\": current_page,\n",
    "                    \"size\": len(current_chunk)\n",
    "                }\n",
    "                chunks.append(chunk_info)\n",
    "                current_chunk = \"\"\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í”„ë¡œì„¸ì„œ ìƒì„±\n",
    "default_config = DocumentProcessingConfig()\n",
    "processor = DocumentProcessor(default_config)\n",
    "print(\"âœ… ë¬¸ì„œ ì²˜ë¦¬ê¸°ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "286a5d0d-a1e2-4901-ae38-97028c5edaf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T00:39:53.863580Z",
     "iopub.status.busy": "2025-07-14T00:39:53.863253Z",
     "iopub.status.idle": "2025-07-14T00:39:53.871161Z",
     "shell.execute_reply": "2025-07-14T00:39:53.870136Z",
     "shell.execute_reply.started": "2025-07-14T00:39:53.863552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ë¬¸ì„œ ì²˜ë¦¬ë¥¼ ì‹œì‘í•˜ë ¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì‹¤í–‰í•˜ì„¸ìš”:\n",
      "results = process_documents(config=custom_config)\n",
      "ë˜ëŠ” ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰:\n",
      "results = process_documents()\n"
     ]
    }
   ],
   "source": [
    "def process_documents(\n",
    "    config: Optional[Dict] = None,\n",
    "    input_dir: Optional[Path] = None,\n",
    "    output_dir: Optional[Path] = None\n",
    ") -> List[Dict]:\n",
    "    \"\"\"ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ì¼ê´„ ì²˜ë¦¬í•©ë‹ˆë‹¤.\"\"\"\n",
    "    # ì„¤ì • ë¡œë“œ\n",
    "    if config is None:\n",
    "        processing_config = default_config\n",
    "    else:\n",
    "        processing_config = DocumentProcessingConfig.from_dict(config)\n",
    "    \n",
    "    # ì…/ì¶œë ¥ ê²½ë¡œ ì„¤ì •\n",
    "    input_dir = input_dir or DOCS_DIR\n",
    "    output_dir = output_dir or PROCESSED_DIR\n",
    "    \n",
    "    # PDF íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°\n",
    "    pdf_files = list(input_dir.glob(\"*.pdf\"))\n",
    "    print(f\"ì´ {len(pdf_files)}ê°œì˜ PDF íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸\n",
    "    results = []\n",
    "    \n",
    "    # ê° íŒŒì¼ ì²˜ë¦¬\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\n{pdf_file.name} ì²˜ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        try:\n",
    "            # ë¬¸ì„œ ì²˜ë¦¬\n",
    "            result = processor.process_document(pdf_file)\n",
    "            \n",
    "            # ê²°ê³¼ ì €ì¥\n",
    "            output_file = output_dir / f\"{pdf_file.stem}_processed.json\"\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            print(f\"ì²˜ë¦¬ ì™„ë£Œ: {output_file}\")\n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}\")\n",
    "            results.append({\n",
    "                \"filename\": pdf_file.name,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "custom_config = {\n",
    "    \"use_ocr\": True,\n",
    "    \"extract_tables\": True,\n",
    "    \"extract_images\": True,\n",
    "    \"extract_outline\": True,\n",
    "    \"min_chunk_size\": 200,\n",
    "    \"max_chunk_size\": 800,\n",
    "    \"overlap_size\": 100,\n",
    "    \"language\": \"kor\"\n",
    "}\n",
    "\n",
    "print(\"\\në¬¸ì„œ ì²˜ë¦¬ë¥¼ ì‹œì‘í•˜ë ¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì‹¤í–‰í•˜ì„¸ìš”:\")\n",
    "print(\"results = process_documents(config=custom_config)\")\n",
    "print(\"ë˜ëŠ” ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰:\")\n",
    "print(\"results = process_documents()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b66e05f2-a17f-4303-9557-352adf200d85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T00:39:57.440119Z",
     "iopub.status.busy": "2025-07-14T00:39:57.439842Z",
     "iopub.status.idle": "2025-07-14T00:39:57.455791Z",
     "shell.execute_reply": "2025-07-14T00:39:57.455095Z",
     "shell.execute_reply.started": "2025-07-14T00:39:57.440098Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_tables_info(document) -> Dict[str, Any]:\n",
    "    \"\"\"ë¬¸ì„œì—ì„œ í…Œì´ë¸” ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\"\"\"\n",
    "    tables_info = []\n",
    "\n",
    "    if hasattr(document, \"tables\") and document.tables:\n",
    "        for idx, table in enumerate(document.tables):\n",
    "            try:\n",
    "                table_data = {\n",
    "                    \"table_index\": idx + 1,\n",
    "                    \"rows\": getattr(table, \"num_rows\", 0),\n",
    "                    \"cols\": getattr(table, \"num_cols\", 0),\n",
    "                    \"markdown\": table.export_to_markdown()\n",
    "                    if hasattr(table, \"export_to_markdown\")\n",
    "                    else \"\",\n",
    "                    \"html\": table.export_to_html()\n",
    "                    if hasattr(table, \"export_to_html\")\n",
    "                    else \"\",\n",
    "                }\n",
    "                # í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ\n",
    "                if hasattr(table, \"prov\") and table.prov:\n",
    "                    for prov in table.prov:\n",
    "                        if hasattr(prov, \"page_no\"):\n",
    "                            table_data[\"page\"] = prov.page_no\n",
    "                            break\n",
    "                tables_info.append(table_data)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"í…Œì´ë¸” {idx} ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "    return {\"count\": len(tables_info), \"data\": tables_info}\n",
    "\n",
    "\n",
    "def extract_images_info(document) -> Dict[str, Any]:\n",
    "    \"\"\"ë¬¸ì„œì—ì„œ ì´ë¯¸ì§€ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\"\"\"\n",
    "    images_info = []\n",
    "\n",
    "    if hasattr(document, \"pictures\") and document.pictures:\n",
    "        for idx, picture in enumerate(document.pictures):\n",
    "            try:\n",
    "                image_data = {\n",
    "                    \"image_index\": idx + 1,\n",
    "                    \"caption\": picture.caption_text(doc=document)\n",
    "                    if hasattr(picture, \"caption_text\")\n",
    "                    else \"\",\n",
    "                    \"has_image\": hasattr(picture, \"image\")\n",
    "                    and picture.image is not None,\n",
    "                }\n",
    "                # í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ\n",
    "                if hasattr(picture, \"prov\") and picture.prov:\n",
    "                    for prov in picture.prov:\n",
    "                        if hasattr(prov, \"page_no\"):\n",
    "                            image_data[\"page\"] = prov.page_no\n",
    "                            break\n",
    "                images_info.append(image_data)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"ì´ë¯¸ì§€ {idx} ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "    return {\n",
    "        \"count\": len(images_info),\n",
    "        \"data\": images_info,\n",
    "        \"processing\": {\"advanced_enabled\": False, \"ocr_enabled\": False},\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_outline_info(result) -> Dict[str, Any]:\n",
    "    \"\"\"ëª¨ë“  ë¬¸ì„œì—ì„œ ì•„ì›ƒë¼ì¸ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\"\"\"\n",
    "    outline_data = _extract_outline_from_text(result)\n",
    "    logger.info(f\"ì•„ì›ƒë¼ì¸ {len(outline_data)}ê°œ ì¶”ì¶œ ì™„ë£Œ\")\n",
    "\n",
    "    return {\n",
    "        \"enabled\": len(outline_data) > 0,\n",
    "        \"reason\": \"ì „ì²´ ë¬¸ì„œ ì•„ì›ƒë¼ì¸ ì¶”ì¶œ\",\n",
    "        \"data\": outline_data,\n",
    "    }\n",
    "\n",
    "\n",
    "def _extract_outline_from_text(result) -> List[Dict[str, Any]]:\n",
    "    \"\"\"section_header ë¼ë²¨ë§Œ í•„í„°ë§í•´ì„œ ì•„ì›ƒë¼ì¸ ì¶”ì¶œ (ë‚´ë¶€ í•¨ìˆ˜)\"\"\"\n",
    "    outline_data = []\n",
    "\n",
    "    try:\n",
    "        document = result.document\n",
    "\n",
    "        logger.info(\"=== section_header ë¼ë²¨ ê¸°ë°˜ ì•„ì›ƒë¼ì¸ ì¶”ì¶œ ===\")\n",
    "        logger.info(f\"document.pages íƒ€ì…: {type(document.pages)}\")\n",
    "        logger.info(\n",
    "            f\"document.pages ê¸¸ì´: {len(document.pages) if hasattr(document.pages, '__len__') else 'N/A'}\"\n",
    "        )\n",
    "\n",
    "        if hasattr(document, \"texts\"):\n",
    "            for item in document.texts:\n",
    "                # section_header ë¼ë²¨ë§Œ ì°¾ê¸°\n",
    "                if hasattr(item, \"label\") and item.label == \"section_header\":\n",
    "                    text_content = \"\"\n",
    "                    if hasattr(item, \"text\") and item.text:\n",
    "                        text_content = item.text.strip()\n",
    "\n",
    "                    if text_content:\n",
    "                        logger.info(f\"ì œëª© ë°œê²¬: '{text_content}'\")\n",
    "\n",
    "                        # ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ\n",
    "                        page_idx = 0\n",
    "                        y_position = 0.0\n",
    "                        bbox_info = {}\n",
    "\n",
    "                        if hasattr(item, \"prov\") and item.prov:\n",
    "                            for prov in item.prov:\n",
    "                                if hasattr(prov, \"page_no\"):\n",
    "                                    page_idx = prov.page_no - 1  # 1-based to 0-based\n",
    "\n",
    "                                if hasattr(prov, \"bbox\"):\n",
    "                                    bbox_info, y_position = _extract_bbox_info(\n",
    "                                        prov.bbox, document, page_idx\n",
    "                                    )\n",
    "                                    break\n",
    "\n",
    "                        # ì²« ë²ˆì§¸ë©´ introduction, ë‚˜ë¨¸ì§€ëŠ” main body\n",
    "                        element_type = (\n",
    "                            \"introduction\" if len(outline_data) == 0 else \"main body\"\n",
    "                        )\n",
    "\n",
    "                        outline_item = {\n",
    "                            \"title\": text_content,\n",
    "                            \"pageIndex\": page_idx,\n",
    "                            \"y\": y_position,\n",
    "                            \"bbox\": bbox_info,\n",
    "                            \"dest\": f\"Section_{len(outline_data) + 2}\",\n",
    "                            \"size\": 23.3333,\n",
    "                            \"type\": element_type,\n",
    "                            \"items\": [],\n",
    "                        }\n",
    "                        outline_data.append(outline_item)\n",
    "\n",
    "        logger.info(f\"section_headerë¡œ ì°¾ì€ ì•„ì›ƒë¼ì¸: {len(outline_data)}ê°œ\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"ì•„ì›ƒë¼ì¸ ì¶”ì¶œ ì—ëŸ¬: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    return outline_data\n",
    "\n",
    "\n",
    "def _extract_bbox_info(bbox, document, page_idx: int) -> tuple[Dict[str, Any], float]:\n",
    "    \"\"\"bbox ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³  ì •ê·œí™”í•©ë‹ˆë‹¤.\"\"\"\n",
    "    # í˜ì´ì§€ í¬ê¸° êµ¬í•˜ê¸° (ì•ˆì „í•˜ê²Œ)\n",
    "    page_width = 595  # A4 ê¸°ë³¸ ë„ˆë¹„\n",
    "    page_height = 842  # A4 ê¸°ë³¸ ë†’ì´\n",
    "    y_position = 0.0\n",
    "\n",
    "    try:\n",
    "        if hasattr(document, \"pages\") and page_idx < len(document.pages):\n",
    "            # ë‹¤ì–‘í•œ ì ‘ê·¼ ë°©ì‹ ì‹œë„\n",
    "            if isinstance(document.pages, list):\n",
    "                page = document.pages[page_idx]\n",
    "            elif isinstance(document.pages, dict):\n",
    "                page = document.pages.get(page_idx) or document.pages.get(str(page_idx))\n",
    "            else:\n",
    "                page = None\n",
    "\n",
    "            if page and hasattr(page, \"size\"):\n",
    "                if hasattr(page.size, \"height\"):\n",
    "                    page_height = page.size.height\n",
    "                if hasattr(page.size, \"width\"):\n",
    "                    page_width = page.size.width\n",
    "    except (KeyError, IndexError, AttributeError) as e:\n",
    "        logger.warning(f\"í˜ì´ì§€ ì •ë³´ ì ‘ê·¼ ì˜¤ë¥˜: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©\")\n",
    "\n",
    "    # bbox ì „ì²´ ì •ë³´ ìˆ˜ì§‘\n",
    "    bbox_info = {}\n",
    "\n",
    "    # ë‹¤ì–‘í•œ bbox ì†ì„± í™•ì¸\n",
    "    if (\n",
    "        hasattr(bbox, \"l\")\n",
    "        and hasattr(bbox, \"r\")\n",
    "        and hasattr(bbox, \"t\")\n",
    "        and hasattr(bbox, \"b\")\n",
    "    ):\n",
    "        # l, r, t, b í˜•ì‹\n",
    "        bbox_info = {\n",
    "            \"left\": bbox.l / page_width,\n",
    "            \"right\": bbox.r / page_width,\n",
    "            \"top\": bbox.t / page_height,\n",
    "            \"bottom\": bbox.b / page_height,\n",
    "            \"width\": (bbox.r - bbox.l) / page_width,\n",
    "            \"height\": (bbox.b - bbox.t) / page_height,\n",
    "        }\n",
    "        y_position = bbox.t / page_height\n",
    "\n",
    "    return bbox_info, y_position\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de33736-f2ff-48f9-8886-7f941105645b",
   "metadata": {},
   "source": [
    "## ğŸ“‹ 3. ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0028bc0-c2cc-436c-955d-a07cfcd5af32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T00:39:59.437447Z",
     "iopub.status.busy": "2025-07-14T00:39:59.436762Z",
     "iopub.status.idle": "2025-07-14T00:39:59.448080Z",
     "shell.execute_reply": "2025-07-14T00:39:59.447326Z",
     "shell.execute_reply.started": "2025-07-14T00:39:59.437414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def process_documents():\n",
    "    \"\"\"ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\"\"\"\n",
    "    logger.info(\"ğŸš€ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘\")\n",
    "    \n",
    "    # 1. docs í´ë”ì˜ PDF íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°\n",
    "    pdf_files = list(DOCS_DIR.glob(\"*.pdf\"))\n",
    "    logger.info(f\"ğŸ“ ì²˜ë¦¬í•  PDF íŒŒì¼: {len(pdf_files)}ê°œ\")\n",
    "    \n",
    "    # 2. ê° PDF íŒŒì¼ ì²˜ë¦¬\n",
    "    for pdf_file in pdf_files:\n",
    "        try:\n",
    "            logger.info(f\"\\n{'='*50}\")\n",
    "            logger.info(f\"ğŸ“„ ì²˜ë¦¬ ì‹œì‘: {pdf_file.name}\")\n",
    "            \n",
    "            # 2.1. PDF íŒŒì¼ ë¶„ì„\n",
    "            result = pdf_service.analyze_pdf(str(pdf_file))\n",
    "            \n",
    "            # 2.2. ì²­í¬ ìƒì„±\n",
    "            chunks = []\n",
    "            \n",
    "            # 2.2.1. í…ìŠ¤íŠ¸ ê¸°ë°˜ ì²­í¬\n",
    "            text_content = result[\"markdown_content\"]\n",
    "            if text_content:\n",
    "                # ì„¹ì…˜ ë‹¨ìœ„ë¡œ ë¶„í• \n",
    "                sections = text_content.split(\"\\n## \")\n",
    "                for section in sections:\n",
    "                    if section.strip():\n",
    "                        chunks.append({\n",
    "                            \"type\": \"text\",\n",
    "                            \"content\": section.strip(),\n",
    "                            \"metadata\": {\n",
    "                                \"source\": pdf_file.name,\n",
    "                                \"type\": \"text_section\"\n",
    "                            }\n",
    "                        })\n",
    "            \n",
    "            # 2.2.2. í…Œì´ë¸” ê¸°ë°˜ ì²­í¬\n",
    "            for table in result[\"tables\"][\"data\"]:\n",
    "                if table[\"markdown\"]:\n",
    "                    chunks.append({\n",
    "                        \"type\": \"table\",\n",
    "                        \"content\": table[\"markdown\"],\n",
    "                        \"metadata\": {\n",
    "                            \"source\": pdf_file.name,\n",
    "                            \"type\": \"table\",\n",
    "                            \"page\": table.get(\"page\", 1),\n",
    "                            \"rows\": table[\"rows\"],\n",
    "                            \"cols\": table[\"cols\"]\n",
    "                        }\n",
    "                    })\n",
    "            \n",
    "            # 2.2.3. ì´ë¯¸ì§€ ê¸°ë°˜ ì²­í¬ (ìº¡ì…˜ì´ ìˆëŠ” ê²½ìš°)\n",
    "            for image in result[\"images\"][\"data\"]:\n",
    "                if image[\"caption\"]:\n",
    "                    chunks.append({\n",
    "                        \"type\": \"image\",\n",
    "                        \"content\": image[\"caption\"],\n",
    "                        \"metadata\": {\n",
    "                            \"source\": pdf_file.name,\n",
    "                            \"type\": \"image_caption\",\n",
    "                            \"page\": image.get(\"page\", 1)\n",
    "                        }\n",
    "                    })\n",
    "            \n",
    "            # 2.3. ì²­í¬ ì €ì¥\n",
    "            chunk_file = CHUNKS_DIR / f\"chunks_{pdf_file.stem}.json\"\n",
    "            with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            logger.info(f\"âœ… ì²­í¬ ìƒì„± ì™„ë£Œ: {len(chunks)}ê°œ\")\n",
    "            logger.info(f\"  - ì €ì¥ ìœ„ì¹˜: {chunk_file}\")\n",
    "            \n",
    "            # 2.4. ë²¡í„° ì €ì¥ (í–¥í›„ êµ¬í˜„)\n",
    "            # TODO: ë²¡í„° ì €ì¥ ë¡œì§ êµ¬í˜„\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ ì²˜ë¦¬ ì‹¤íŒ¨ ({pdf_file.name}): {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    logger.info(\"\\nğŸ‰ ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "# íŒŒì´í”„ë¼ì¸ ì¤€ë¹„\n",
    "print(\"âœ… íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1ac0b8-f0fd-4053-a8ad-8f8c7d5ca06c",
   "metadata": {},
   "source": [
    "## ğŸš€ 4. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5c5d612-e50d-4a69-9205-95b73f565ea5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T00:40:01.408816Z",
     "iopub.status.busy": "2025-07-14T00:40:01.407990Z",
     "iopub.status.idle": "2025-07-14T00:40:01.427001Z",
     "shell.execute_reply": "2025-07-14T00:40:01.417474Z",
     "shell.execute_reply.started": "2025-07-14T00:40:01.408788Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:ğŸš€ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘\n",
      "INFO:__main__:ğŸ“ ì²˜ë¦¬í•  PDF íŒŒì¼: 0ê°œ\n",
      "INFO:__main__:\n",
      "ğŸ‰ ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "process_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508a33bd-ff18-4a00-899d-9731e7ef37cc",
   "metadata": {},
   "source": [
    "## ğŸ§ª 3. RAG í‰ê°€ í´ë˜ìŠ¤ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6632c47c-0664-4e32-92df-538ddf0ba4f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T00:42:20.958785Z",
     "iopub.status.busy": "2025-07-14T00:42:20.958392Z",
     "iopub.status.idle": "2025-07-14T00:42:20.981213Z",
     "shell.execute_reply": "2025-07-14T00:42:20.980402Z",
     "shell.execute_reply.started": "2025-07-14T00:42:20.958760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SmartCLMRAGEvaluator í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# Smart CLM ì²­í‚¹ ë¡œì§\n",
    "class HierarchicalChunker:\n",
    "    \"\"\"\n",
    "    Doclingì—ì„œ ì¶”ì¶œí•œ ë§ˆí¬ë‹¤ìš´ì„ ê¸°ë°˜ìœ¼ë¡œ ê³„ì¸µì  ì²­í‚¹ì„ ìˆ˜í–‰í•˜ëŠ” í´ë˜ìŠ¤\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # í—¤ë” ì •ì˜ (í—¤ë” ë ˆë²¨ê³¼ ë©”íƒ€ë°ì´í„° í‚¤ ì´ë¦„)\n",
    "        self.headers_to_split_on = [\n",
    "            (\"#\", \"Header 1\"),\n",
    "            (\"##\", \"Header 2\"),\n",
    "            (\"###\", \"Header 3\"),\n",
    "            (\"####\", \"Header 4\"),\n",
    "        ]\n",
    "        \n",
    "        # Child ë¬¸ì„œìš© í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì •\n",
    "        self.chunk_size = 500\n",
    "        self.chunk_overlap = 50\n",
    "\n",
    "    def chunk_markdown(self, markdown_content: str, filename: str = \"document\") -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸ ë¥¼ ê³„ì¸µì ìœ¼ë¡œ ì²­í‚¹í•©ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "        # ë‹¨ìˆœí™”ëœ ì²­í‚¹ ë¡œì§ (ì‹¤ì œ êµ¬í˜„ì„ ê¸°ë°˜ìœ¼ë¡œ ê°„ì†Œí™”)\n",
    "        lines = markdown_content.split('\\n')\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_headers = {}\n",
    "        \n",
    "        for line in lines:\n",
    "            # í—¤ë” ê°ì§€\n",
    "            if line.startswith('#'):\n",
    "                if current_chunk:\n",
    "                    # ì´ì „ ì²­í¬ ì €ì¥\n",
    "                    chunk_content = '\\n'.join(current_chunk)\n",
    "                    if len(chunk_content.strip()) > 0:\n",
    "                        chunks.append({\n",
    "                            'content': chunk_content,\n",
    "                            'headers': current_headers.copy(),\n",
    "                            'char_count': len(chunk_content)\n",
    "                        })\n",
    "                    current_chunk = []\n",
    "                \n",
    "                # í—¤ë” ë ˆë²¨ íŒŒì•…\n",
    "                level = len(line) - len(line.lstrip('#'))\n",
    "                header_text = line.lstrip('#').strip()\n",
    "                current_headers[f'header_{level}'] = header_text\n",
    "                \n",
    "                # í•˜ìœ„ ë ˆë²¨ í—¤ë” ì´ˆê¸°í™”\n",
    "                for i in range(level + 1, 5):\n",
    "                    if f'header_{i}' in current_headers:\n",
    "                        del current_headers[f'header_{i}']\n",
    "            \n",
    "            current_chunk.append(line)\n",
    "        \n",
    "        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥\n",
    "        if current_chunk:\n",
    "            chunk_content = '\\n'.join(current_chunk)\n",
    "            if len(chunk_content.strip()) > 0:\n",
    "                chunks.append({\n",
    "                    'content': chunk_content,\n",
    "                    'headers': current_headers.copy(),\n",
    "                    'char_count': len(chunk_content)\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'filename': filename,\n",
    "            'chunks': chunks,\n",
    "            'summary': {\n",
    "                'total_chunks': len(chunks),\n",
    "                'average_size': sum(c['char_count'] for c in chunks) // len(chunks) if chunks else 0\n",
    "            }\n",
    "        }\n",
    "\n",
    "class SmartCLMRAGEvaluator:\n",
    "    \"\"\"\n",
    "    Smart CLM RAG ì‹œìŠ¤í…œ ì „ìš© í‰ê°€ê¸°\n",
    "    \n",
    "    ê³„ì•½ì„œ ë„ë©”ì¸ì— íŠ¹í™”ëœ RAG í‰ê°€ ë©”íŠ¸ë¦­ê³¼ ë°©ë²•ë¡ ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    \n",
    "    # def __init__(self, api_client: SmartCLMAPIClient):\n",
    "    #     self.api_client = api_client\n",
    "    #     self.chunker = HierarchicalChunker()\n",
    "    #     self.evaluation_results = {}\n",
    "        \n",
    "    def evaluate_chunking_quality(self, contract_id: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        ì²­í‚¹ í’ˆì§ˆ í‰ê°€\n",
    "        \n",
    "        Args:\n",
    "            contract_id: í‰ê°€í•  ê³„ì•½ì„œ ID\n",
    "            \n",
    "        Returns:\n",
    "            ì²­í‚¹ í’ˆì§ˆ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ“Š ê³„ì•½ì„œ {contract_id} ì²­í‚¹ í’ˆì§ˆ í‰ê°€ ì‹œì‘\")\n",
    "        \n",
    "        try:\n",
    "            # APIë¥¼ í†µí•´ ì²­í¬ ë°ì´í„° ì¡°íšŒ\n",
    "            chunks_data = self.api_client.get_contract_chunks(contract_id)\n",
    "            \n",
    "            if not chunks_data:\n",
    "                print(f\"âš ï¸ APIì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª©ì—… ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "                chunks_data = self._create_mock_chunks_data(contract_id)\n",
    "            \n",
    "            if not chunks_data:\n",
    "                print(f\"âš ï¸ ê³„ì•½ì„œ {contract_id}ì˜ ì²­í¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤\")\n",
    "                return {\"error\": \"No chunks found\"}\n",
    "            \n",
    "            # ì²­í‚¹ ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "            char_counts = [chunk.get('char_count', 500) for chunk in chunks_data]\n",
    "            \n",
    "            metrics = {\n",
    "                \"total_chunks\": len(chunks_data),\n",
    "                \"parent_chunks\": len([c for c in chunks_data if c.get('chunk_type') == \"parent\"]),\n",
    "                \"child_chunks\": len([c for c in chunks_data if c.get('chunk_type') == \"child\"]),\n",
    "                \"avg_chunk_length\": float(np.mean(char_counts)) if char_counts else 0,\n",
    "                \"std_chunk_length\": float(np.std(char_counts)) if char_counts else 0,\n",
    "                \"chunks_with_embeddings\": len([c for c in chunks_data if c.get('embedding') is not None]),\n",
    "                \"hierarchical_coverage\": self._calculate_hierarchical_coverage_from_data(chunks_data)\n",
    "            }\n",
    "            \n",
    "            print(f\"âœ… ì²­í‚¹ í‰ê°€ ì™„ë£Œ: {metrics['total_chunks']}ê°œ ì²­í¬, ì„ë² ë”© {metrics['chunks_with_embeddings']}ê°œ\")\n",
    "            return metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì²­í‚¹ í‰ê°€ ì˜¤ë¥˜: {e}\")\n",
    "            return {\"error\": str(e)}\n",
    "    \n",
    "    def _create_mock_chunks_data(self, contract_id: int) -> List[Dict[str, Any]]:\n",
    "        \"\"\"ëª©ì—… ì²­í¬ ë°ì´í„° ìƒì„± (ë”•ì…”ë„ˆë¦¬ í˜•íƒœ)\"\"\"\n",
    "        mock_chunks = []\n",
    "        for i in range(15):  # 15ê°œ ì²­í¬ ìƒì„±\n",
    "            chunk_data = {\n",
    "                'id': i + 1,\n",
    "                'contract_id': contract_id,\n",
    "                'chunk_type': 'parent' if i < 3 else 'child',\n",
    "                'char_count': int(np.random.randint(300, 800)),\n",
    "                'embedding': [0.1] * 1024 if i < 10 else None,\n",
    "                'header_1': f\"ì œ{i+1}ì¡°\" if i < 5 else None,\n",
    "                'header_2': f\"í•­ëª© {i+1}\" if i < 8 else None,\n",
    "                'header_3': None,\n",
    "                'header_4': None,\n",
    "                'content': f\"ê³„ì•½ì„œ {contract_id} ì²­í¬ {i+1} ë‚´ìš©ì…ë‹ˆë‹¤. ì´ê²ƒì€ ëª©ì—… ë°ì´í„°ì…ë‹ˆë‹¤.\"\n",
    "            }\n",
    "            mock_chunks.append(chunk_data)\n",
    "        return mock_chunks\n",
    "    \n",
    "    def _calculate_hierarchical_coverage_from_data(self, chunks_data: List[Dict[str, Any]]) -> float:\n",
    "        \"\"\"ê³„ì¸µ êµ¬ì¡° ì»¤ë²„ë¦¬ì§€ ê³„ì‚° (ë”•ì…”ë„ˆë¦¬ ë°ì´í„°ìš©)\"\"\"\n",
    "        headers = [\"header_1\", \"header_2\", \"header_3\", \"header_4\"]\n",
    "        coverage_scores = []\n",
    "        \n",
    "        for header in headers:\n",
    "            chunks_with_header = len([c for c in chunks_data if c.get(header) is not None])\n",
    "            coverage = chunks_with_header / len(chunks_data) if chunks_data else 0\n",
    "            coverage_scores.append(coverage)\n",
    "        \n",
    "        return float(np.mean(coverage_scores))\n",
    "    \n",
    "    def evaluate_retrieval_accuracy(self, test_queries: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        ê²€ìƒ‰ ì •í™•ë„ í‰ê°€\n",
    "        \n",
    "        Args:\n",
    "            test_queries: í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ [{\"query\": \"...\", \"expected_chunks\": [...], \"contract_id\": ...}]\n",
    "            \n",
    "        Returns:\n",
    "            ê²€ìƒ‰ ì •í™•ë„ ë©”íŠ¸ë¦­\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ” {len(test_queries)}ê°œ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ ì •í™•ë„ í‰ê°€ ì‹œì‘\")\n",
    "        \n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        f1_scores = []\n",
    "        \n",
    "        for i, test_case in enumerate(test_queries):\n",
    "            query = test_case[\"query\"]\n",
    "            expected_chunks = set(test_case.get(\"expected_chunks\", []))\n",
    "            contract_id = test_case.get(\"contract_id\", 1)\n",
    "            \n",
    "            # API ë˜ëŠ” ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•œ ë²¡í„° ê²€ìƒ‰\n",
    "            retrieved_chunks = self._perform_vector_search(query, contract_id)\n",
    "            retrieved_chunk_ids = set([str(chunk.get('id', i)) for chunk in retrieved_chunks])\n",
    "            \n",
    "            # ì •ë°€ë„, ì¬í˜„ìœ¨, F1 ê³„ì‚°\n",
    "            precision = 0.0\n",
    "            recall = 0.0\n",
    "            f1 = 0.0\n",
    "            \n",
    "            if retrieved_chunk_ids:\n",
    "                precision = len(expected_chunks & retrieved_chunk_ids) / len(retrieved_chunk_ids)\n",
    "                precision_scores.append(precision)\n",
    "            \n",
    "            if expected_chunks:\n",
    "                recall = len(expected_chunks & retrieved_chunk_ids) / len(expected_chunks)\n",
    "                recall_scores.append(recall)\n",
    "            \n",
    "            if precision > 0 or recall > 0:\n",
    "                f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "                f1_scores.append(f1)\n",
    "            \n",
    "            print(f\"  ì¿¼ë¦¬ {i+1}: P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}\")\n",
    "        \n",
    "        results = {\n",
    "            \"avg_precision\": float(np.mean(precision_scores)) if precision_scores else 0.0,\n",
    "            \"avg_recall\": float(np.mean(recall_scores)) if recall_scores else 0.0,\n",
    "            \"avg_f1\": float(np.mean(f1_scores)) if f1_scores else 0.0,\n",
    "            \"total_queries\": len(test_queries)\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… ê²€ìƒ‰ í‰ê°€ ì™„ë£Œ: P={results['avg_precision']:.3f}, R={results['avg_recall']:.3f}, F1={results['avg_f1']:.3f}\")\n",
    "        return results\n",
    "    \n",
    "    def _perform_vector_search(self, query: str, contract_id: int, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰ (API ìš°ì„ , ì‹¤íŒ¨ì‹œ ëª©ì—… ë°ì´í„°)\"\"\"\n",
    "        # ë¨¼ì € APIë¥¼ í†µí•œ ì‹¤ì œ ê²€ìƒ‰ ì‹œë„\n",
    "        search_results = self.api_client.search_chunks(query, contract_id, top_k)\n",
    "        \n",
    "        if search_results:\n",
    "            return search_results\n",
    "        else:\n",
    "            # API ì‹¤íŒ¨ì‹œ ëª©ì—… ë°ì´í„° ë°˜í™˜\n",
    "            print(f\"  âš ï¸ API ê²€ìƒ‰ ì‹¤íŒ¨, ëª©ì—… ë°ì´í„° ì‚¬ìš©\")\n",
    "            mock_chunks = self._create_mock_chunks_data(contract_id)\n",
    "            return mock_chunks[:top_k]\n",
    "\n",
    "print(\"âœ… SmartCLMRAGEvaluator í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe70428-49de-45c2-a694-98008d72777c",
   "metadata": {},
   "source": [
    "## ğŸ“ 4. í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ë° í‰ê°€ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "548bfe28-442c-46d6-964d-4d572e2231ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T00:42:33.587105Z",
     "iopub.status.busy": "2025-07-14T00:42:33.586789Z",
     "iopub.status.idle": "2025-07-14T00:42:33.593339Z",
     "shell.execute_reply": "2025-07-14T00:42:33.592639Z",
     "shell.execute_reply.started": "2025-07-14T00:42:33.587082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ìƒì„±ëœ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: 5ê°œ\n",
      "  1. [ê¸°ë³¸ì •ë³´] ê³„ì•½ ê¸°ê°„ì€ ì–¸ì œê¹Œì§€ì¸ê°€ìš”?...\n",
      "  2. [ë¦¬ìŠ¤í¬] ìœ„ì•½ê¸ˆ ì¡°í•­ì´ ìˆë‚˜ìš”?...\n",
      "  3. [ì¬ë¬´] ì§€ê¸‰ ì¡°ê±´ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?...\n",
      "  4. [í•´ì§€] ê³„ì•½ í•´ì§€ ì‚¬ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?...\n",
      "  5. [ë²•ì ì±…ì„] ì†í•´ë°°ìƒ ì±…ì„ì€ ëˆ„êµ¬ì—ê²Œ ìˆë‚˜ìš”?...\n"
     ]
    }
   ],
   "source": [
    "# RAG í‰ê°€ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (API í´ë¼ì´ì–¸íŠ¸ì™€ í•¨ê»˜)\n",
    "# evaluator = SmartCLMRAGEvaluator(api_client)\n",
    "\n",
    "# ê³„ì•½ì„œ ë„ë©”ì¸ íŠ¹í™” í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìƒì„±\n",
    "test_queries = [\n",
    "    {\n",
    "        \"query\": \"ê³„ì•½ ê¸°ê°„ì€ ì–¸ì œê¹Œì§€ì¸ê°€ìš”?\",\n",
    "        \"expected_chunks\": [\"1\", \"2\", \"3\"],\n",
    "        \"contract_id\": 1,\n",
    "        \"category\": \"ê¸°ë³¸ì •ë³´\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"ìœ„ì•½ê¸ˆ ì¡°í•­ì´ ìˆë‚˜ìš”?\",\n",
    "        \"expected_chunks\": [\"5\", \"6\"],\n",
    "        \"contract_id\": 1,\n",
    "        \"category\": \"ë¦¬ìŠ¤í¬\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"ì§€ê¸‰ ì¡°ê±´ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?\",\n",
    "        \"expected_chunks\": [\"4\", \"7\", \"8\"],\n",
    "        \"contract_id\": 1,\n",
    "        \"category\": \"ì¬ë¬´\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"ê³„ì•½ í•´ì§€ ì‚¬ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "        \"expected_chunks\": [\"9\", \"10\"],\n",
    "        \"contract_id\": 1,\n",
    "        \"category\": \"í•´ì§€\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"ì†í•´ë°°ìƒ ì±…ì„ì€ ëˆ„êµ¬ì—ê²Œ ìˆë‚˜ìš”?\",\n",
    "        \"expected_chunks\": [\"11\", \"12\"],\n",
    "        \"contract_id\": 1,\n",
    "        \"category\": \"ë²•ì ì±…ì„\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"ğŸ“Š ìƒì„±ëœ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {len(test_queries)}ê°œ\")\n",
    "for i, query in enumerate(test_queries):\n",
    "    print(f\"  {i+1}. [{query['category']}] {query['query'][:30]}...\")\n",
    "\n",
    "# í‰ê°€ ê²°ê³¼ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬\n",
    "evaluation_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73e2a426-8b47-4d80-b124-6841b80ea594",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T00:42:44.176285Z",
     "iopub.status.busy": "2025-07-14T00:42:44.175803Z",
     "iopub.status.idle": "2025-07-14T00:42:44.198500Z",
     "shell.execute_reply": "2025-07-14T00:42:44.197390Z",
     "shell.execute_reply.started": "2025-07-14T00:42:44.176259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” === ì²­í‚¹ í’ˆì§ˆ í‰ê°€ ì‹œì‘ ===\n",
      "\n",
      "ğŸ“Š ì²­í‚¹ í‰ê°€ ê²°ê³¼:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'chunking_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# chunking_results = evaluator.evaluate_chunking_quality(contract_id=1)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# evaluation_results['chunking'] = chunking_results\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mğŸ“Š ì²­í‚¹ í‰ê°€ ê²°ê³¼:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - ì´ ì²­í¬ ìˆ˜: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mchunking_results\u001b[49m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_chunks\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - Parent ì²­í¬: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunking_results\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparent_chunks\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - Child ì²­í¬: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunking_results\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchild_chunks\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chunking_results' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. ì²­í‚¹ í’ˆì§ˆ í‰ê°€ ì‹¤í–‰\n",
    "print(\"ğŸ” === ì²­í‚¹ í’ˆì§ˆ í‰ê°€ ì‹œì‘ ===\")\n",
    "chunking_results = evaluator.evaluate_chunking_quality(contract_id=1)\n",
    "evaluation_results['chunking'] = chunking_results\n",
    "\n",
    "print(\"\\nğŸ“Š ì²­í‚¹ í‰ê°€ ê²°ê³¼:\")\n",
    "print(f\"  - ì´ ì²­í¬ ìˆ˜: {chunking_results.get('total_chunks', 0)}\")\n",
    "print(f\"  - Parent ì²­í¬: {chunking_results.get('parent_chunks', 0)}\")\n",
    "print(f\"  - Child ì²­í¬: {chunking_results.get('child_chunks', 0)}\")\n",
    "print(f\"  - í‰ê·  ì²­í¬ ê¸¸ì´: {chunking_results.get('avg_chunk_length', 0):.1f}ì\")\n",
    "print(f\"  - ì„ë² ë”© ë³´ìœ  ì²­í¬: {chunking_results.get('chunks_with_embeddings', 0)}\")\n",
    "print(f\"  - ê³„ì¸µ êµ¬ì¡° ì»¤ë²„ë¦¬ì§€: {chunking_results.get('hierarchical_coverage', 0):.3f}\")\n",
    "\n",
    "# ì²­í‚¹ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°\n",
    "chunk_quality_score = 0\n",
    "if chunking_results.get('total_chunks', 0) > 0:\n",
    "    embedding_ratio = chunking_results.get('chunks_with_embeddings', 0) / chunking_results.get('total_chunks', 1)\n",
    "    hierarchy_score = chunking_results.get('hierarchical_coverage', 0)\n",
    "    chunk_quality_score = (embedding_ratio * 0.6 + hierarchy_score * 0.4) * 100\n",
    "\n",
    "print(f\"\\nâœ… ì²­í‚¹ í’ˆì§ˆ ì¢…í•© ì ìˆ˜: {chunk_quality_score:.1f}/100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3dc428b-0541-408a-b52f-57097ea2ba9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T03:59:01.652314Z",
     "iopub.status.busy": "2025-07-11T03:59:01.651848Z",
     "iopub.status.idle": "2025-07-11T03:59:01.851701Z",
     "shell.execute_reply": "2025-07-11T03:59:01.850822Z",
     "shell.execute_reply.started": "2025-07-11T03:59:01.652255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?.\n",
      "See 'docker run --help'.\n"
     ]
    }
   ],
   "source": [
    "!docker run pgvector/pgvector:0.8.0-pg16 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afa7a910-a338-440e-b780-229cfbf6dda0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T03:58:02.080939Z",
     "iopub.status.busy": "2025-07-11T03:58:02.080543Z",
     "iopub.status.idle": "2025-07-11T03:58:02.096387Z",
     "shell.execute_reply": "2025-07-11T03:58:02.095674Z",
     "shell.execute_reply.started": "2025-07-11T03:58:02.080902Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sagemaker-user'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096811e2-f938-4c5a-8473-efa80abdd3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
