{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Smart CLM RAG 파이프라인\n",
        "\n",
        "근거자료 저장 및 검색 성능 테스트를 위한 통합 노트북\n",
        "\n",
        "## 주요 기능\n",
        "1. **Step 1**: 근거자료 저장 - 문서 파일들을 분석하고 데이터베이스에 저장\n",
        "2. **Step 2**: 검색 테스트 - 저장된 문서들에 대한 검색 기능 테스트  \n",
        "3. **Step 3**: 계약서 검토 테스트 - 3단계 체인 방식으로 계약서 위법 조항 분석\n",
        "4. **Step 4**: 결과 시각화 및 분석\n",
        "\n",
        "## 사용 방법\n",
        "각 셀을 순서대로 실행하여 파이프라인을 단계별로 수행할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 환경 설정 및 라이브러리 임포트\n",
        "import asyncio\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# 환경변수 로딩\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "from typing import Dict, List, Any, Optional\n",
        "import httpx\n",
        "from dataclasses import dataclass\n",
        "import pandas as pd\n",
        "\n",
        "# 로깅 설정\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
        "    handlers=[\n",
        "        logging.StreamHandler(sys.stdout),\n",
        "        logging.FileHandler(\"rag_pipeline.log\")\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# 프로젝트 루트 디렉토리 설정\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "DOCS_ROOT = PROJECT_ROOT / \"docs\"\n",
        "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
        "PROCESSED_DIR = PROJECT_ROOT / \"processed\"\n",
        "\n",
        "# 결과 저장 디렉토리 생성\n",
        "RESULTS_DIR.mkdir(exist_ok=True)\n",
        "PROCESSED_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"✅ 환경 설정 완료\")\n",
        "print(f\"📁 프로젝트 루트: {PROJECT_ROOT}\")\n",
        "print(f\"📁 문서 폴더: {DOCS_ROOT}\")\n",
        "print(f\"📁 결과 폴더: {RESULTS_DIR}\")\n",
        "print(f\"📁 처리 폴더: {PROCESSED_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 문서 타입 매핑 및 데이터 클래스 정의\n",
        "\n",
        "# 문서 타입 매핑 (폴더 경로 -> 문서 타입)\n",
        "DOCUMENT_TYPE_MAPPING = {\n",
        "    \"근거 자료/법령\": \"law\",  # 법령 파일들\n",
        "    \"근거자료/법령\": \"law\",  # 법령 파일들 (공백 없는 버전)\n",
        "    \"근거 자료/체결계약\": \"executed_contract\",  # 체결된 계약서들\n",
        "    \"근거자료/체결계약\": \"executed_contract\",  # 체결된 계약서들 (공백 없는 버전)\n",
        "    \"근거 자료/표준계약서\": \"standard_contract\",  # 표준 계약서 템플릿들\n",
        "    \"근거자료/표준계약서\": \"standard_contract\",  # 표준 계약서 템플릿들 (공백 없는 버전)\n",
        "    \"1. NDA\": \"standard_contract\",  # NDA 표준계약서 (기존 폴더)\n",
        "    \"2. 제품(서비스) 판매\": \"standard_contract\",  # 판매계약서 표준 (기존 폴더)\n",
        "}\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ProcessingResult:\n",
        "    \"\"\"문서 처리 결과를 담는 데이터 클래스\"\"\"\n",
        "    filename: str\n",
        "    doc_type: str\n",
        "    folder_path: str\n",
        "    success: bool\n",
        "    processing_time: float\n",
        "    page_count: Optional[int] = None\n",
        "    chunk_count: Optional[int] = None\n",
        "    error_message: Optional[str] = None\n",
        "    doc_parser_result: Optional[Dict] = None\n",
        "\n",
        "print(\"✅ 데이터 클래스 정의 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ExternalServiceClient 클래스 정의 (완전한 클래스)\n",
        "\n",
        "class ExternalServiceClient:\n",
        "    \"\"\"외부 서비스 클라이언트 통합 클래스\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.doc_converter_url = os.getenv(\"DOC_CONVERTER_URL\", \"http://localhost:8001\")\n",
        "        self.doc_parser_url = os.getenv(\"DOC_PARSER_URL\", \"http://localhost:8002\")\n",
        "        self.api_url = os.getenv(\"API_URL\", \"http://localhost:8000\")\n",
        "        self.timeout = 600.0\n",
        "    \n",
        "    async def health_check_all(self) -> Dict[str, bool]:\n",
        "        \"\"\"모든 서비스의 헬스체크\"\"\"\n",
        "        services = {\n",
        "            \"doc-converter\": f\"{self.doc_converter_url}/health\",\n",
        "            \"doc-parser\": f\"{self.doc_parser_url}/health\",\n",
        "            \"api\": f\"{self.api_url}/health\"\n",
        "        }\n",
        "        \n",
        "        results = {}\n",
        "        async with httpx.AsyncClient(timeout=10.0) as client:\n",
        "            for service, url in services.items():\n",
        "                try:\n",
        "                    response = await client.get(url)\n",
        "                    results[service] = response.status_code == 200\n",
        "                    logger.info(f\"✅ {service} 서비스 정상\")\n",
        "                except Exception as e:\n",
        "                    results[service] = False\n",
        "                    logger.error(f\"❌ {service} 서비스 오류: {e}\")\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def determine_doc_type(self, file_path: Path) -> str:\n",
        "        \"\"\"파일 경로를 기반으로 문서 타입 결정\"\"\"\n",
        "        # 상대 경로 계산 (docs 폴더 기준)\n",
        "        relative_path = file_path.relative_to(DOCS_ROOT)\n",
        "        folder_path = str(relative_path.parent)\n",
        "        \n",
        "        # 폴더 경로 매핑에서 문서 타입 찾기\n",
        "        for pattern, doc_type in DOCUMENT_TYPE_MAPPING.items():\n",
        "            if folder_path.startswith(pattern) or folder_path == pattern:\n",
        "                return doc_type\n",
        "        \n",
        "        # 기본값: law (근거자료)\n",
        "        return \"law\"\n",
        "\n",
        "    async def search_documents_direct(self, query: str, top_k: int = 5, doc_types: List[str] = None) -> Dict:\n",
        "        \"\"\"데이터베이스 직접 접근으로 문서 검색\"\"\"\n",
        "        try:\n",
        "            # 직접 임베딩 서비스 및 검색 로직 구현\n",
        "            from sqlmodel.ext.asyncio.session import AsyncSession\n",
        "            from sqlalchemy.ext.asyncio import create_async_engine\n",
        "            from src.aws.embedding_service import TitanEmbeddingService\n",
        "            from sqlalchemy import text\n",
        "            \n",
        "            # 데이터베이스 연결\n",
        "            database_url = os.getenv(\"DATABASE_URL\") or f\"postgresql+asyncpg://{os.getenv('DATABASE_USER', 'postgres')}:{os.getenv('DATABASE_PASSWORD', 'postgres')}@{os.getenv('DATABASE_HOST', 'localhost')}:{os.getenv('DATABASE_PORT', '5434')}/{os.getenv('DATABASE_NAME', 'smartclm-poc')}\"\n",
        "            async_engine = create_async_engine(database_url, echo=False)\n",
        "            \n",
        "            # 임베딩 서비스 초기화\n",
        "            embedding_service = TitanEmbeddingService()\n",
        "            \n",
        "            async with AsyncSession(async_engine) as session:\n",
        "                # 1. 쿼리 임베딩 생성\n",
        "                query_embedding = await embedding_service.create_single_embedding(query)\n",
        "                \n",
        "                # 2. 벡터 검색 쿼리 구성\n",
        "                base_query = \"\"\"\n",
        "                SELECT \n",
        "                    c.id as chunk_id,\n",
        "                    c.content,\n",
        "                    c.chunk_type,\n",
        "                    c.parent_id,\n",
        "                    d.id as document_id,\n",
        "                    d.filename,\n",
        "                    d.doc_type,\n",
        "                    d.category,\n",
        "                    (1 - (c.embedding <=> :query_embedding)) as similarity_score\n",
        "                FROM chunks c\n",
        "                JOIN documents d ON c.document_id = d.id\n",
        "                WHERE c.embedding IS NOT NULL\n",
        "                AND d.room_id IS NULL \n",
        "                \"\"\"\n",
        "                \n",
        "                # 벡터를 pgvector 형식 문자열로 변환\n",
        "                vector_str = \"[\" + \",\".join(map(str, query_embedding)) + \"]\"\n",
        "                params = {\"query_embedding\": vector_str}\n",
        "                \n",
        "                # 문서 유형 필터\n",
        "                if doc_types:\n",
        "                    type_conditions = []\n",
        "                    for i, doc_type in enumerate(doc_types):\n",
        "                        param_name = f\"doc_type_{i}\"\n",
        "                        type_conditions.append(f\"d.doc_type = :{param_name}\")\n",
        "                        params[param_name] = doc_type\n",
        "                    base_query += f\" AND ({' OR '.join(type_conditions)})\"\n",
        "                \n",
        "                # 유사도 정렬 및 제한\n",
        "                base_query += \" ORDER BY similarity_score DESC LIMIT :top_k\"\n",
        "                params[\"top_k\"] = top_k\n",
        "                \n",
        "                # 3. 쿼리 실행\n",
        "                connection = await session.connection()\n",
        "                result = await connection.execute(text(base_query), params)\n",
        "                rows = result.fetchall()\n",
        "                \n",
        "                # 4. 결과 변환\n",
        "                search_results = []\n",
        "                for row in rows:\n",
        "                    search_results.append({\n",
        "                        \"chunk_id\": row.chunk_id,\n",
        "                        \"content\": row.content,\n",
        "                        \"similarity_score\": round(row.similarity_score, 4),\n",
        "                        \"chunk_type\": row.chunk_type,\n",
        "                        \"parent_id\": row.parent_id,\n",
        "                        \"document_id\": row.document_id,\n",
        "                        \"filename\": row.filename,\n",
        "                        \"doc_type\": row.doc_type,\n",
        "                        \"category\": row.category,\n",
        "                    })\n",
        "                \n",
        "                return {\n",
        "                    \"results\": search_results,\n",
        "                    \"total_found\": len(search_results),\n",
        "                    \"query\": query\n",
        "                }\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ 직접 검색 실패: {str(e)}\")\n",
        "            return {\"results\": [], \"total_found\": 0, \"error\": str(e)}\n",
        "\n",
        "print(\"✅ ExternalServiceClient 기본 메서드 정의 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ExternalServiceClient 클래스 확장 메서드들\n",
        "\n",
        "def add_client_methods(client_class):\n",
        "    \"\"\"ExternalServiceClient에 추가 메서드들을 동적으로 추가\"\"\"\n",
        "    \n",
        "    async def convert_to_pdf_if_needed(self, file_path: Path) -> Path:\n",
        "        \"\"\"필요시 PDF로 변환 (DOCX 등)\"\"\"\n",
        "        if file_path.suffix.lower() == '.pdf':\n",
        "            return file_path\n",
        "        \n",
        "        logger.info(f\"🔄 PDF 변환 중: {file_path.name}\")\n",
        "        \n",
        "        try:\n",
        "            async with httpx.AsyncClient(timeout=self.timeout) as client:\n",
        "                with open(file_path, \"rb\") as f:\n",
        "                    files = {\n",
        "                        \"file\": (file_path.name, f, \"application/octet-stream\")\n",
        "                    }\n",
        "                    \n",
        "                    response = await client.post(\n",
        "                        f\"{self.doc_converter_url}/convert\",\n",
        "                        files=files\n",
        "                    )\n",
        "                \n",
        "                if response.status_code == 200:\n",
        "                    # PDF 파일을 임시 저장\n",
        "                    pdf_path = PROCESSED_DIR / f\"{file_path.stem}_converted.pdf\"\n",
        "                    with open(pdf_path, \"wb\") as f:\n",
        "                        f.write(response.content)\n",
        "                    \n",
        "                    logger.info(f\"✅ PDF 변환 완료: {pdf_path.name}\")\n",
        "                    return pdf_path\n",
        "                else:\n",
        "                    raise Exception(f\"변환 실패: HTTP {response.status_code}\")\n",
        "                    \n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ PDF 변환 실패: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    async def analyze_document_file(self, file_path: Path) -> ProcessingResult:\n",
        "        \"\"\"문서 파일을 분석\"\"\"\n",
        "        start_time = time.time()\n",
        "        doc_type = self.determine_doc_type(file_path)\n",
        "        relative_path = file_path.relative_to(DOCS_ROOT)\n",
        "        folder_path = str(relative_path.parent)\n",
        "        \n",
        "        try:\n",
        "            logger.info(f\"📄 문서 분석 시작: {file_path.name} (타입: {doc_type})\")\n",
        "            \n",
        "            # PDF로 변환 (필요시)\n",
        "            pdf_file_path = await self.convert_to_pdf_if_needed(file_path)\n",
        "            \n",
        "            async with httpx.AsyncClient(timeout=self.timeout) as client:\n",
        "                with open(pdf_file_path, \"rb\") as f:\n",
        "                    files = {\n",
        "                        \"file\": (pdf_file_path.name, f, \"application/pdf\")\n",
        "                    }\n",
        "                    data = {\"smart_pipeline\": True}\n",
        "                    \n",
        "                    response = await client.post(\n",
        "                        f\"{self.doc_parser_url}/analyze\",\n",
        "                        files=files,\n",
        "                        data=data\n",
        "                    )\n",
        "                \n",
        "                if response.status_code == 200:\n",
        "                    result = response.json()\n",
        "                    processing_time = time.time() - start_time\n",
        "                    \n",
        "                    return ProcessingResult(\n",
        "                        filename=file_path.name,\n",
        "                        doc_type=doc_type,\n",
        "                        folder_path=folder_path,\n",
        "                        success=True,\n",
        "                        processing_time=processing_time,\n",
        "                        page_count=result.get(\"page_count\"),\n",
        "                        chunk_count=len(result.get(\"chunks\", [])),\n",
        "                        doc_parser_result=result\n",
        "                    )\n",
        "                else:\n",
        "                    error_msg = f\"Doc Parser 오류 (HTTP {response.status_code}): {response.text}\"\n",
        "                    logger.error(error_msg)\n",
        "                    return ProcessingResult(\n",
        "                        filename=file_path.name,\n",
        "                        doc_type=doc_type,\n",
        "                        folder_path=folder_path,\n",
        "                        success=False,\n",
        "                        processing_time=time.time() - start_time,\n",
        "                        error_message=error_msg\n",
        "                    )\n",
        "            \n",
        "            # 임시 PDF 파일 정리\n",
        "            if pdf_file_path != file_path and pdf_file_path.exists():\n",
        "                pdf_file_path.unlink()\n",
        "                    \n",
        "        except Exception as e:\n",
        "            error_msg = f\"문서 분석 실패: {str(e)}\"\n",
        "            logger.error(error_msg)\n",
        "            return ProcessingResult(\n",
        "                filename=file_path.name,\n",
        "                doc_type=doc_type,\n",
        "                folder_path=folder_path,\n",
        "                success=False,\n",
        "                processing_time=time.time() - start_time,\n",
        "                error_message=error_msg\n",
        "            )\n",
        "\n",
        "    # 메서드들을 클래스에 추가\n",
        "    client_class.convert_to_pdf_if_needed = convert_to_pdf_if_needed\n",
        "    client_class.analyze_document_file = analyze_document_file\n",
        "    return client_class\n",
        "\n",
        "# ExternalServiceClient에 메서드 추가\n",
        "ExternalServiceClient = add_client_methods(ExternalServiceClient)\n",
        "print(\"✅ ExternalServiceClient 변환 및 분석 메서드 추가 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ExternalServiceClient save_to_database 메서드 추가\n",
        "\n",
        "def add_save_method(client_class):\n",
        "    \"\"\"save_to_database 메서드 추가\"\"\"\n",
        "    \n",
        "    async def save_to_database(self, parsed_result: Dict, source_file: Path, doc_type: str) -> bool:\n",
        "        \"\"\"파싱된 결과를 직접 데이터베이스에 저장 (S3 업로드 없음)\"\"\"\n",
        "        try:\n",
        "            logger.info(f\"💾 데이터베이스 저장 시작: {source_file.name} (타입: {doc_type})\")\n",
        "            \n",
        "            # 로컬에서 직접 데이터베이스에 저장\n",
        "            from datetime import datetime\n",
        "            from sqlmodel.ext.asyncio.session import AsyncSession\n",
        "            from sqlalchemy.ext.asyncio import create_async_engine\n",
        "            from src.models import Document, Chunk\n",
        "            from src.aws.embedding_service import TitanEmbeddingService\n",
        "            from src.documents.chunking import HierarchicalChunker\n",
        "            import uuid\n",
        "            \n",
        "            # 직접 환경변수에서 데이터베이스 URL 가져오기\n",
        "            database_url = os.getenv(\"DATABASE_URL\") or f\"postgresql+asyncpg://{os.getenv('DATABASE_USER', 'postgres')}:{os.getenv('DATABASE_PASSWORD', 'postgres')}@{os.getenv('DATABASE_HOST', '127.0.0.1')}:{os.getenv('DATABASE_PORT', '5434')}/{os.getenv('DATABASE_NAME', 'smartclm-poc')}\"\n",
        "            async_engine = create_async_engine(database_url, echo=False)\n",
        "            \n",
        "            async with AsyncSession(async_engine) as session:\n",
        "                try:\n",
        "                    # 1. Document 메타데이터 구성\n",
        "                    filename = source_file.name\n",
        "                    title = parsed_result.get(\"title\", filename)\n",
        "                    page_count = parsed_result.get(\"page_count\", 0)\n",
        "                    markdown_content = parsed_result.get(\"markdown_content\", \"\")\n",
        "                    html_content = parsed_result.get(\"html_content\", \"\")\n",
        "                    \n",
        "                    # 파일 크기 계산\n",
        "                    file_size = source_file.stat().st_size if source_file.exists() else 0\n",
        "                    \n",
        "                    # docs/ 기준 상대 경로 계산\n",
        "                    relative_path = source_file.relative_to(DOCS_ROOT)\n",
        "                    \n",
        "                    # 2. Document 생성 (실제 파일 경로 정보 사용)\n",
        "                    document = Document(\n",
        "                        room_id=None,  # 전역 문서\n",
        "                        doc_type=doc_type,\n",
        "                        category=None,\n",
        "                        processing_status=\"processing\",\n",
        "                        filename=filename,\n",
        "                        version=None,\n",
        "                        s3_bucket=\"local\",  # 로컬 저장 표시\n",
        "                        s3_key=str(relative_path),  # docs/ 기준 상대 경로\n",
        "                        pdf_s3_bucket=\"local\",\n",
        "                        pdf_s3_key=f\"processed/{doc_type}/{filename}\",  # 처리된 결과 경로\n",
        "                        file_size=file_size,\n",
        "                        pdf_file_size=file_size,  # PDF와 원본이 같다고 가정\n",
        "                        page_count=page_count,\n",
        "                        document_metadata={\n",
        "                            \"source\": \"rag_pipeline\",\n",
        "                            \"local_processing\": True,\n",
        "                            \"original_path\": str(source_file),\n",
        "                            \"processing_time\": parsed_result.get(\"processing_time\", 0),\n",
        "                            \"chunks_count\": len(parsed_result.get(\"chunks\", [])),\n",
        "                        },\n",
        "                        auto_tags=[doc_type],\n",
        "                        html_content=html_content,\n",
        "                        markdown_content=markdown_content,\n",
        "                    )\n",
        "                    \n",
        "                    # 3. Document 저장\n",
        "                    session.add(document)\n",
        "                    await session.commit()\n",
        "                    await session.refresh(document)\n",
        "                    document_id = document.id  # ID를 미리 저장\n",
        "                    logger.info(f\"📄 Document 저장 완료: ID {document_id}\")\n",
        "                    \n",
        "                    # 4. 청킹 및 임베딩 처리\n",
        "                    if markdown_content:\n",
        "                        logger.info(f\"✂️ 청킹 및 임베딩 시작: {filename}\")\n",
        "                        \n",
        "                        chunker = HierarchicalChunker()\n",
        "                        embedding_service = TitanEmbeddingService()\n",
        "                        \n",
        "                        # 마크다운 청킹\n",
        "                        chunking_result = chunker.chunk_markdown(\n",
        "                            markdown_content=markdown_content,\n",
        "                            filename=filename,\n",
        "                        )\n",
        "                        \n",
        "                        # 벡터용 청크 생성\n",
        "                        vector_ready_chunks = chunker.create_vector_ready_chunks(chunking_result)\n",
        "                        \n",
        "                        # 임베딩 생성\n",
        "                        embedded_chunks = await embedding_service.embed_chunked_documents(vector_ready_chunks)\n",
        "                        \n",
        "                        # 청크 저장\n",
        "                        chunk_objects = []\n",
        "                        for i, chunk_data in enumerate(embedded_chunks):\n",
        "                            if chunk_data.get(\"content\", \"\").strip():\n",
        "                                chunk = Chunk(\n",
        "                                    document_id=document.id or 0,\n",
        "                                    content=chunk_data.get(\"content\", \"\"),\n",
        "                                    chunk_index=i,\n",
        "                                    header_1=chunk_data.get(\"headers\", {}).get(\"header_1\"),\n",
        "                                    header_2=chunk_data.get(\"headers\", {}).get(\"header_2\"),\n",
        "                                    header_3=chunk_data.get(\"headers\", {}).get(\"header_3\"),\n",
        "                                    header_4=chunk_data.get(\"headers\", {}).get(\"header_4\"),\n",
        "                                    parent_id=chunk_data.get(\"parent_id\"),\n",
        "                                    child_id=chunk_data.get(\"child_id\"),\n",
        "                                    chunk_type=chunk_data.get(\"chunk_type\", \"child\"),\n",
        "                                    embedding=chunk_data.get(\"embedding\"),\n",
        "                                    word_count=len(chunk_data.get(\"content\", \"\").split()),\n",
        "                                    char_count=len(chunk_data.get(\"content\", \"\")),\n",
        "                                    chunk_metadata=chunk_data.get(\"metadata\", {}),\n",
        "                                    auto_tags=chunk_data.get(\"auto_tags\", []),\n",
        "                                )\n",
        "                                chunk_objects.append(chunk)\n",
        "                        \n",
        "                        # 배치로 청크 저장\n",
        "                        for chunk in chunk_objects:\n",
        "                            session.add(chunk)\n",
        "                        \n",
        "                        await session.commit()\n",
        "                        logger.info(f\"✅ 청크 저장 완료: {len(chunk_objects)}개\")\n",
        "                    \n",
        "                    # 5. 처리 완료 상태로 변경\n",
        "                    document.processing_status = \"completed\"\n",
        "                    session.add(document)\n",
        "                    await session.commit()\n",
        "                    \n",
        "                    logger.info(f\"✅ 데이터베이스 저장 완료: {filename} (Document ID: {document_id})\")\n",
        "                    return True\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    await session.rollback()\n",
        "                    raise e\n",
        "                    \n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ 데이터베이스 저장 오류: {str(e)}\")\n",
        "            import traceback\n",
        "            logger.error(f\"상세 오류: {traceback.format_exc()}\")\n",
        "            return False\n",
        "    \n",
        "    # 메서드를 클래스에 추가\n",
        "    client_class.save_to_database = save_to_database\n",
        "    return client_class\n",
        "\n",
        "# ExternalServiceClient에 메서드 추가\n",
        "ExternalServiceClient = add_save_method(ExternalServiceClient)\n",
        "print(\"✅ ExternalServiceClient save_to_database 메서드 추가 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAGPipeline 클래스 정의\n",
        "\n",
        "class RAGPipeline:\n",
        "    \"\"\"RAG 파이프라인 메인 클래스\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.client = ExternalServiceClient()\n",
        "        self.results: List[ProcessingResult] = []\n",
        "    \n",
        "    def find_all_documents(self) -> List[Path]:\n",
        "        \"\"\"근거 자료 폴더에서만 문서 파일 찾기\"\"\"\n",
        "        supported_extensions = {'.pdf', '.docx', '.doc', '.hwp', '.xlsx', '.xls', '.pptx', '.ppt'}\n",
        "        document_files = []\n",
        "        \n",
        "        # 근거 자료 폴더만 검색\n",
        "        base_folder = DOCS_ROOT / \"근거 자료\"\n",
        "        if base_folder.exists():\n",
        "            for ext in supported_extensions:\n",
        "                # 모든 하위 폴더 검색 (법령, 체결계약, 표준계약서)\n",
        "                document_files.extend(base_folder.glob(f\"**/*{ext}\"))\n",
        "        \n",
        "        return sorted(set(document_files))  # 중복 제거 및 정렬\n",
        "\n",
        "    async def step1_store_reference_materials(self, force: bool = False):\n",
        "        \"\"\"Step 1: 근거자료 저장\"\"\"\n",
        "        logger.info(\"🚀 Step 1: 근거자료 저장 시작\")\n",
        "        \n",
        "        # 서비스 헬스체크\n",
        "        health_status = await self.client.health_check_all()\n",
        "        if not all(health_status.values()):\n",
        "            unhealthy_services = [k for k, v in health_status.items() if not v]\n",
        "            logger.error(f\"❌ 다음 서비스들이 비정상입니다: {unhealthy_services}\")\n",
        "            return False\n",
        "        \n",
        "        # 문서 파일 목록 가져오기\n",
        "        document_files = self.find_all_documents()\n",
        "        if not document_files:\n",
        "            logger.warning(\"⚠️ docs 폴더에 지원되는 문서 파일이 없습니다.\")\n",
        "            return False\n",
        "        \n",
        "        logger.info(f\"📁 발견된 문서 파일: {len(document_files)}개\")\n",
        "        \n",
        "        # 문서 타입별 분류 출력\n",
        "        type_counts = {}\n",
        "        for file_path in document_files:\n",
        "            doc_type = self.client.determine_doc_type(file_path)\n",
        "            type_counts[doc_type] = type_counts.get(doc_type, 0) + 1\n",
        "        \n",
        "        logger.info(\"📊 문서 타입별 분류:\")\n",
        "        for doc_type, count in type_counts.items():\n",
        "            logger.info(f\"  - {doc_type}: {count}개\")\n",
        "        \n",
        "        # 각 문서 파일 처리\n",
        "        for i, file_path in enumerate(document_files, 1):\n",
        "            doc_type = self.client.determine_doc_type(file_path)\n",
        "            logger.info(f\"📄 처리 중 ({i}/{len(document_files)}): {file_path.name} ({doc_type})\")\n",
        "            \n",
        "            # 1. JSON 파일이 있는지 확인 (문서 분석 건너뛰기용)\n",
        "            doc_type_folder = PROCESSED_DIR / doc_type\n",
        "            result_file = doc_type_folder / f\"{file_path.stem}_parsed.json\"\n",
        "            old_format_file = PROCESSED_DIR / f\"{file_path.stem}_{doc_type}_parsed.json\"\n",
        "            \n",
        "            if not force and (result_file.exists() or old_format_file.exists()):\n",
        "                # 기존 JSON 파일 로드\n",
        "                json_file = result_file if result_file.exists() else old_format_file\n",
        "                logger.info(f\"📄 기존 분석 결과 사용: {json_file.name}\")\n",
        "                \n",
        "                with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                    doc_parser_result = json.load(f)\n",
        "                \n",
        "                result = ProcessingResult(\n",
        "                    filename=file_path.name,\n",
        "                    doc_type=doc_type,\n",
        "                    folder_path=str(file_path.parent),\n",
        "                    success=True,\n",
        "                    processing_time=0,\n",
        "                    page_count=doc_parser_result.get(\"page_count\"),\n",
        "                    chunk_count=len(doc_parser_result.get(\"chunks\", [])),\n",
        "                    doc_parser_result=doc_parser_result\n",
        "                )\n",
        "            else:\n",
        "                # 새로 문서 분석\n",
        "                logger.info(f\"🔍 문서 분석 시작: {file_path.name}\")\n",
        "                result = await self.client.analyze_document_file(file_path)\n",
        "            \n",
        "            self.results.append(result)\n",
        "            \n",
        "            if result.success:\n",
        "                # 2. 타입별 폴더 생성\n",
        "                import shutil\n",
        "                doc_type_folder = PROCESSED_DIR / doc_type\n",
        "                doc_type_folder.mkdir(exist_ok=True)\n",
        "                \n",
        "                # 3. 분석 결과 저장 (타입별 폴더에 JSON 저장)\n",
        "                result_file = doc_type_folder / f\"{file_path.stem}_parsed.json\"\n",
        "                with open(result_file, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(result.doc_parser_result, f, ensure_ascii=False, indent=2)\n",
        "                logger.info(f\"📄 JSON 저장 완료: {result_file}\")\n",
        "                \n",
        "                # 4. 원본 파일을 타입별 폴더로 복사\n",
        "                copied_file = doc_type_folder / file_path.name\n",
        "                shutil.copy2(file_path, copied_file)\n",
        "                logger.info(f\"📁 파일 복사 완료: {copied_file}\")\n",
        "                \n",
        "                # 5. 데이터베이스 저장 (중복 체크)\n",
        "                db_already_exists = False\n",
        "                if not force:\n",
        "                    try:\n",
        "                        from sqlmodel.ext.asyncio.session import AsyncSession\n",
        "                        from sqlalchemy.ext.asyncio import create_async_engine\n",
        "                        from src.models import Document\n",
        "                        from sqlmodel import select\n",
        "                        \n",
        "                        database_url = os.getenv(\"DATABASE_URL\") or f\"postgresql+asyncpg://{os.getenv('DATABASE_USER', 'postgres')}:{os.getenv('DATABASE_PASSWORD', 'postgres')}@{os.getenv('DATABASE_HOST', 'localhost')}:{os.getenv('DATABASE_PORT', '5434')}/{os.getenv('DATABASE_NAME', 'smartclm-poc')}\"\n",
        "                        async_engine = create_async_engine(database_url, echo=False)\n",
        "                        \n",
        "                        async with AsyncSession(async_engine) as session:\n",
        "                            query = select(Document).where(\n",
        "                                Document.filename == file_path.name,\n",
        "                                Document.doc_type == doc_type,\n",
        "                                Document.processing_status == \"completed\"\n",
        "                            )\n",
        "                            result_doc = await session.exec(query)\n",
        "                            existing_doc = result_doc.first()\n",
        "                            \n",
        "                            if existing_doc:\n",
        "                                logger.info(f\"💾 DB 저장 건너뛰기: {file_path.name} (이미 저장됨: ID {existing_doc.id})\")\n",
        "                                db_already_exists = True\n",
        "                    except Exception as e:\n",
        "                        logger.warning(f\"⚠️ DB 중복 확인 실패: {file_path.name} - {str(e)}\")\n",
        "                        logger.info(f\"💾 중복 확인 실패로 인해 저장을 시도합니다: {file_path.name}\")\n",
        "                \n",
        "                if not db_already_exists:\n",
        "                    db_success = await self.client.save_to_database(\n",
        "                        result.doc_parser_result, \n",
        "                        file_path,\n",
        "                        doc_type\n",
        "                    )\n",
        "                    \n",
        "                    if db_success:\n",
        "                        logger.info(f\"✅ 완료: {file_path.name} ({result.processing_time:.2f}초)\")\n",
        "                    else:\n",
        "                        logger.error(f\"❌ DB 저장 실패: {file_path.name}\")\n",
        "                else:\n",
        "                    logger.info(f\"✅ 완료: {file_path.name} ({result.processing_time:.2f}초) - DB는 기존 사용\")\n",
        "            else:\n",
        "                logger.error(f\"❌ 처리 실패: {file_path.name} - {result.error_message}\")\n",
        "            \n",
        "            # 처리 간 잠시 대기 (시스템 부하 방지)\n",
        "            await asyncio.sleep(1)\n",
        "        \n",
        "        # 결과 요약 저장\n",
        "        await self._save_processing_summary()\n",
        "        \n",
        "        return True\n",
        "\n",
        "    async def step2_test_search(self):\n",
        "        \"\"\"Step 2: 검색 테스트\"\"\"\n",
        "        logger.info(\"🔍 Step 2: 검색 테스트 시작\")\n",
        "        \n",
        "        # 테스트 쿼리 정의\n",
        "        test_queries = [\n",
        "            # 법령 관련 쿼리\n",
        "            {\n",
        "                \"query\": \"개인정보보호법의 주요 내용은?\",\n",
        "                \"category\": \"law\",\n",
        "                \"expected_doc_type\": \"law\"\n",
        "            },\n",
        "            {\n",
        "                \"query\": \"전자상거래 소비자보호에 관한 법률\",\n",
        "                \"category\": \"law\", \n",
        "                \"expected_doc_type\": \"law\"\n",
        "            },\n",
        "            # 계약서 관련 쿼리\n",
        "            {\n",
        "                \"query\": \"계약해지 조건과 절차\",\n",
        "                \"category\": \"contract\",\n",
        "                \"expected_doc_type\": [\"executed_contract\", \"standard_contract\"]\n",
        "            },\n",
        "            {\n",
        "                \"query\": \"손해배상 책임 범위\",\n",
        "                \"category\": \"contract\",\n",
        "                \"expected_doc_type\": [\"executed_contract\", \"standard_contract\"]\n",
        "            },\n",
        "            {\n",
        "                \"query\": \"계약 기간 및 갱신 조건\",\n",
        "                \"category\": \"contract\", \n",
        "                \"expected_doc_type\": [\"executed_contract\", \"standard_contract\"]\n",
        "            },\n",
        "            # 일반적인 질문\n",
        "            {\n",
        "                \"query\": \"계약서 작성 시 주의사항\",\n",
        "                \"category\": \"general\",\n",
        "                \"expected_doc_type\": \"any\"\n",
        "            }\n",
        "        ]\n",
        "        \n",
        "        search_results = []\n",
        "        \n",
        "        # 각 쿼리에 대해 검색 수행\n",
        "        for i, test_case in enumerate(test_queries, 1):\n",
        "            logger.info(f\"🔍 검색 테스트 {i}/{len(test_queries)}: {test_case['query']}\")\n",
        "            \n",
        "            try:\n",
        "                # 직접 검색 호출\n",
        "                start_time = time.time()\n",
        "                result = await self.client.search_documents_direct(\n",
        "                    query=test_case[\"query\"],\n",
        "                    top_k=5,\n",
        "                    doc_types=None  # 모든 문서 타입에서 검색\n",
        "                )\n",
        "                search_time = time.time() - start_time\n",
        "                \n",
        "                # 결과 분석\n",
        "                search_result = {\n",
        "                    \"query\": test_case[\"query\"],\n",
        "                    \"category\": test_case[\"category\"],\n",
        "                    \"expected_doc_type\": test_case[\"expected_doc_type\"],\n",
        "                    \"search_time\": search_time,\n",
        "                    \"total_results\": len(result.get(\"results\", [])),\n",
        "                    \"results\": result.get(\"results\", []),\n",
        "                    \"success\": True\n",
        "                }\n",
        "                \n",
        "                # 결과 품질 평가\n",
        "                if result.get(\"results\"):\n",
        "                    # 문서 타입 분포 확인\n",
        "                    doc_types = [r.get(\"doc_type\") for r in result[\"results\"]]\n",
        "                    search_result[\"doc_type_distribution\"] = {\n",
        "                        doc_type: doc_types.count(doc_type) \n",
        "                        for doc_type in set(doc_types) if doc_type\n",
        "                    }\n",
        "                    \n",
        "                    # 평균 점수 계산\n",
        "                    scores = [r.get(\"similarity_score\", 0) for r in result[\"results\"]]\n",
        "                    search_result[\"avg_similarity_score\"] = sum(scores) / len(scores) if scores else 0\n",
        "                    search_result[\"max_similarity_score\"] = max(scores) if scores else 0\n",
        "                    search_result[\"min_similarity_score\"] = min(scores) if scores else 0\n",
        "                \n",
        "                logger.info(f\"  ✅ 검색 완료: {search_result['total_results']}개 결과, {search_time:.2f}초\")\n",
        "                if search_result.get(\"doc_type_distribution\"):\n",
        "                    for doc_type, count in search_result[\"doc_type_distribution\"].items():\n",
        "                        logger.info(f\"    - {doc_type}: {count}개\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                logger.error(f\"  ❌ 검색 실패: {str(e)}\")\n",
        "                search_result = {\n",
        "                    \"query\": test_case[\"query\"],\n",
        "                    \"category\": test_case[\"category\"],\n",
        "                    \"expected_doc_type\": test_case[\"expected_doc_type\"],\n",
        "                    \"error\": str(e),\n",
        "                    \"success\": False\n",
        "                }\n",
        "            \n",
        "            search_results.append(search_result)\n",
        "            \n",
        "            # 요청 간 잠시 대기\n",
        "            await asyncio.sleep(0.5)\n",
        "        \n",
        "        # 결과 요약 및 저장\n",
        "        await self._save_search_test_results(search_results)\n",
        "        \n",
        "        logger.info(\"🎉 Step 2 검색 테스트 완료!\")\n",
        "        return True\n",
        "\n",
        "print(\"✅ RAGPipeline 클래스 정의 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAGPipeline 유틸리티 메서드들 추가\n",
        "\n",
        "def add_rag_utility_methods(rag_class):\n",
        "    \"\"\"RAGPipeline에 유틸리티 메서드들 추가\"\"\"\n",
        "    \n",
        "    async def _save_processing_summary(self):\n",
        "        \"\"\"처리 결과 요약 저장\"\"\"\n",
        "        # 타입별 통계\n",
        "        type_stats = {}\n",
        "        for result in self.results:\n",
        "            doc_type = result.doc_type\n",
        "            if doc_type not in type_stats:\n",
        "                type_stats[doc_type] = {\"total\": 0, \"success\": 0, \"failed\": 0}\n",
        "            \n",
        "            type_stats[doc_type][\"total\"] += 1\n",
        "            if result.success:\n",
        "                type_stats[doc_type][\"success\"] += 1\n",
        "            else:\n",
        "                type_stats[doc_type][\"failed\"] += 1\n",
        "        \n",
        "        summary = {\n",
        "            \"total_files\": len(self.results),\n",
        "            \"successful\": len([r for r in self.results if r.success]),\n",
        "            \"failed\": len([r for r in self.results if not r.success]),\n",
        "            \"total_processing_time\": sum(r.processing_time for r in self.results),\n",
        "            \"type_statistics\": type_stats,\n",
        "            \"details\": [\n",
        "                {\n",
        "                    \"filename\": r.filename,\n",
        "                    \"doc_type\": r.doc_type,\n",
        "                    \"folder_path\": r.folder_path,\n",
        "                    \"success\": r.success,\n",
        "                    \"processing_time\": r.processing_time,\n",
        "                    \"page_count\": r.page_count,\n",
        "                    \"chunk_count\": r.chunk_count,\n",
        "                    \"error\": r.error_message\n",
        "                }\n",
        "                for r in self.results\n",
        "            ]\n",
        "        }\n",
        "        \n",
        "        summary_file = RESULTS_DIR / \"processing_summary.json\"\n",
        "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(summary, f, ensure_ascii=False, indent=2)\n",
        "        \n",
        "        logger.info(f\"📊 처리 요약:\")\n",
        "        logger.info(f\"  - 전체: {summary['total_files']}개\")\n",
        "        logger.info(f\"  - 성공: {summary['successful']}개\")\n",
        "        logger.info(f\"  - 실패: {summary['failed']}개\")\n",
        "        logger.info(f\"  - 총 처리 시간: {summary['total_processing_time']:.2f}초\")\n",
        "        \n",
        "        logger.info(f\"📋 타입별 통계:\")\n",
        "        for doc_type, stats in type_stats.items():\n",
        "            logger.info(f\"  - {doc_type}: {stats['success']}/{stats['total']} 성공\")\n",
        "        \n",
        "        logger.info(f\"💾 요약 저장: {summary_file}\")\n",
        "\n",
        "    async def _save_search_test_results(self, search_results: List[Dict]):\n",
        "        \"\"\"검색 테스트 결과 저장\"\"\"\n",
        "        # 결과 요약 계산\n",
        "        total_queries = len(search_results)\n",
        "        successful_queries = len([r for r in search_results if r.get(\"success\", False)])\n",
        "        failed_queries = total_queries - successful_queries\n",
        "        \n",
        "        # 검색 시간 통계\n",
        "        search_times = [r.get(\"search_time\", 0) for r in search_results if r.get(\"success\")]\n",
        "        avg_search_time = sum(search_times) / len(search_times) if search_times else 0\n",
        "        \n",
        "        # 결과 수 통계\n",
        "        result_counts = [r.get(\"total_results\", 0) for r in search_results if r.get(\"success\")]\n",
        "        avg_results = sum(result_counts) / len(result_counts) if result_counts else 0\n",
        "        \n",
        "        # 카테고리별 성공률\n",
        "        category_stats = {}\n",
        "        for result in search_results:\n",
        "            category = result.get(\"category\", \"unknown\")\n",
        "            if category not in category_stats:\n",
        "                category_stats[category] = {\"total\": 0, \"success\": 0}\n",
        "            category_stats[category][\"total\"] += 1\n",
        "            if result.get(\"success\", False):\n",
        "                category_stats[category][\"success\"] += 1\n",
        "        \n",
        "        summary = {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"test_summary\": {\n",
        "                \"total_queries\": total_queries,\n",
        "                \"successful_queries\": successful_queries,\n",
        "                \"failed_queries\": failed_queries,\n",
        "                \"success_rate\": successful_queries / total_queries if total_queries > 0 else 0,\n",
        "                \"avg_search_time_seconds\": round(avg_search_time, 3),\n",
        "                \"avg_results_per_query\": round(avg_results, 1)\n",
        "            },\n",
        "            \"category_performance\": {\n",
        "                category: {\n",
        "                    \"success_rate\": stats[\"success\"] / stats[\"total\"] if stats[\"total\"] > 0 else 0,\n",
        "                    \"success_count\": stats[\"success\"],\n",
        "                    \"total_count\": stats[\"total\"]\n",
        "                }\n",
        "                for category, stats in category_stats.items()\n",
        "            },\n",
        "            \"detailed_results\": search_results\n",
        "        }\n",
        "        \n",
        "        # 결과 저장\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        results_file = RESULTS_DIR / f\"search_test_results_{timestamp}.json\"\n",
        "        \n",
        "        with open(results_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(summary, f, ensure_ascii=False, indent=2)\n",
        "        \n",
        "        # 로그 출력\n",
        "        logger.info(\"=\" * 50)\n",
        "        logger.info(\"🔍 검색 테스트 결과 요약\")\n",
        "        logger.info(\"=\" * 50)\n",
        "        logger.info(f\"📊 전체 쿼리: {total_queries}개\")\n",
        "        logger.info(f\"✅ 성공: {successful_queries}개 ({successful_queries/total_queries*100:.1f}%)\")\n",
        "        logger.info(f\"❌ 실패: {failed_queries}개\")\n",
        "        logger.info(f\"⏱️ 평균 검색 시간: {avg_search_time:.3f}초\")\n",
        "        logger.info(f\"📄 평균 결과 수: {avg_results:.1f}개\")\n",
        "        \n",
        "        logger.info(\"\\n📈 카테고리별 성공률:\")\n",
        "        for category, stats in category_stats.items():\n",
        "            success_rate = stats[\"success\"] / stats[\"total\"] * 100\n",
        "            logger.info(f\"  - {category}: {stats['success']}/{stats['total']} ({success_rate:.1f}%)\")\n",
        "        \n",
        "        logger.info(f\"\\n💾 상세 결과 저장: {results_file}\")\n",
        "\n",
        "    # 메서드들을 클래스에 추가\n",
        "    rag_class._save_processing_summary = _save_processing_summary\n",
        "    rag_class._save_search_test_results = _save_search_test_results\n",
        "    return rag_class\n",
        "\n",
        "# RAGPipeline에 유틸리티 메서드 추가\n",
        "RAGPipeline = add_rag_utility_methods(RAGPipeline)\n",
        "print(\"✅ RAGPipeline 유틸리티 메서드 추가 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: 계약서 검토 테스트 (3단계 체인 방식) 메서드 추가\n",
        "\n",
        "def add_step3_methods(rag_class):\n",
        "    \"\"\"RAGPipeline에 Step 3 관련 메서드들 추가\"\"\"\n",
        "    \n",
        "    async def step3_contract_review_test(self):\n",
        "        \"\"\"Step 3: 계약서 검토 기능 테스트 (3단계 체인 방식)\"\"\"\n",
        "        logger.info(\"📝 Step 3: 계약서 검토 기능 테스트 시작 (체인 방식)\")\n",
        "        \n",
        "        # 1. 테스트 대상 문서 선택\n",
        "        test_document_path = DOCS_ROOT / \"1. NDA\" / \"초안_비밀유지계약서.docx\"\n",
        "        \n",
        "        if not test_document_path.exists():\n",
        "            logger.error(f\"❌ 테스트 문서가 없습니다: {test_document_path}\")\n",
        "            return False\n",
        "        \n",
        "        logger.info(f\"📄 테스트 문서: {test_document_path.name}\")\n",
        "        \n",
        "        # 2. 계약서를 DB에 저장 (중복 체크 포함)\n",
        "        logger.info(\"💾 계약서 DB 저장 확인/처리 중...\")\n",
        "        document_id = await self._ensure_contract_in_db(test_document_path)\n",
        "        \n",
        "        if not document_id:\n",
        "            logger.error(\"❌ 계약서 DB 저장 실패\")\n",
        "            return False\n",
        "        \n",
        "        logger.info(f\"✅ 계약서 DB 저장 완료 (Document ID: {document_id})\")\n",
        "        \n",
        "        # 3. DB에서 계약서 정보 조회\n",
        "        document_info = await self._get_document_from_db(document_id)\n",
        "        if not document_info:\n",
        "            logger.error(\"❌ DB에서 계약서 정보 조회 실패\")\n",
        "            return False\n",
        "        \n",
        "        # 4. 계약서 조항 조회\n",
        "        chunks = await self._get_document_chunks_from_db(document_id)\n",
        "        parent_chunks = [c for c in chunks if c.get(\"chunk_type\") == \"parent\"]\n",
        "        \n",
        "        logger.info(f\"🔍 DB에서 조회한 청크 수: {len(chunks)}\")\n",
        "        logger.info(f\"🔍 Parent 청크 수: {len(parent_chunks)}\")\n",
        "        \n",
        "        if not parent_chunks:\n",
        "            logger.error(\"❌ 분석할 조항을 생성할 수 없습니다.\")\n",
        "            return False\n",
        "        \n",
        "        logger.info(f\"📊 총 {len(parent_chunks)}개 조항 발견\")\n",
        "        \n",
        "        try:\n",
        "            # === 3단계 체인 분석 시작 ===\n",
        "            \n",
        "            # Chain 1: 전체 계약서 스캔 → 위법 조항 식별\n",
        "            logger.info(\"🔍 Chain 1: 전체 계약서 위법 조항 식별 중...\")\n",
        "            start_time = time.time()\n",
        "            \n",
        "            violation_candidates = await self._chain1_identify_violations(\n",
        "                document_name=test_document_path.name,\n",
        "                all_chunks=parent_chunks,\n",
        "                document_id=document_id\n",
        "            )\n",
        "            \n",
        "            chain1_time = time.time() - start_time\n",
        "            logger.info(f\"✅ Chain 1 완료: {len(violation_candidates)}개 위법 조항 후보 발견 ({chain1_time:.2f}초)\")\n",
        "            \n",
        "            if not violation_candidates:\n",
        "                logger.info(\"✅ 위법 조항이 발견되지 않았습니다.\")\n",
        "                await self._save_chain_analysis_results(test_document_path.name, {\"violations\": []}, {\n",
        "                    \"chain1_time\": chain1_time,\n",
        "                    \"chain2_time\": 0,\n",
        "                    \"chain3_time\": 0,\n",
        "                    \"total_time\": chain1_time,\n",
        "                    \"violations_found\": 0,\n",
        "                    \"total_clauses\": len(parent_chunks)\n",
        "                })\n",
        "                return True\n",
        "            \n",
        "            # Chain 2: 각 위법 조항별 관련 법령 검색 (병렬 처리)\n",
        "            logger.info(\"🔍 Chain 2: 위법 조항별 관련 법령 검색 중...\")\n",
        "            start_time = time.time()\n",
        "            \n",
        "            violations_with_laws = await self._chain2_search_related_laws(violation_candidates)\n",
        "            \n",
        "            chain2_time = time.time() - start_time\n",
        "            logger.info(f\"✅ Chain 2 완료: {len(violations_with_laws)}개 조항의 법령 검색 완료 ({chain2_time:.2f}초)\")\n",
        "            \n",
        "            # Chain 3: 최종 상세 분석\n",
        "            logger.info(\"🔍 Chain 3: 최종 상세 분석 중...\")\n",
        "            start_time = time.time()\n",
        "            \n",
        "            final_analysis = await self._chain3_detailed_analysis(\n",
        "                violations_with_laws=violations_with_laws,\n",
        "                document_name=test_document_path.name\n",
        "            )\n",
        "            \n",
        "            chain3_time = time.time() - start_time\n",
        "            logger.info(f\"✅ Chain 3 완료: 최종 분석 완료 ({chain3_time:.2f}초)\")\n",
        "            \n",
        "            # 결과 저장\n",
        "            await self._save_chain_analysis_results(test_document_path.name, final_analysis, {\n",
        "                \"chain1_time\": chain1_time,\n",
        "                \"chain2_time\": chain2_time, \n",
        "                \"chain3_time\": chain3_time,\n",
        "                \"total_time\": chain1_time + chain2_time + chain3_time,\n",
        "                \"violations_found\": len(final_analysis.get(\"violations\", [])),\n",
        "                \"total_clauses\": len(parent_chunks)\n",
        "            })\n",
        "            \n",
        "            logger.info(\"🎉 Step 3 계약서 검토 테스트 완료 (체인 방식)!\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ 체인 분석 실패: {str(e)}\")\n",
        "            return False\n",
        "    \n",
        "    # 메서드를 클래스에 추가\n",
        "    rag_class.step3_contract_review_test = step3_contract_review_test\n",
        "    return rag_class\n",
        "\n",
        "# RAGPipeline에 Step 3 메서드 추가\n",
        "RAGPipeline = add_step3_methods(RAGPipeline)\n",
        "print(\"✅ RAGPipeline Step 3 메서드 추가 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3 체인 분석 헬퍼 메서드들 (Part 1: DB 관련)\n",
        "\n",
        "def add_chain_helper_methods_part1(rag_class):\n",
        "    \"\"\"RAGPipeline에 체인 분석 헬퍼 메서드들 추가 (Part 1: DB 관련)\"\"\"\n",
        "    \n",
        "    async def _ensure_contract_in_db(self, document_path: Path) -> Optional[int]:\n",
        "        \"\"\"계약서가 DB에 없으면 저장하고, 있으면 기존 ID 반환\"\"\"\n",
        "        try:\n",
        "            from sqlmodel.ext.asyncio.session import AsyncSession\n",
        "            from sqlalchemy.ext.asyncio import create_async_engine\n",
        "            from src.models import Document\n",
        "            from sqlmodel import select\n",
        "            \n",
        "            # 데이터베이스 연결\n",
        "            database_url = os.getenv(\"DATABASE_URL\") or f\"postgresql+asyncpg://{os.getenv('DATABASE_USER', 'postgres')}:{os.getenv('DATABASE_PASSWORD', 'postgres')}@{os.getenv('DATABASE_HOST', 'localhost')}:{os.getenv('DATABASE_PORT', '5434')}/{os.getenv('DATABASE_NAME', 'smartclm-poc')}\"\n",
        "            async_engine = create_async_engine(database_url, echo=False)\n",
        "            \n",
        "            async with AsyncSession(async_engine) as session:\n",
        "                # 1. 기존 문서 확인\n",
        "                query = select(Document).where(\n",
        "                    Document.filename == document_path.name,\n",
        "                    Document.doc_type == \"contract\",\n",
        "                    Document.processing_status == \"completed\"\n",
        "                )\n",
        "                result = await session.exec(query)\n",
        "                existing_doc = result.first()\n",
        "                \n",
        "                if existing_doc:\n",
        "                    logger.info(f\"🗑️ 기존 계약서 삭제 후 재생성: {document_path.name} (ID: {existing_doc.id})\")\n",
        "                    # 기존 청크들 삭제\n",
        "                    from sqlmodel import delete\n",
        "                    from src.models import Chunk\n",
        "                    chunk_delete_stmt = delete(Chunk).where(Chunk.document_id == existing_doc.id)\n",
        "                    await session.exec(chunk_delete_stmt)\n",
        "                    \n",
        "                    # 기존 문서 삭제\n",
        "                    doc_delete_stmt = delete(Document).where(Document.id == existing_doc.id)\n",
        "                    await session.exec(doc_delete_stmt)\n",
        "                    await session.commit()\n",
        "                    logger.info(f\"✅ 기존 데이터 삭제 완료\")\n",
        "                \n",
        "                # 2. 새로 분석 및 저장\n",
        "                logger.info(f\"🔍 계약서 분석 및 DB 저장: {document_path.name}\")\n",
        "                doc_data = await self.client.analyze_document_file(document_path)\n",
        "                \n",
        "                if not doc_data.success:\n",
        "                    logger.error(f\"❌ 문서 분석 실패: {doc_data.error_message}\")\n",
        "                    return None\n",
        "                \n",
        "                logger.info(f\"✅ 문서 분석 완료 ({doc_data.processing_time:.2f}초)\")\n",
        "                \n",
        "                # 3. DB 저장\n",
        "                db_success = await self.client.save_to_database(\n",
        "                    doc_data.doc_parser_result,\n",
        "                    document_path,\n",
        "                    \"contract\"  # contract 타입으로 저장\n",
        "                )\n",
        "                \n",
        "                if not db_success:\n",
        "                    logger.error(f\"❌ DB 저장 실패: {document_path.name}\")\n",
        "                    return None\n",
        "                \n",
        "                # 4. 저장된 문서 ID 조회\n",
        "                result2 = await session.exec(query)\n",
        "                new_doc = result2.first()\n",
        "                \n",
        "                if new_doc:\n",
        "                    logger.info(f\"✅ 새 계약서 저장 완료: ID {new_doc.id}\")\n",
        "                    return new_doc.id\n",
        "                \n",
        "                return None\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ 계약서 DB 처리 실패: {str(e)}\")\n",
        "            return None\n",
        "    \n",
        "    async def _get_document_from_db(self, document_id: int) -> Optional[Dict]:\n",
        "        \"\"\"DB에서 문서 정보 조회\"\"\"\n",
        "        try:\n",
        "            from sqlmodel.ext.asyncio.session import AsyncSession\n",
        "            from sqlalchemy.ext.asyncio import create_async_engine\n",
        "            from src.models import Document\n",
        "            from sqlmodel import select\n",
        "            \n",
        "            database_url = os.getenv(\"DATABASE_URL\") or f\"postgresql+asyncpg://{os.getenv('DATABASE_USER', 'postgres')}:{os.getenv('DATABASE_PASSWORD', 'postgres')}@{os.getenv('DATABASE_HOST', 'localhost')}:{os.getenv('DATABASE_PORT', '5434')}/{os.getenv('DATABASE_NAME', 'smartclm-poc')}\"\n",
        "            async_engine = create_async_engine(database_url, echo=False)\n",
        "            \n",
        "            async with AsyncSession(async_engine) as session:\n",
        "                query = select(Document).where(Document.id == document_id)\n",
        "                result = await session.exec(query)\n",
        "                document = result.first()\n",
        "                \n",
        "                if document:\n",
        "                    return {\n",
        "                        \"id\": document.id,\n",
        "                        \"filename\": document.filename,\n",
        "                        \"doc_type\": document.doc_type,\n",
        "                        \"markdown_content\": document.markdown_content,\n",
        "                        \"page_count\": document.page_count\n",
        "                    }\n",
        "                return None\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ 문서 조회 실패: {str(e)}\")\n",
        "            return None\n",
        "    \n",
        "    async def _get_document_chunks_from_db(self, document_id: int) -> List[Dict]:\n",
        "        \"\"\"DB에서 문서의 청크들 조회\"\"\"\n",
        "        try:\n",
        "            from sqlmodel.ext.asyncio.session import AsyncSession\n",
        "            from sqlalchemy.ext.asyncio import create_async_engine\n",
        "            from src.models import Chunk\n",
        "            from sqlmodel import select\n",
        "            \n",
        "            database_url = os.getenv(\"DATABASE_URL\") or f\"postgresql+asyncpg://{os.getenv('DATABASE_USER', 'postgres')}:{os.getenv('DATABASE_PASSWORD', 'postgres')}@{os.getenv('DATABASE_HOST', 'localhost')}:{os.getenv('DATABASE_PORT', '5434')}/{os.getenv('DATABASE_NAME', 'smartclm-poc')}\"\n",
        "            async_engine = create_async_engine(database_url, echo=False)\n",
        "            \n",
        "            async with AsyncSession(async_engine) as session:\n",
        "                query = select(Chunk).where(\n",
        "                    Chunk.document_id == document_id,\n",
        "                    Chunk.chunk_type == \"parent\"\n",
        "                ).order_by(Chunk.chunk_index)\n",
        "                \n",
        "                result = await session.exec(query)\n",
        "                chunks = result.all()\n",
        "                \n",
        "                # Chunk 모델을 딕셔너리로 변환\n",
        "                chunk_list = []\n",
        "                for chunk in chunks:\n",
        "                    chunk_dict = {\n",
        "                        \"chunk_index\": chunk.chunk_index,\n",
        "                        \"chunk_type\": chunk.chunk_type,\n",
        "                        \"content\": chunk.content,\n",
        "                        \"header_1\": chunk.header_1,\n",
        "                        \"header_2\": chunk.header_2,\n",
        "                        \"header_3\": chunk.header_3,\n",
        "                        \"chunk_metadata\": chunk.chunk_metadata or {}\n",
        "                    }\n",
        "                    chunk_list.append(chunk_dict)\n",
        "                \n",
        "                logger.info(f\"📊 DB에서 {len(chunk_list)}개 parent 청크 조회\")\n",
        "                return chunk_list\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ 청크 조회 실패: {str(e)}\")\n",
        "            return []\n",
        "    \n",
        "    # 메서드들을 클래스에 추가\n",
        "    rag_class._ensure_contract_in_db = _ensure_contract_in_db\n",
        "    rag_class._get_document_from_db = _get_document_from_db\n",
        "    rag_class._get_document_chunks_from_db = _get_document_chunks_from_db\n",
        "    return rag_class\n",
        "\n",
        "# RAGPipeline에 체인 헬퍼 메서드 추가 (Part 1)\n",
        "RAGPipeline = add_chain_helper_methods_part1(RAGPipeline)\n",
        "print(\"✅ RAGPipeline 체인 헬퍼 메서드 추가 완료 (Part 1: DB 관련)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3 체인 분석 헬퍼 메서드들 (Part 2: 체인 분석)\n",
        "\n",
        "def add_chain_helper_methods_part2(rag_class):\n",
        "    \"\"\"RAGPipeline에 체인 분석 메서드들 추가 (Part 2: 체인 분석)\"\"\"\n",
        "    \n",
        "    async def _chain1_identify_violations(self, document_name: str, all_chunks: List[Dict], document_id: int) -> List[Dict]:\n",
        "        \"\"\"Chain 1: 전체 계약서를 스캔하여 위법 조항들을 식별\"\"\"\n",
        "        try:\n",
        "            # 전체 계약서 구조 생성 (원본 조항 번호 보존)\n",
        "            contract_structure = []\n",
        "            for i, chunk in enumerate(all_chunks, 1):\n",
        "                title = chunk.get(\"header_1\", f\"조항 {i}\")\n",
        "                content = chunk.get(\"content\", \"\")[:200] + \"...\" if len(chunk.get(\"content\", \"\")) > 200 else chunk.get(\"content\", \"\")\n",
        "                \n",
        "\n",
        "                # title이 이미 \"제X조\" 형태인지 확인\n",
        "                if title.startswith(\"제\") and \"조\" in title:\n",
        "                    # 이미 조항 번호가 있으면 그대로 사용\n",
        "                    contract_structure.append(f\"{title}: {content}\")\n",
        "                else:\n",
        "                    # 조항 번호가 없으면 순서대로 추가\n",
        "                    contract_structure.append(f\"제{i}조 {title}: {content}\")\n",
        "            \n",
        "            full_contract = \"\\n\\n\".join(contract_structure)\n",
        "            \n",
        "            # Chain 1 프롬프트\n",
        "            chain1_prompt = f\"\"\"# 📋 (주)비에스지파트너스 관점 계약서 위험 조항 식별\n",
        "\n",
        "**계약서명:** {document_name}\n",
        "**총 조항수:** {len(all_chunks)}개\n",
        "**검토 관점:** **(주)비에스지파트너스 입장에서 불리한 조항 중심 분석**\n",
        "\n",
        "**전체 계약서 구조:**\n",
        "```\n",
        "{full_contract}\n",
        "```\n",
        "\n",
        "## 🎯 분석 요구사항\n",
        "\n",
        "이 계약서를 **(주)비에스지파트너스 입장**에서 검토하여 **비에스지파트너스에게 불리하거나 위험한 조항들**을 모두 찾아주세요.\n",
        "\n",
        "### 비에스지파트너스 관점 상세 검토 기준:\n",
        "\n",
        "**🔍 비밀정보 관련 위험:**\n",
        "1. **정보 보호 범위의 과도한 축소**: \"비밀\" 표시 누락 시 핵심정보도 보호받지 못하는 조항\n",
        "2. **일방적 비밀유지 의무**: 비에스지파트너스에게만 과도한 비밀유지 의무 부과\n",
        "3. **비밀정보 정의의 불균형**: 상대방 정보는 넓게, 비에스지파트너스 정보는 좁게 정의\n",
        "\n",
        "**⚖️ 책임 및 배상 관련 위험:**\n",
        "4. **과도한 손해배상**: 비에스지파트너스에게만 과중한 배상책임 부과\n",
        "5. **배상 한도의 불균형**: 상대방은 무제한, 비에스지파트너스는 제한된 배상\n",
        "6. **입증책임의 전가**: 비에스지파트너스가 무과실을 입증해야 하는 조항\n",
        "\n",
        "**🚪 계약 해지 및 권리 관련 위험:**\n",
        "7. **일방적 해지권**: 상대방에게만 해지권을 부여하는 불평등 조항\n",
        "8. **권리 행사 제한**: 비에스지파트너스의 정당한 권리(가처분, 손해배상청구 등) 제한\n",
        "9. **통지 의무의 불균형**: 비에스지파트너스에게만 까다로운 통지 의무 부과\n",
        "\n",
        "**📋 절차 및 기타 위험:**\n",
        "10. **관할 법원의 불리함**: 비에스지파트너스에게 불리한 원거리 관할 지정\n",
        "11. **계약 기간의 불균형**: 의무는 길게, 권리는 짧게 설정\n",
        "12. **법적 위험**: 강행법규 위반으로 비에스지파트너스가 불이익을 받을 수 있는 조항\n",
        "\n",
        "**⚠️ 특히 주의깊게 검토할 조항:**\n",
        "- 비밀정보 정의에서 \"표시\" 요구사항 (실무상 누락 위험 높음)\n",
        "- 손해배상 조항의 금액 제한 및 면책 조항\n",
        "- 계약 위반 시 구제수단 제한 조항\n",
        "- 준거법 및 관할 조항\n",
        "\n",
        "## 📝 출력 형식 (JSON)\n",
        "\n",
        "위법 조항이 있다면 다음 형식으로 출력해주세요:\n",
        "\n",
        "```json\n",
        "{{\n",
        "  \"violations_found\": true,\n",
        "  \"total_violations\": 3,\n",
        "  \"violations\": [\n",
        "    {{\n",
        "      \"clause_number\": \"제3조\",\n",
        "      \"clause_title\": \"비밀정보의 정의\",\n",
        "      \"risk_type\": \"정보_보호_범위의_과도한_축소\",\n",
        "      \"risk_level\": \"높음\",\n",
        "      \"brief_reason\": \"비밀 표시 누락 시 핵심정보도 보호받지 못해 비에스지파트너스에게 매우 위험\"\n",
        "    }},\n",
        "    {{\n",
        "      \"clause_number\": \"제9조\", \n",
        "      \"clause_title\": \"손해배상\",\n",
        "      \"risk_type\": \"과도한_손해배상_및_권리제한\",\n",
        "      \"risk_level\": \"높음\",\n",
        "      \"brief_reason\": \"비에스지파트너스만 배상 한도 제한되고 가처분 청구권도 박탈당함\"\n",
        "    }},\n",
        "    {{\n",
        "      \"clause_number\": \"제8조\",\n",
        "      \"clause_title\": \"계약기간\",\n",
        "      \"risk_type\": \"계약_기간의_불균형\",\n",
        "      \"risk_level\": \"중간\", \n",
        "      \"brief_reason\": \"비에스지파트너스의 의무는 3년+1년, 상대방 의무는 계약 종료와 함께 소멸\"\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "```\n",
        "\n",
        "위법 조항이 없다면:\n",
        "```json\n",
        "{{\n",
        "  \"violations_found\": false,\n",
        "  \"total_violations\": 0,\n",
        "  \"violations\": []\n",
        "}}\n",
        "```\n",
        "\n",
        "**중요**: 반드시 JSON 형식으로만 출력하고, 추가 설명은 하지 마세요.\"\"\"\n",
        "\n",
        "            # AI 호출\n",
        "            from src.aws.bedrock_service import BedrockService\n",
        "            bedrock_service = BedrockService()\n",
        "            \n",
        "            response = bedrock_service.invoke_model(\n",
        "                prompt=chain1_prompt,\n",
        "                max_tokens=2000,\n",
        "                temperature=0.0\n",
        "            )\n",
        "            \n",
        "            # JSON 파싱\n",
        "            response_text = response.get(\"text\", \"\") if isinstance(response, dict) else str(response)\n",
        "            \n",
        "            import json\n",
        "            import re\n",
        "            \n",
        "            # JSON 추출 (```json 태그 제거)\n",
        "            json_match = re.search(r'```json\\s*(\\{.*?\\})\\s*```', response_text, re.DOTALL)\n",
        "            if json_match:\n",
        "                json_str = json_match.group(1)\n",
        "            else:\n",
        "                # JSON 태그 없이 직접 JSON인 경우\n",
        "                json_str = response_text.strip()\n",
        "            \n",
        "            try:\n",
        "                result = json.loads(json_str)\n",
        "                violations = result.get(\"violations\", [])\n",
        "                \n",
        "                # chunk_index 추가 (조항 번호에서 추출)\n",
        "                for violation in violations:\n",
        "                    clause_num = violation.get(\"clause_number\", \"\")\n",
        "                    # \"제3조\" -> 3 추출\n",
        "                    import re\n",
        "                    match = re.search(r'제(\\d+)조', clause_num)\n",
        "                    if match:\n",
        "                        violation[\"chunk_index\"] = int(match.group(1)) - 1  # 0-based\n",
        "                    else:\n",
        "                        violation[\"chunk_index\"] = 0\n",
        "                \n",
        "                logger.info(f\"🔍 Chain 1 결과: {len(violations)}개 위법 조항 후보 식별\")\n",
        "                return violations\n",
        "                \n",
        "            except json.JSONDecodeError as e:\n",
        "                logger.error(f\"❌ Chain 1 JSON 파싱 실패: {e}\")\n",
        "                logger.error(f\"응답 텍스트: {response_text[:500]}...\")\n",
        "                return []\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ Chain 1 실패: {str(e)}\")\n",
        "            return []\n",
        "    \n",
        "    async def _chain2_search_related_laws(self, violation_candidates: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"Chain 2: 각 위법 조항별로 관련 법령을 병렬 검색\"\"\"\n",
        "        violations_with_laws = []\n",
        "        \n",
        "        # 병렬 처리를 위한 태스크 생성\n",
        "        search_tasks = []\n",
        "        for violation in violation_candidates:\n",
        "            search_query = f\"{violation.get('clause_title', '')} {violation.get('brief_reason', '')}\"\n",
        "            task = self._search_laws_for_violation(violation, search_query)\n",
        "            search_tasks.append(task)\n",
        "        \n",
        "        # 병렬 실행\n",
        "        if search_tasks:\n",
        "            results = await asyncio.gather(*search_tasks, return_exceptions=True)\n",
        "            \n",
        "            for result in results:\n",
        "                if isinstance(result, Exception):\n",
        "                    logger.error(f\"❌ 법령 검색 실패: {result}\")\n",
        "                else:\n",
        "                    violations_with_laws.append(result)\n",
        "        \n",
        "        logger.info(f\"🔍 Chain 2 결과: {len(violations_with_laws)}개 조항의 법령 검색 완료\")\n",
        "        return violations_with_laws\n",
        "    \n",
        "    async def _search_laws_for_violation(self, violation: Dict, search_query: str) -> Dict:\n",
        "        \"\"\"개별 위법 조항에 대한 법령 검색\"\"\"\n",
        "        try:\n",
        "            # 관련 법령 검색\n",
        "            legal_docs = await self.client.search_documents_direct(\n",
        "                query=search_query,\n",
        "                top_k=3,\n",
        "                doc_types=[\"law\"]\n",
        "            )\n",
        "            \n",
        "            violation_with_laws = violation.copy()\n",
        "            violation_with_laws[\"related_laws\"] = legal_docs.get(\"results\", [])\n",
        "            violation_with_laws[\"laws_found\"] = len(legal_docs.get(\"results\", []))\n",
        "            \n",
        "            logger.info(f\"  📖 {violation.get('clause_title', '')}: {len(legal_docs.get('results', []))}개 관련 법령 발견\")\n",
        "            return violation_with_laws\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ 법령 검색 실패 ({violation.get('clause_title', '')}): {e}\")\n",
        "            violation_with_laws = violation.copy()\n",
        "            violation_with_laws[\"related_laws\"] = []\n",
        "            violation_with_laws[\"laws_found\"] = 0\n",
        "            return violation_with_laws\n",
        "    \n",
        "    # 메서드들을 클래스에 추가\n",
        "    rag_class._chain1_identify_violations = _chain1_identify_violations\n",
        "    rag_class._chain2_search_related_laws = _chain2_search_related_laws\n",
        "    rag_class._search_laws_for_violation = _search_laws_for_violation\n",
        "    return rag_class\n",
        "\n",
        "# RAGPipeline에 체인 헬퍼 메서드 추가 (Part 2)\n",
        "RAGPipeline = add_chain_helper_methods_part2(RAGPipeline)\n",
        "print(\"✅ RAGPipeline 체인 헬퍼 메서드 추가 완료 (Part 2: Chain 1, 2)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3 체인 분석 헬퍼 메서드들 (Part 3: Chain 3 및 결과 저장)\n",
        "\n",
        "def add_chain_helper_methods_part3(rag_class):\n",
        "    \"\"\"RAGPipeline에 체인 분석 메서드들 추가 (Part 3: Chain 3 및 결과 저장)\"\"\"\n",
        "    \n",
        "    async def _chain3_detailed_analysis(self, violations_with_laws: List[Dict], document_name: str) -> Dict:\n",
        "        \"\"\"Chain 3: 위법 조항과 법령 근거를 바탕으로 최종 상세 분석\"\"\"\n",
        "        try:\n",
        "            if not violations_with_laws:\n",
        "                return {\"violations\": []}\n",
        "            \n",
        "            final_violations = []\n",
        "            \n",
        "            for violation_data in violations_with_laws:\n",
        "                try:\n",
        "                    # 상세 분석을 위한 프롬프트 구성\n",
        "                    clause_title = violation_data.get(\"clause_title\", \"\")\n",
        "                    clause_number = violation_data.get(\"clause_number\", \"\")\n",
        "                    risk_type = violation_data.get(\"risk_type\", \"\")\n",
        "                    brief_reason = violation_data.get(\"brief_reason\", \"\")\n",
        "                    related_laws = violation_data.get(\"related_laws\", [])\n",
        "                    \n",
        "                    # 관련 법령 텍스트 구성\n",
        "                    laws_text = \"\"\n",
        "                    if related_laws:\n",
        "                        law_descriptions = []\n",
        "                        for i, law in enumerate(related_laws, 1):\n",
        "                            filename = law.get('filename', '').replace('.pdf', '')\n",
        "                            content = law.get('content', '')[:300] + \"...\" if len(law.get('content', '')) > 300 else law.get('content', '')\n",
        "                            similarity = f\"(유사도: {law.get('similarity_score', 0):.3f})\"\n",
        "                            law_descriptions.append(f\"{i}. {filename} {similarity}\\n{content}\")\n",
        "                        laws_text = \"\\n\\n\".join(law_descriptions)\n",
        "                    else:\n",
        "                        laws_text = \"관련 법령을 찾을 수 없습니다.\"\n",
        "                    \n",
        "                    chain3_prompt = f\"\"\"# 🏛️ (주)비에스지파트너스 관점 계약서 조항 상세 분석\n",
        "\n",
        "**분석 대상 조항:** {clause_number} {clause_title}\n",
        "**검토 관점:** (주)비에스지파트너스 입장에서의 위험성 평가\n",
        "**예비 위험 유형:** {risk_type}\n",
        "**예비 판단:** {brief_reason}\n",
        "\n",
        "## 📚 관련 법령 근거\n",
        "\n",
        "{laws_text}\n",
        "\n",
        "## 🎯 상세 분석 요구사항\n",
        "\n",
        "위 조항과 관련 법령을 바탕으로 **(주)비에스지파트너스 입장에서** 다음 형식으로 상세 분석해주세요:\n",
        "\n",
        "### 출력 형식 (JSON):\n",
        "```json\n",
        "{{\n",
        "  \"조항_위치\": \"{clause_number} ({clause_title})\",\n",
        "  \"리스크_유형\": \"비에스지파트너스 관점의 구체적인 위험 유형 (예: 정보 보호 범위의 과도한 축소, 과도한_배상책임, 불리한_해지조건)\",\n",
        "  \"판단_근거\": \"비에스지파트너스에게 어떤 불이익이 있는지 관련 법령과 함께 구체적 제시 (예: 비에스지파트너스에게만 민법 제398조 위반 수준의 과도한 배상책임 부과)\",\n",
        "  \"의견\": \"비에스지파트너스 입장에서의 구체적인 개선 방안 (예: 상호 배상책임으로 변경하거나 비에스지파트너스 배상 한도 설정 필요)\",\n",
        "  \"관련_법령\": [\"구체적인 법령 조항명\"]\n",
        "}}\n",
        "```\n",
        "\n",
        "**중요**: \n",
        "1. 반드시 JSON 형식으로만 출력\n",
        "2. **(주)비에스지파트너스 입장**에서 불리한 점을 중심으로 분석\n",
        "3. 관련 법령이 있다면 구체적인 조항명 명시  \n",
        "4. 비에스지파트너스에게 실무적으로 도움이 되는 개선방안 제시\n",
        "5. 추가 설명 없이 JSON만 출력\"\"\"\n",
        "\n",
        "                    # AI 호출\n",
        "                    from src.aws.bedrock_service import BedrockService\n",
        "                    bedrock_service = BedrockService()\n",
        "                    \n",
        "                    response = bedrock_service.invoke_model(\n",
        "                        prompt=chain3_prompt,\n",
        "                        max_tokens=1500,\n",
        "                        temperature=0.0\n",
        "                    )\n",
        "                    \n",
        "                    # JSON 파싱\n",
        "                    response_text = response.get(\"text\", \"\") if isinstance(response, dict) else str(response)\n",
        "                    \n",
        "                    import json\n",
        "                    import re\n",
        "                    \n",
        "                    # JSON 추출\n",
        "                    json_match = re.search(r'```json\\s*(\\{.*?\\})\\s*```', response_text, re.DOTALL)\n",
        "                    if json_match:\n",
        "                        json_str = json_match.group(1)\n",
        "                    else:\n",
        "                        json_str = response_text.strip()\n",
        "                    \n",
        "                    try:\n",
        "                        detailed_analysis = json.loads(json_str)\n",
        "                        final_violations.append(detailed_analysis)\n",
        "                        logger.info(f\"  ✅ {clause_title} 상세 분석 완료\")\n",
        "                        \n",
        "                    except json.JSONDecodeError as e:\n",
        "                        logger.error(f\"❌ Chain 3 JSON 파싱 실패 ({clause_title}): {e}\")\n",
        "                        # 실패 시 기본 형식으로 대체\n",
        "                        final_violations.append({\n",
        "                            \"조항_위치\": f\"{clause_number} ({clause_title})\",\n",
        "                            \"리스크_유형\": risk_type,\n",
        "                            \"판단_근거\": brief_reason,\n",
        "                            \"의견\": \"상세 분석 실패로 인해 기본 정보만 제공\",\n",
        "                            \"관련_법령\": [law.get('filename', '').replace('.pdf', '') for law in related_laws[:2]]\n",
        "                        })\n",
        "                \n",
        "                except Exception as e:\n",
        "                    logger.error(f\"❌ 개별 조항 분석 실패 ({violation_data.get('clause_title', '')}): {e}\")\n",
        "                    continue\n",
        "            \n",
        "            return {\n",
        "                \"contract_analysis\": {\n",
        "                    \"document_name\": document_name,\n",
        "                    \"total_violations\": len(final_violations),\n",
        "                    \"analysis_method\": \"3_chain_analysis\",\n",
        "                    \"analysis_date\": datetime.now().isoformat()\n",
        "                },\n",
        "                \"violations\": final_violations\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ Chain 3 실패: {str(e)}\")\n",
        "            return {\"violations\": []}\n",
        "    \n",
        "    async def _save_chain_analysis_results(self, document_name: str, analysis_result: Dict, performance_stats: Dict):\n",
        "        \"\"\"체인 분석 결과 저장\"\"\"\n",
        "        try:\n",
        "            # 타임스탬프 생성\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            \n",
        "            # 결과 파일명\n",
        "            result_filename = f\"chain_analysis_{document_name.replace('.docx', '')}_{timestamp}.json\"\n",
        "            result_file = RESULTS_DIR / result_filename\n",
        "            \n",
        "            # 최종 결과 구성\n",
        "            final_result = {\n",
        "                \"metadata\": {\n",
        "                    \"timestamp\": datetime.now().isoformat(),\n",
        "                    \"document_name\": document_name,\n",
        "                    \"analysis_method\": \"3_chain_analysis\",\n",
        "                    \"performance\": performance_stats\n",
        "                },\n",
        "                \"summary\": {\n",
        "                    \"violations_found\": len(analysis_result.get(\"violations\", [])),\n",
        "                    \"total_analysis_time\": performance_stats.get(\"total_time\", 0),\n",
        "                    \"efficiency_gain\": f\"기존 조항별 분석 대비 {performance_stats.get('total_clauses', 0) * 30 - performance_stats.get('total_time', 0):.1f}초 단축\"\n",
        "                },\n",
        "                \"analysis_result\": analysis_result\n",
        "            }\n",
        "            \n",
        "            # JSON 파일 저장\n",
        "            with open(result_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(final_result, f, ensure_ascii=False, indent=2)\n",
        "            \n",
        "            # 로그 출력\n",
        "            logger.info(\"=\" * 50)\n",
        "            logger.info(\"📝 체인 분석 결과 요약\")\n",
        "            logger.info(\"=\" * 50)\n",
        "            logger.info(f\"📄 문서명: {document_name}\")\n",
        "            logger.info(f\"🔍 위법 조항 발견: {len(analysis_result.get('violations', []))}개\")\n",
        "            logger.info(f\"⏱️ 전체 분석 시간: {performance_stats.get('total_time', 0):.2f}초\")\n",
        "            logger.info(f\"   - Chain 1 (위법 조항 식별): {performance_stats.get('chain1_time', 0):.2f}초\")\n",
        "            logger.info(f\"   - Chain 2 (법령 검색): {performance_stats.get('chain2_time', 0):.2f}초\")  \n",
        "            logger.info(f\"   - Chain 3 (상세 분석): {performance_stats.get('chain3_time', 0):.2f}초\")\n",
        "            \n",
        "            # 개별 위법 조항 요약\n",
        "            if analysis_result.get(\"violations\"):\n",
        "                logger.info(f\"📊 발견된 위법 조항:\")\n",
        "                for i, violation in enumerate(analysis_result[\"violations\"], 1):\n",
        "                    logger.info(f\"   {i}. {violation.get('조항_위치', '')} - {violation.get('리스크_유형', '')}\")\n",
        "            \n",
        "            logger.info(f\"💾 결과 저장: {result_file}\")\n",
        "            logger.info(\"=\" * 50)\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ 체인 분석 결과 저장 실패: {str(e)}\")\n",
        "    \n",
        "    # 메서드들을 클래스에 추가\n",
        "    rag_class._chain3_detailed_analysis = _chain3_detailed_analysis\n",
        "    rag_class._save_chain_analysis_results = _save_chain_analysis_results\n",
        "    return rag_class\n",
        "\n",
        "# RAGPipeline에 체인 헬퍼 메서드 추가 (Part 3)\n",
        "RAGPipeline = add_chain_helper_methods_part3(RAGPipeline)\n",
        "print(\"✅ RAGPipeline 체인 헬퍼 메서드 추가 완료 (Part 3: Chain 3 및 결과 저장)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 파이프라인 실행 섹션\n",
        "\n",
        "이제 모든 클래스와 메서드가 정의되었습니다. 아래 셀들을 순서대로 실행하여 각 단계를 수행할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 파이프라인 인스턴스 생성 및 환경 확인\n",
        "\n",
        "# 비동기 실행을 위한 nest_asyncio 적용\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# 파이프라인 인스턴스 생성\n",
        "pipeline = RAGPipeline()\n",
        "\n",
        "print(\"✅ RAG 파이프라인 인스턴스 생성 완료\")\n",
        "print(f\"📁 문서 루트: {DOCS_ROOT}\")\n",
        "\n",
        "# 환경변수 확인\n",
        "required_env_vars = [\n",
        "    \"DOC_CONVERTER_URL\", \"DOC_PARSER_URL\", \"API_URL\",\n",
        "    \"DATABASE_URL\", \"DATABASE_USER\", \"DATABASE_PASSWORD\", \n",
        "    \"DATABASE_HOST\", \"DATABASE_PORT\", \"DATABASE_NAME\"\n",
        "]\n",
        "\n",
        "print(\"\\n📋 환경변수 확인:\")\n",
        "for var in required_env_vars:\n",
        "    value = os.getenv(var)\n",
        "    if value:\n",
        "        # 비밀번호는 마스킹\n",
        "        if \"PASSWORD\" in var:\n",
        "            value = \"*\" * len(value)\n",
        "        print(f\"  ✅ {var}: {value}\")\n",
        "    else:\n",
        "        print(f\"  ⚠️ {var}: 설정되지 않음\")\n",
        "\n",
        "# 문서 파일 확인\n",
        "document_files = pipeline.find_all_documents()\n",
        "print(f\"\\n📁 발견된 문서 파일: {len(document_files)}개\")\n",
        "\n",
        "if document_files:\n",
        "    # 파일 타입별 분류\n",
        "    type_counts = {}\n",
        "    for file_path in document_files:\n",
        "        doc_type = pipeline.client.determine_doc_type(file_path)\n",
        "        type_counts[doc_type] = type_counts.get(doc_type, 0) + 1\n",
        "    \n",
        "    print(\"📊 문서 타입별 분류:\")\n",
        "    for doc_type, count in type_counts.items():\n",
        "        print(f\"  - {doc_type}: {count}개\")\n",
        "    \n",
        "    print(\"\\n📄 문서 파일 목록 (처음 5개):\")\n",
        "    for i, file_path in enumerate(document_files[:5], 1):\n",
        "        doc_type = pipeline.client.determine_doc_type(file_path)\n",
        "        print(f\"  {i}. {file_path.name} ({doc_type})\")\n",
        "    \n",
        "    if len(document_files) > 5:\n",
        "        print(f\"  ... 및 {len(document_files) - 5}개 파일 더\")\n",
        "else:\n",
        "    print(\"⚠️ 처리할 문서 파일이 없습니다.\")\n",
        "    print(f\"   다음 폴더에 문서 파일을 넣어주세요: {DOCS_ROOT / '근거 자료'}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Step 1: 근거자료 저장\n",
        "\n",
        "문서 파일들을 분석하고 데이터베이스에 저장합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1 실행: 근거자료 저장\n",
        "\n",
        "FORCE_REPROCESS = False  # 강제 재처리 여부 (True로 설정하면 기존 파일도 다시 처리)\n",
        "\n",
        "print(\"🚀 Step 1: 근거자료 저장 시작\")\n",
        "print(f\"⚙️ 강제 재처리: {'예' if FORCE_REPROCESS else '아니오'}\")\n",
        "\n",
        "try:\n",
        "    success = await pipeline.step1_store_reference_materials(force=FORCE_REPROCESS)\n",
        "    \n",
        "    if success:\n",
        "        print(\"\\n🎉 Step 1 완료!\")\n",
        "        print(f\"📄 처리된 파일: {len(pipeline.results)}개\")\n",
        "        \n",
        "        # 성공/실패 통계\n",
        "        successful = len([r for r in pipeline.results if r.success])\n",
        "        failed = len([r for r in pipeline.results if not r.success])\n",
        "        \n",
        "        print(f\"✅ 성공: {successful}개\")\n",
        "        print(f\"❌ 실패: {failed}개\")\n",
        "        \n",
        "        if failed > 0:\n",
        "            print(\"\\n❌ 실패한 파일들:\")\n",
        "            for result in pipeline.results:\n",
        "                if not result.success:\n",
        "                    print(f\"  - {result.filename}: {result.error_message}\")\n",
        "        \n",
        "        # 총 처리 시간\n",
        "        total_time = sum(r.processing_time for r in pipeline.results)\n",
        "        print(f\"\\n⏱️ 총 처리 시간: {total_time:.2f}초\")\n",
        "        \n",
        "        # 타입별 통계\n",
        "        type_stats = {}\n",
        "        for result in pipeline.results:\n",
        "            doc_type = result.doc_type\n",
        "            if doc_type not in type_stats:\n",
        "                type_stats[doc_type] = {\"total\": 0, \"success\": 0}\n",
        "            type_stats[doc_type][\"total\"] += 1\n",
        "            if result.success:\n",
        "                type_stats[doc_type][\"success\"] += 1\n",
        "        \n",
        "        print(\"\\n📊 타입별 성공률:\")\n",
        "        for doc_type, stats in type_stats.items():\n",
        "            success_rate = stats[\"success\"] / stats[\"total\"] * 100\n",
        "            print(f\"  - {doc_type}: {stats['success']}/{stats['total']} ({success_rate:.1f}%)\")\n",
        "        \n",
        "    else:\n",
        "        print(\"❌ Step 1 실패!\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"❌ Step 1 실행 중 오류 발생: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Step 2: 검색 테스트\n",
        "\n",
        "저장된 문서들에 대한 검색 기능을 테스트합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2 실행: 검색 테스트\n",
        "\n",
        "print(\"🔍 Step 2: 검색 테스트 시작\")\n",
        "\n",
        "try:\n",
        "    success = await pipeline.step2_test_search()\n",
        "    \n",
        "    if success:\n",
        "        print(\"🎉 Step 2 검색 테스트 완료!\")\n",
        "    else:\n",
        "        print(\"❌ Step 2 실패!\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"❌ Step 2 실행 중 오류 발생: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 개별 검색 테스트 (원하는 쿼리로 직접 테스트)\n",
        "\n",
        "SEARCH_QUERY = \"개인정보보호법\"  # 원하는 검색어로 변경\n",
        "TOP_K = 5  # 반환할 결과 수\n",
        "DOC_TYPES = None  # 특정 문서 타입만 검색하려면 [\"law\", \"standard_contract\"] 등으로 설정\n",
        "\n",
        "print(f\"🔍 개별 검색 테스트\")\n",
        "print(f\"검색어: '{SEARCH_QUERY}'\")\n",
        "print(f\"결과 수: {TOP_K}개\")\n",
        "print(f\"문서 타입 필터: {DOC_TYPES if DOC_TYPES else '모든 타입'}\")\n",
        "\n",
        "try:\n",
        "    # 검색 실행\n",
        "    start_time = time.time()\n",
        "    result = await pipeline.client.search_documents_direct(\n",
        "        query=SEARCH_QUERY,\n",
        "        top_k=TOP_K,\n",
        "        doc_types=DOC_TYPES\n",
        "    )\n",
        "    search_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"\\n⏱️ 검색 시간: {search_time:.3f}초\")\n",
        "    print(f\"📊 검색 결과: {len(result.get('results', []))}개\")\n",
        "    \n",
        "    # 결과 출력\n",
        "    for i, doc in enumerate(result.get(\"results\", []), 1):\n",
        "        print(f\"\\n📄 {i}. {doc.get('filename', 'Unknown')}\")\n",
        "        print(f\"   📂 타입: {doc.get('doc_type', 'Unknown')}\")\n",
        "        print(f\"   🎯 유사도: {doc.get('similarity_score', 0):.4f}\")\n",
        "        print(f\"   📝 내용 미리보기:\")\n",
        "        content = doc.get('content', '')[:200] + \"...\" if len(doc.get('content', '')) > 200 else doc.get('content', '')\n",
        "        print(f\"   {content}\")\n",
        "    \n",
        "    if not result.get(\"results\"):\n",
        "        print(\"❌ 검색 결과가 없습니다.\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"❌ 검색 실패: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Step 3: 계약서 검토 테스트 (3단계 체인 방식)\n",
        "\n",
        "3단계 체인 방식으로 계약서의 위법 조항을 분석합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3 실행: 계약서 검토 테스트 (3단계 체인 방식)\n",
        "\n",
        "print(\"📝 Step 3: 계약서 검토 테스트 시작\")\n",
        "\n",
        "try:\n",
        "    success = await pipeline.step3_contract_review_test()\n",
        "    \n",
        "    if success:\n",
        "        print(\"🎉 Step 3 계약서 검토 테스트 완료!\")\n",
        "    else:\n",
        "        print(\"❌ Step 3 실패!\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"❌ Step 3 실행 중 오류 발생: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 결과 분석 및 시각화\n",
        "\n",
        "실행된 단계들의 결과를 분석하고 시각화합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 결과 분석 및 시각화\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "\n",
        "# 한글 폰트 설정 (시스템에 따라 조정 필요)\n",
        "plt.rcParams['font.family'] = ['DejaVu Sans', 'Malgun Gothic', 'NanumGothic']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "print(\"📊 결과 분석 시작\")\n",
        "\n",
        "# 1. 처리 결과 요약 파일 확인\n",
        "summary_file = RESULTS_DIR / \"processing_summary.json\"\n",
        "search_results_files = list(RESULTS_DIR.glob(\"search_test_results_*.json\"))\n",
        "chain_results_files = list(RESULTS_DIR.glob(\"chain_analysis_*.json\"))\n",
        "\n",
        "if summary_file.exists():\n",
        "    # 처리 결과 분석\n",
        "    with open(summary_file, 'r', encoding='utf-8') as f:\n",
        "        summary = json.load(f)\n",
        "    \n",
        "    print(f\"\\n📄 문서 처리 결과 요약:\")\n",
        "    print(f\"  전체 파일: {summary['total_files']}개\")\n",
        "    print(f\"  성공: {summary['successful']}개\")\n",
        "    print(f\"  실패: {summary['failed']}개\")\n",
        "    print(f\"  총 처리 시간: {summary['total_processing_time']:.2f}초\")\n",
        "    \n",
        "    # 문서 타입별 성공률 시각화\n",
        "    if summary.get('type_statistics'):\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "        \n",
        "        # 타입별 파일 수\n",
        "        doc_types = list(summary['type_statistics'].keys())\n",
        "        totals = [summary['type_statistics'][dt]['total'] for dt in doc_types]\n",
        "        \n",
        "        ax1.bar(doc_types, totals, color='skyblue')\n",
        "        ax1.set_title('문서 타입별 파일 수')\n",
        "        ax1.set_xlabel('문서 타입')\n",
        "        ax1.set_ylabel('파일 수')\n",
        "        ax1.tick_params(axis='x', rotation=45)\n",
        "        \n",
        "        # 타입별 성공률\n",
        "        success_rates = [summary['type_statistics'][dt]['success'] / summary['type_statistics'][dt]['total'] * 100 \n",
        "                        for dt in doc_types]\n",
        "        \n",
        "        ax2.bar(doc_types, success_rates, color='lightgreen')\n",
        "        ax2.set_title('문서 타입별 성공률')\n",
        "        ax2.set_xlabel('문서 타입')\n",
        "        ax2.set_ylabel('성공률 (%)')\n",
        "        ax2.set_ylim(0, 100)\n",
        "        ax2.tick_params(axis='x', rotation=45)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # 상세 통계 출력\n",
        "        print(f\"\\n📊 문서 타입별 통계:\")\n",
        "        for doc_type, stats in summary['type_statistics'].items():\n",
        "            success_rate = stats['success'] / stats['total'] * 100\n",
        "            print(f\"  {doc_type}: {stats['success']}/{stats['total']} ({success_rate:.1f}%)\")\n",
        "    \n",
        "else:\n",
        "    print(\"⚠️ 처리 결과 요약 파일이 없습니다. Step 1을 먼저 실행해주세요.\")\n",
        "\n",
        "# 2. 검색 결과 분석\n",
        "if search_results_files:\n",
        "    # 가장 최근 검색 결과 파일 사용\n",
        "    latest_search_file = sorted(search_results_files)[-1]\n",
        "    \n",
        "    with open(latest_search_file, 'r', encoding='utf-8') as f:\n",
        "        search_data = json.load(f)\n",
        "    \n",
        "    print(f\"\\n🔍 검색 테스트 결과 요약:\")\n",
        "    test_summary = search_data['test_summary']\n",
        "    print(f\"  전체 쿼리: {test_summary['total_queries']}개\")\n",
        "    print(f\"  성공: {test_summary['successful_queries']}개\")\n",
        "    print(f\"  실패: {test_summary['failed_queries']}개\")\n",
        "    print(f\"  성공률: {test_summary['success_rate']*100:.1f}%\")\n",
        "    print(f\"  평균 검색 시간: {test_summary['avg_search_time_seconds']:.3f}초\")\n",
        "    print(f\"  평균 결과 수: {test_summary['avg_results_per_query']:.1f}개\")\n",
        "    \n",
        "    # 카테고리별 성공률 시각화\n",
        "    if search_data.get('category_performance'):\n",
        "        categories = list(search_data['category_performance'].keys())\n",
        "        success_rates = [search_data['category_performance'][cat]['success_rate'] * 100 \n",
        "                        for cat in categories]\n",
        "        \n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.bar(categories, success_rates, color='orange')\n",
        "        plt.title('검색 카테고리별 성공률')\n",
        "        plt.xlabel('카테고리')\n",
        "        plt.ylabel('성공률 (%)')\n",
        "        plt.ylim(0, 100)\n",
        "        plt.tick_params(axis='x', rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        print(f\"\\n📈 카테고리별 성공률:\")\n",
        "        for category, performance in search_data['category_performance'].items():\n",
        "            success_rate = performance['success_rate'] * 100\n",
        "            print(f\"  {category}: {performance['success_count']}/{performance['total_count']} ({success_rate:.1f}%)\")\n",
        "    \n",
        "else:\n",
        "    print(\"⚠️ 검색 결과 파일이 없습니다. Step 2를 먼저 실행해주세요.\")\n",
        "\n",
        "# 3. 체인 분석 결과\n",
        "if chain_results_files:\n",
        "    latest_chain_file = sorted(chain_results_files)[-1]\n",
        "    \n",
        "    with open(latest_chain_file, 'r', encoding='utf-8') as f:\n",
        "        chain_data = json.load(f)\n",
        "    \n",
        "    print(f\"\\n📝 체인 분석 결과 요약:\")\n",
        "    summary_data = chain_data['summary']\n",
        "    performance = chain_data['metadata']['performance']\n",
        "    \n",
        "    print(f\"  문서명: {chain_data['metadata']['document_name']}\")\n",
        "    print(f\"  위법 조항 발견: {summary_data['violations_found']}개\")\n",
        "    print(f\"  전체 분석 시간: {summary_data['total_analysis_time']:.2f}초\")\n",
        "    print(f\"  Chain 1 시간: {performance['chain1_time']:.2f}초\")\n",
        "    print(f\"  Chain 2 시간: {performance['chain2_time']:.2f}초\")\n",
        "    print(f\"  Chain 3 시간: {performance['chain3_time']:.2f}초\")\n",
        "    \n",
        "    # 체인별 시간 분포 시각화\n",
        "    chain_times = [performance['chain1_time'], performance['chain2_time'], performance['chain3_time']]\n",
        "    chain_labels = ['Chain 1\\n(위법조항 식별)', 'Chain 2\\n(법령 검색)', 'Chain 3\\n(상세 분석)']\n",
        "    \n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.bar(chain_labels, chain_times, color=['red', 'blue', 'green'])\n",
        "    plt.title('체인별 분석 시간')\n",
        "    plt.xlabel('분석 단계')\n",
        "    plt.ylabel('시간 (초)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # 발견된 위법 조항 요약\n",
        "    violations = chain_data['analysis_result'].get('violations', [])\n",
        "    if violations:\n",
        "        print(f\"\\n📊 발견된 위법 조항:\")\n",
        "        for i, violation in enumerate(violations, 1):\n",
        "            print(f\"  {i}. {violation.get('조항_위치', '')} - {violation.get('리스크_유형', '')}\")\n",
        "    \n",
        "else:\n",
        "    print(\"⚠️ 체인 분석 결과 파일이 없습니다. Step 3을 먼저 실행해주세요.\")\n",
        "\n",
        "# 4. 결과 파일 목록\n",
        "print(f\"\\n📁 생성된 결과 파일들:\")\n",
        "result_files = list(RESULTS_DIR.glob(\"*\"))\n",
        "for result_file in sorted(result_files):\n",
        "    file_size = result_file.stat().st_size\n",
        "    file_size_mb = file_size / (1024 * 1024)\n",
        "    print(f\"  📄 {result_file.name} ({file_size_mb:.2f} MB)\")\n",
        "\n",
        "print(f\"\\n📊 분석 완료! 결과는 {RESULTS_DIR} 폴더에서 확인할 수 있습니다.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 추가 유틸리티\n",
        "\n",
        "데이터베이스 상태 확인, 파일 정리 등 유용한 유틸리티 기능들입니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 데이터베이스 상태 확인\n",
        "\n",
        "async def check_database_status():\n",
        "    \"\"\"데이터베이스 상태 및 저장된 문서 확인\"\"\"\n",
        "    try:\n",
        "        from sqlmodel.ext.asyncio.session import AsyncSession\n",
        "        from sqlalchemy.ext.asyncio import create_async_engine\n",
        "        from src.models import Document, Chunk\n",
        "        from sqlmodel import select, func\n",
        "        \n",
        "        database_url = os.getenv(\"DATABASE_URL\") or f\"postgresql+asyncpg://{os.getenv('DATABASE_USER', 'postgres')}:{os.getenv('DATABASE_PASSWORD', 'postgres')}@{os.getenv('DATABASE_HOST', 'localhost')}:{os.getenv('DATABASE_PORT', '5434')}/{os.getenv('DATABASE_NAME', 'smartclm-poc')}\"\n",
        "        async_engine = create_async_engine(database_url, echo=False)\n",
        "        \n",
        "        async with AsyncSession(async_engine) as session:\n",
        "            # 문서 수 확인\n",
        "            doc_count_query = select(func.count(Document.id))\n",
        "            doc_result = await session.exec(doc_count_query)\n",
        "            total_docs = doc_result.one()\n",
        "            \n",
        "            # 타입별 문서 수\n",
        "            type_query = select(Document.doc_type, func.count(Document.id)).group_by(Document.doc_type)\n",
        "            type_result = await session.exec(type_query)\n",
        "            type_counts = {doc_type: count for doc_type, count in type_result.all()}\n",
        "            \n",
        "            # 청크 수 확인\n",
        "            chunk_count_query = select(func.count(Chunk.id))\n",
        "            chunk_result = await session.exec(chunk_count_query)\n",
        "            total_chunks = chunk_result.one()\n",
        "            \n",
        "            # 처리 상태별 문서 수\n",
        "            status_query = select(Document.processing_status, func.count(Document.id)).group_by(Document.processing_status)\n",
        "            status_result = await session.exec(status_query)\n",
        "            status_counts = {status: count for status, count in status_result.all()}\n",
        "            \n",
        "            print(\"📊 데이터베이스 상태:\")\n",
        "            print(f\"  총 문서 수: {total_docs}개\")\n",
        "            print(f\"  총 청크 수: {total_chunks}개\")\n",
        "            \n",
        "            print(\"\\n📋 문서 타입별 분포:\")\n",
        "            for doc_type, count in type_counts.items():\n",
        "                print(f\"  - {doc_type}: {count}개\")\n",
        "            \n",
        "            print(\"\\n⚙️ 처리 상태별 분포:\")\n",
        "            for status, count in status_counts.items():\n",
        "                print(f\"  - {status}: {count}개\")\n",
        "            \n",
        "            # 최근 추가된 문서들\n",
        "            recent_query = select(Document.filename, Document.doc_type, Document.processing_status).order_by(Document.id.desc()).limit(5)\n",
        "            recent_result = await session.exec(recent_query)\n",
        "            recent_docs = recent_result.all()\n",
        "            \n",
        "            if recent_docs:\n",
        "                print(\"\\n🆕 최근 추가된 문서 (최대 5개):\")\n",
        "                for i, (filename, doc_type, status) in enumerate(recent_docs, 1):\n",
        "                    print(f\"  {i}. {filename} ({doc_type}) - {status}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"❌ 데이터베이스 상태 확인 실패: {str(e)}\")\n",
        "\n",
        "# 실행\n",
        "print(\"🔍 데이터베이스 상태 확인 중...\")\n",
        "await check_database_status()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 파일 정리 및 유틸리티\n",
        "\n",
        "def cleanup_temporary_files():\n",
        "    \"\"\"임시 파일들 정리\"\"\"\n",
        "    try:\n",
        "        import shutil\n",
        "        \n",
        "        # 변환된 PDF 파일들 정리\n",
        "        converted_pdfs = list(PROCESSED_DIR.glob(\"*_converted.pdf\"))\n",
        "        for pdf_file in converted_pdfs:\n",
        "            pdf_file.unlink()\n",
        "            print(f\"🗑️ 임시 PDF 삭제: {pdf_file.name}\")\n",
        "        \n",
        "        print(f\"✅ 임시 파일 정리 완료: {len(converted_pdfs)}개 파일 삭제\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ 파일 정리 실패: {str(e)}\")\n",
        "\n",
        "def show_results_summary():\n",
        "    \"\"\"결과 파일들 요약 출력\"\"\"\n",
        "    print(\"📁 결과 파일 요약:\")\n",
        "    \n",
        "    # 처리 요약\n",
        "    summary_file = RESULTS_DIR / \"processing_summary.json\"\n",
        "    if summary_file.exists():\n",
        "        print(f\"  ✅ 처리 요약: {summary_file.name}\")\n",
        "    \n",
        "    # 검색 테스트 결과\n",
        "    search_files = list(RESULTS_DIR.glob(\"search_test_results_*.json\"))\n",
        "    print(f\"  🔍 검색 테스트 결과: {len(search_files)}개 파일\")\n",
        "    \n",
        "    # 체인 분석 결과\n",
        "    chain_files = list(RESULTS_DIR.glob(\"chain_analysis_*.json\"))\n",
        "    print(f\"  📝 체인 분석 결과: {len(chain_files)}개 파일\")\n",
        "    \n",
        "    # 처리된 문서들\n",
        "    processed_folders = [f for f in PROCESSED_DIR.iterdir() if f.is_dir()]\n",
        "    print(f\"  📄 처리된 문서 타입: {len(processed_folders)}개 폴더\")\n",
        "    for folder in processed_folders:\n",
        "        doc_count = len(list(folder.glob(\"*\")))\n",
        "        print(f\"    - {folder.name}: {doc_count}개 파일\")\n",
        "\n",
        "# 전체 파이프라인 상태 요약\n",
        "def show_pipeline_status():\n",
        "    \"\"\"전체 파이프라인 상태 요약\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"🎯 Smart CLM RAG 파이프라인 상태 요약\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Step 1 상태\n",
        "    if hasattr(pipeline, 'results') and pipeline.results:\n",
        "        successful = len([r for r in pipeline.results if r.success])\n",
        "        total = len(pipeline.results)\n",
        "        print(f\"📄 Step 1 (문서 저장): {successful}/{total} 성공\")\n",
        "    else:\n",
        "        print(\"📄 Step 1 (문서 저장): 미실행\")\n",
        "    \n",
        "    # Step 2 상태\n",
        "    search_files = list(RESULTS_DIR.glob(\"search_test_results_*.json\"))\n",
        "    if search_files:\n",
        "        print(f\"🔍 Step 2 (검색 테스트): 완료 ({len(search_files)}개 결과)\")\n",
        "    else:\n",
        "        print(\"🔍 Step 2 (검색 테스트): 미실행\")\n",
        "    \n",
        "    # Step 3 상태\n",
        "    chain_files = list(RESULTS_DIR.glob(\"chain_analysis_*.json\"))\n",
        "    if chain_files:\n",
        "        print(f\"📝 Step 3 (계약서 검토): 완료 ({len(chain_files)}개 결과)\")\n",
        "    else:\n",
        "        print(\"📝 Step 3 (계약서 검토): 미실행\")\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "\n",
        "# 실행\n",
        "print(\"🧹 파일 정리 중...\")\n",
        "cleanup_temporary_files()\n",
        "\n",
        "print(\"\\n📊 결과 요약:\")\n",
        "show_results_summary()\n",
        "\n",
        "print(\"\\n📈 파이프라인 상태:\")\n",
        "show_pipeline_status()\n",
        "\n",
        "print(f\"\\n🎉 노트북 실행 완료! 모든 결과는 {RESULTS_DIR} 폴더에서 확인할 수 있습니다.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
