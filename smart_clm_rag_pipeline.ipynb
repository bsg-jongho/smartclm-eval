{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Smart CLM RAG íŒŒì´í”„ë¼ì¸\n",
        "\n",
        "ê·¼ê±°ìë£Œ ì €ì¥ ë° ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ í†µí•© ë…¸íŠ¸ë¶\n",
        "\n",
        "## ì£¼ìš” ê¸°ëŠ¥\n",
        "1. **Step 1**: ê·¼ê±°ìë£Œ ì €ì¥ - ë¬¸ì„œ íŒŒì¼ë“¤ì„ ë¶„ì„í•˜ê³  ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥\n",
        "2. **Step 2**: ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ - ì €ì¥ëœ ë¬¸ì„œë“¤ì— ëŒ€í•œ ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸  \n",
        "3. **Step 3**: ê³„ì•½ì„œ ê²€í†  í…ŒìŠ¤íŠ¸ - 3ë‹¨ê³„ ì²´ì¸ ë°©ì‹ìœ¼ë¡œ ê³„ì•½ì„œ ìœ„ë²• ì¡°í•­ ë¶„ì„\n",
        "4. **Step 4**: ê²°ê³¼ ì‹œê°í™” ë° ë¶„ì„\n",
        "\n",
        "## ì‚¬ìš© ë°©ë²•\n",
        "ê° ì…€ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ì—¬ íŒŒì´í”„ë¼ì¸ì„ ë‹¨ê³„ë³„ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
        "import asyncio\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# í™˜ê²½ë³€ìˆ˜ ë¡œë”©\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "from typing import Dict, List, Any, Optional\n",
        "import httpx\n",
        "from dataclasses import dataclass\n",
        "import pandas as pd\n",
        "\n",
        "# ë¡œê¹… ì„¤ì •\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
        "    handlers=[\n",
        "        logging.StreamHandler(sys.stdout),\n",
        "        logging.FileHandler(\"rag_pipeline.log\")\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì„¤ì •\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "DOCS_ROOT = PROJECT_ROOT / \"docs\"\n",
        "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
        "PROCESSED_DIR = PROJECT_ROOT / \"processed\"\n",
        "\n",
        "# ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
        "RESULTS_DIR.mkdir(exist_ok=True)\n",
        "PROCESSED_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ\")\n",
        "print(f\"ğŸ“ í”„ë¡œì íŠ¸ ë£¨íŠ¸: {PROJECT_ROOT}\")\n",
        "print(f\"ğŸ“ ë¬¸ì„œ í´ë”: {DOCS_ROOT}\")\n",
        "print(f\"ğŸ“ ê²°ê³¼ í´ë”: {RESULTS_DIR}\")\n",
        "print(f\"ğŸ“ ì²˜ë¦¬ í´ë”: {PROCESSED_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë¬¸ì„œ íƒ€ì… ë§¤í•‘ ë° ë°ì´í„° í´ë˜ìŠ¤ ì •ì˜\n",
        "\n",
        "# ë¬¸ì„œ íƒ€ì… ë§¤í•‘ (í´ë” ê²½ë¡œ -> ë¬¸ì„œ íƒ€ì…)\n",
        "DOCUMENT_TYPE_MAPPING = {\n",
        "    \"ê·¼ê±° ìë£Œ/ë²•ë ¹\": \"law\",  # ë²•ë ¹ íŒŒì¼ë“¤\n",
        "    \"ê·¼ê±°ìë£Œ/ë²•ë ¹\": \"law\",  # ë²•ë ¹ íŒŒì¼ë“¤ (ê³µë°± ì—†ëŠ” ë²„ì „)\n",
        "    \"ê·¼ê±° ìë£Œ/ì²´ê²°ê³„ì•½\": \"executed_contract\",  # ì²´ê²°ëœ ê³„ì•½ì„œë“¤\n",
        "    \"ê·¼ê±°ìë£Œ/ì²´ê²°ê³„ì•½\": \"executed_contract\",  # ì²´ê²°ëœ ê³„ì•½ì„œë“¤ (ê³µë°± ì—†ëŠ” ë²„ì „)\n",
        "    \"ê·¼ê±° ìë£Œ/í‘œì¤€ê³„ì•½ì„œ\": \"standard_contract\",  # í‘œì¤€ ê³„ì•½ì„œ í…œí”Œë¦¿ë“¤\n",
        "    \"ê·¼ê±°ìë£Œ/í‘œì¤€ê³„ì•½ì„œ\": \"standard_contract\",  # í‘œì¤€ ê³„ì•½ì„œ í…œí”Œë¦¿ë“¤ (ê³µë°± ì—†ëŠ” ë²„ì „)\n",
        "    \"1. NDA\": \"standard_contract\",  # NDA í‘œì¤€ê³„ì•½ì„œ (ê¸°ì¡´ í´ë”)\n",
        "    \"2. ì œí’ˆ(ì„œë¹„ìŠ¤) íŒë§¤\": \"standard_contract\",  # íŒë§¤ê³„ì•½ì„œ í‘œì¤€ (ê¸°ì¡´ í´ë”)\n",
        "}\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ProcessingResult:\n",
        "    \"\"\"ë¬¸ì„œ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤\"\"\"\n",
        "    filename: str\n",
        "    doc_type: str\n",
        "    folder_path: str\n",
        "    success: bool\n",
        "    processing_time: float\n",
        "    page_count: Optional[int] = None\n",
        "    chunk_count: Optional[int] = None\n",
        "    error_message: Optional[str] = None\n",
        "    doc_parser_result: Optional[Dict] = None\n",
        "\n",
        "print(\"âœ… ë°ì´í„° í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ExternalServiceClient í´ë˜ìŠ¤ ì •ì˜ (ì™„ì „í•œ í´ë˜ìŠ¤)\n",
        "\n",
        "class ExternalServiceClient:\n",
        "    \"\"\"ì™¸ë¶€ ì„œë¹„ìŠ¤ í´ë¼ì´ì–¸íŠ¸ í†µí•© í´ë˜ìŠ¤\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.doc_converter_url = os.getenv(\"DOC_CONVERTER_URL\", \"http://localhost:8001\")\n",
        "        self.doc_parser_url = os.getenv(\"DOC_PARSER_URL\", \"http://localhost:8002\")\n",
        "        self.api_url = os.getenv(\"API_URL\", \"http://localhost:8000\")\n",
        "        self.timeout = 600.0\n",
        "    \n",
        "    async def health_check_all(self) -> Dict[str, bool]:\n",
        "        \"\"\"ëª¨ë“  ì„œë¹„ìŠ¤ì˜ í—¬ìŠ¤ì²´í¬\"\"\"\n",
        "        services = {\n",
        "            \"doc-converter\": f\"{self.doc_converter_url}/health\",\n",
        "            \"doc-parser\": f\"{self.doc_parser_url}/health\",\n",
        "            \"api\": f\"{self.api_url}/health\"\n",
        "        }\n",
        "        \n",
        "        results = {}\n",
        "        async with httpx.AsyncClient(timeout=10.0) as client:\n",
        "            for service, url in services.items():\n",
        "                try:\n",
        "                    response = await client.get(url)\n",
        "                    results[service] = response.status_code == 200\n",
        "                    logger.info(f\"âœ… {service} ì„œë¹„ìŠ¤ ì •ìƒ\")\n",
        "                except Exception as e:\n",
        "                    results[service] = False\n",
        "                    logger.error(f\"âŒ {service} ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {e}\")\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def determine_doc_type(self, file_path: Path) -> str:\n",
        "        \"\"\"íŒŒì¼ ê²½ë¡œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œ íƒ€ì… ê²°ì •\"\"\"\n",
        "        # ìƒëŒ€ ê²½ë¡œ ê³„ì‚° (docs í´ë” ê¸°ì¤€)\n",
        "        relative_path = file_path.relative_to(DOCS_ROOT)\n",
        "        folder_path = str(relative_path.parent)\n",
        "        \n",
        "        # í´ë” ê²½ë¡œ ë§¤í•‘ì—ì„œ ë¬¸ì„œ íƒ€ì… ì°¾ê¸°\n",
        "        for pattern, doc_type in DOCUMENT_TYPE_MAPPING.items():\n",
        "            if folder_path.startswith(pattern) or folder_path == pattern:\n",
        "                return doc_type\n",
        "        \n",
        "        # ê¸°ë³¸ê°’: law (ê·¼ê±°ìë£Œ)\n",
        "        return \"law\"\n",
        "\n",
        "    async def search_documents_direct(self, query: str, top_k: int = 5, doc_types: List[str] = None) -> Dict:\n",
        "        \"\"\"ë°ì´í„°ë² ì´ìŠ¤ ì§ì ‘ ì ‘ê·¼ìœ¼ë¡œ ë¬¸ì„œ ê²€ìƒ‰\"\"\"\n",
        "        try:\n",
        "            # ì§ì ‘ ì„ë² ë”© ì„œë¹„ìŠ¤ ë° ê²€ìƒ‰ ë¡œì§ êµ¬í˜„\n",
        "            from sqlmodel.ext.asyncio.session import AsyncSession\n",
        "            from sqlalchemy.ext.asyncio import create_async_engine\n",
        "            from src.aws.embedding_service import TitanEmbeddingService\n",
        "            from sqlalchemy import text\n",
        "            \n",
        "            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°\n",
        "            database_url = os.getenv(\"DATABASE_URL\") or f\"postgresql+asyncpg://{os.getenv('DATABASE_USER', 'postgres')}:{os.getenv('DATABASE_PASSWORD', 'postgres')}@{os.getenv('DATABASE_HOST', 'localhost')}:{os.getenv('DATABASE_PORT', '5434')}/{os.getenv('DATABASE_NAME', 'smartclm-poc')}\"\n",
        "            async_engine = create_async_engine(database_url, echo=False)\n",
        "            \n",
        "            # ì„ë² ë”© ì„œë¹„ìŠ¤ ì´ˆê¸°í™”\n",
        "            embedding_service = TitanEmbeddingService()\n",
        "            \n",
        "            async with AsyncSession(async_engine) as session:\n",
        "                # 1. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±\n",
        "                query_embedding = await embedding_service.create_single_embedding(query)\n",
        "                \n",
        "                # 2. ë²¡í„° ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±\n",
        "                base_query = \"\"\"\n",
        "                SELECT \n",
        "                    c.id as chunk_id,\n",
        "                    c.content,\n",
        "                    c.chunk_type,\n",
        "                    c.parent_id,\n",
        "                    d.id as document_id,\n",
        "                    d.filename,\n",
        "                    d.doc_type,\n",
        "                    d.category,\n",
        "                    (1 - (c.embedding <=> :query_embedding)) as similarity_score\n",
        "                FROM chunks c\n",
        "                JOIN documents d ON c.document_id = d.id\n",
        "                WHERE c.embedding IS NOT NULL\n",
        "                AND d.room_id IS NULL \n",
        "                \"\"\"\n",
        "                \n",
        "                # ë²¡í„°ë¥¼ pgvector í˜•ì‹ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
        "                vector_str = \"[\" + \",\".join(map(str, query_embedding)) + \"]\"\n",
        "                params = {\"query_embedding\": vector_str}\n",
        "                \n",
        "                # ë¬¸ì„œ ìœ í˜• í•„í„°\n",
        "                if doc_types:\n",
        "                    type_conditions = []\n",
        "                    for i, doc_type in enumerate(doc_types):\n",
        "                        param_name = f\"doc_type_{i}\"\n",
        "                        type_conditions.append(f\"d.doc_type = :{param_name}\")\n",
        "                        params[param_name] = doc_type\n",
        "                    base_query += f\" AND ({' OR '.join(type_conditions)})\"\n",
        "                \n",
        "                # ìœ ì‚¬ë„ ì •ë ¬ ë° ì œí•œ\n",
        "                base_query += \" ORDER BY similarity_score DESC LIMIT :top_k\"\n",
        "                params[\"top_k\"] = top_k\n",
        "                \n",
        "                # 3. ì¿¼ë¦¬ ì‹¤í–‰\n",
        "                connection = await session.connection()\n",
        "                result = await connection.execute(text(base_query), params)\n",
        "                rows = result.fetchall()\n",
        "                \n",
        "                # 4. ê²°ê³¼ ë³€í™˜\n",
        "                search_results = []\n",
        "                for row in rows:\n",
        "                    search_results.append({\n",
        "                        \"chunk_id\": row.chunk_id,\n",
        "                        \"content\": row.content,\n",
        "                        \"similarity_score\": round(row.similarity_score, 4),\n",
        "                        \"chunk_type\": row.chunk_type,\n",
        "                        \"parent_id\": row.parent_id,\n",
        "                        \"document_id\": row.document_id,\n",
        "                        \"filename\": row.filename,\n",
        "                        \"doc_type\": row.doc_type,\n",
        "                        \"category\": row.category,\n",
        "                    })\n",
        "                \n",
        "                return {\n",
        "                    \"results\": search_results,\n",
        "                    \"total_found\": len(search_results),\n",
        "                    \"query\": query\n",
        "                }\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"âŒ ì§ì ‘ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}\")\n",
        "            return {\"results\": [], \"total_found\": 0, \"error\": str(e)}\n",
        "\n",
        "print(\"âœ… ExternalServiceClient ê¸°ë³¸ ë©”ì„œë“œ ì •ì˜ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ExternalServiceClient í´ë˜ìŠ¤ í™•ì¥ ë©”ì„œë“œë“¤\n",
        "\n",
        "def add_client_methods(client_class):\n",
        "    \"\"\"ExternalServiceClientì— ì¶”ê°€ ë©”ì„œë“œë“¤ì„ ë™ì ìœ¼ë¡œ ì¶”ê°€\"\"\"\n",
        "    \n",
        "    async def convert_to_pdf_if_needed(self, file_path: Path) -> Path:\n",
        "        \"\"\"í•„ìš”ì‹œ PDFë¡œ ë³€í™˜ (DOCX ë“±)\"\"\"\n",
        "        if file_path.suffix.lower() == '.pdf':\n",
        "            return file_path\n",
        "        \n",
        "        logger.info(f\"ğŸ”„ PDF ë³€í™˜ ì¤‘: {file_path.name}\")\n",
        "        \n",
        "        try:\n",
        "            async with httpx.AsyncClient(timeout=self.timeout) as client:\n",
        "                with open(file_path, \"rb\") as f:\n",
        "                    files = {\n",
        "                        \"file\": (file_path.name, f, \"application/octet-stream\")\n",
        "                    }\n",
        "                    \n",
        "                    response = await client.post(\n",
        "                        f\"{self.doc_converter_url}/convert\",\n",
        "                        files=files\n",
        "                    )\n",
        "                \n",
        "                if response.status_code == 200:\n",
        "                    # PDF íŒŒì¼ì„ ì„ì‹œ ì €ì¥\n",
        "                    pdf_path = PROCESSED_DIR / f\"{file_path.stem}_converted.pdf\"\n",
        "                    with open(pdf_path, \"wb\") as f:\n",
        "                        f.write(response.content)\n",
        "                    \n",
        "                    logger.info(f\"âœ… PDF ë³€í™˜ ì™„ë£Œ: {pdf_path.name}\")\n",
        "                    return pdf_path\n",
        "                else:\n",
        "                    raise Exception(f\"ë³€í™˜ ì‹¤íŒ¨: HTTP {response.status_code}\")\n",
        "                    \n",
        "        except Exception as e:\n",
        "            logger.error(f\"âŒ PDF ë³€í™˜ ì‹¤íŒ¨: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    async def analyze_document_file(self, file_path: Path) -> ProcessingResult:\n",
        "        \"\"\"ë¬¸ì„œ íŒŒì¼ì„ ë¶„ì„\"\"\"\n",
        "        start_time = time.time()\n",
        "        doc_type = self.determine_doc_type(file_path)\n",
        "        relative_path = file_path.relative_to(DOCS_ROOT)\n",
        "        folder_path = str(relative_path.parent)\n",
        "        \n",
        "        try:\n",
        "            logger.info(f\"ğŸ“„ ë¬¸ì„œ ë¶„ì„ ì‹œì‘: {file_path.name} (íƒ€ì…: {doc_type})\")\n",
        "            \n",
        "            # PDFë¡œ ë³€í™˜ (í•„ìš”ì‹œ)\n",
        "            pdf_file_path = await self.convert_to_pdf_if_needed(file_path)\n",
        "            \n",
        "            async with httpx.AsyncClient(timeout=self.timeout) as client:\n",
        "                with open(pdf_file_path, \"rb\") as f:\n",
        "                    files = {\n",
        "                        \"file\": (pdf_file_path.name, f, \"application/pdf\")\n",
        "                    }\n",
        "                    data = {\"smart_pipeline\": True}\n",
        "                    \n",
        "                    response = await client.post(\n",
        "                        f\"{self.doc_parser_url}/analyze\",\n",
        "                        files=files,\n",
        "                        data=data\n",
        "                    )\n",
        "                \n",
        "                if response.status_code == 200:\n",
        "                    result = response.json()\n",
        "                    processing_time = time.time() - start_time\n",
        "                    \n",
        "                    return ProcessingResult(\n",
        "                        filename=file_path.name,\n",
        "                        doc_type=doc_type,\n",
        "                        folder_path=folder_path,\n",
        "                        success=True,\n",
        "                        processing_time=processing_time,\n",
        "                        page_count=result.get(\"page_count\"),\n",
        "                        chunk_count=len(result.get(\"chunks\", [])),\n",
        "                        doc_parser_result=result\n",
        "                    )\n",
        "                else:\n",
        "                    error_msg = f\"Doc Parser ì˜¤ë¥˜ (HTTP {response.status_code}): {response.text}\"\n",
        "                    logger.error(error_msg)\n",
        "                    return ProcessingResult(\n",
        "                        filename=file_path.name,\n",
        "                        doc_type=doc_type,\n",
        "                        folder_path=folder_path,\n",
        "                        success=False,\n",
        "                        processing_time=time.time() - start_time,\n",
        "                        error_message=error_msg\n",
        "                    )\n",
        "            \n",
        "            # ì„ì‹œ PDF íŒŒì¼ ì •ë¦¬\n",
        "            if pdf_file_path != file_path and pdf_file_path.exists():\n",
        "                pdf_file_path.unlink()\n",
        "                    \n",
        "        except Exception as e:\n",
        "            error_msg = f\"ë¬¸ì„œ ë¶„ì„ ì‹¤íŒ¨: {str(e)}\"\n",
        "            logger.error(error_msg)\n",
        "            return ProcessingResult(\n",
        "                filename=file_path.name,\n",
        "                doc_type=doc_type,\n",
        "                folder_path=folder_path,\n",
        "                success=False,\n",
        "                processing_time=time.time() - start_time,\n",
        "                error_message=error_msg\n",
        "            )\n",
        "\n",
        "    # ë©”ì„œë“œë“¤ì„ í´ë˜ìŠ¤ì— ì¶”ê°€\n",
        "    client_class.convert_to_pdf_if_needed = convert_to_pdf_if_needed\n",
        "    client_class.analyze_document_file = analyze_document_file\n",
        "    return client_class\n",
        "\n",
        "# ExternalServiceClientì— ë©”ì„œë“œ ì¶”ê°€\n",
        "ExternalServiceClient = add_client_methods(ExternalServiceClient)\n",
        "print(\"âœ… ExternalServiceClient ë³€í™˜ ë° ë¶„ì„ ë©”ì„œë“œ ì¶”ê°€ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ExternalServiceClient save_to_database ë©”ì„œë“œ ì¶”ê°€\n",
        "\n",
        "def add_save_method(client_class):\n",
        "    \"\"\"save_to_database ë©”ì„œë“œ ì¶”ê°€\"\"\"\n",
        "    \n",
        "    async def save_to_database(self, parsed_result: Dict, source_file: Path, doc_type: str) -> bool:\n",
        "        \"\"\"íŒŒì‹±ëœ ê²°ê³¼ë¥¼ ì§ì ‘ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (S3 ì—…ë¡œë“œ ì—†ìŒ)\"\"\"\n",
        "        try:\n",
        "            logger.info(f\"ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹œì‘: {source_file.name} (íƒ€ì…: {doc_type})\")\n",
        "            \n",
        "            # ë¡œì»¬ì—ì„œ ì§ì ‘ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥\n",
        "            from datetime import datetime\n",
        "            from sqlmodel.ext.asyncio.session import AsyncSession\n",
        "            from sqlalchemy.ext.asyncio import create_async_engine\n",
        "            from src.models import Document, Chunk\n",
        "            from src.aws.embedding_service import TitanEmbeddingService\n",
        "            from src.documents.chunking import HierarchicalChunker\n",
        "            import uuid\n",
        "            \n",
        "            # ì§ì ‘ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ URL ê°€ì ¸ì˜¤ê¸°\n",
        "            database_url = os.getenv(\"DATABASE_URL\") or f\"postgresql+asyncpg://{os.getenv('DATABASE_USER', 'postgres')}:{os.getenv('DATABASE_PASSWORD', 'postgres')}@{os.getenv('DATABASE_HOST', '127.0.0.1')}:{os.getenv('DATABASE_PORT', '5434')}/{os.getenv('DATABASE_NAME', 'smartclm-poc')}\"\n",
        "            async_engine = create_async_engine(database_url, echo=False)\n",
        "            \n",
        "            async with AsyncSession(async_engine) as session:\n",
        "                try:\n",
        "                    # 1. Document ë©”íƒ€ë°ì´í„° êµ¬ì„±\n",
        "                    filename = source_file.name\n",
        "                    title = parsed_result.get(\"title\", filename)\n",
        "                    page_count = parsed_result.get(\"page_count\", 0)\n",
        "                    markdown_content = parsed_result.get(\"markdown_content\", \"\")\n",
        "                    html_content = parsed_result.get(\"html_content\", \"\")\n",
        "                    \n",
        "                    # íŒŒì¼ í¬ê¸° ê³„ì‚°\n",
        "                    file_size = source_file.stat().st_size if source_file.exists() else 0\n",
        "                    \n",
        "                    # docs/ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ ê³„ì‚°\n",
        "                    relative_path = source_file.relative_to(DOCS_ROOT)\n",
        "                    \n",
        "                    # 2. Document ìƒì„± (ì‹¤ì œ íŒŒì¼ ê²½ë¡œ ì •ë³´ ì‚¬ìš©)\n",
        "                    document = Document(\n",
        "                        room_id=None,  # ì „ì—­ ë¬¸ì„œ\n",
        "                        doc_type=doc_type,\n",
        "                        category=None,\n",
        "                        processing_status=\"processing\",\n",
        "                        filename=filename,\n",
        "                        version=None,\n",
        "                        s3_bucket=\"local\",  # ë¡œì»¬ ì €ì¥ í‘œì‹œ\n",
        "                        s3_key=str(relative_path),  # docs/ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ\n",
        "                        pdf_s3_bucket=\"local\",\n",
        "                        pdf_s3_key=f\"processed/{doc_type}/{filename}\",  # ì²˜ë¦¬ëœ ê²°ê³¼ ê²½ë¡œ\n",
        "                        file_size=file_size,\n",
        "                        pdf_file_size=file_size,  # PDFì™€ ì›ë³¸ì´ ê°™ë‹¤ê³  ê°€ì •\n",
        "                        page_count=page_count,\n",
        "                        document_metadata={\n",
        "                            \"source\": \"rag_pipeline\",\n",
        "                            \"local_processing\": True,\n",
        "                            \"original_path\": str(source_file),\n",
        "                            \"processing_time\": parsed_result.get(\"processing_time\", 0),\n",
        "                            \"chunks_count\": len(parsed_result.get(\"chunks\", [])),\n",
        "                        },\n",
        "                        auto_tags=[doc_type],\n",
        "                        html_content=html_content,\n",
        "                        markdown_content=markdown_content,\n",
        "                    )\n",
        "                    \n",
        "                    # 3. Document ì €ì¥\n",
        "                    session.add(document)\n",
        "                    await session.commit()\n",
        "                    await session.refresh(document)\n",
        "                    document_id = document.id  # IDë¥¼ ë¯¸ë¦¬ ì €ì¥\n",
        "                    logger.info(f\"ğŸ“„ Document ì €ì¥ ì™„ë£Œ: ID {document_id}\")\n",
        "                    \n",
        "                    # 4. ì²­í‚¹ ë° ì„ë² ë”© ì²˜ë¦¬\n",
        "                    if markdown_content:\n",
        "                        logger.info(f\"âœ‚ï¸ ì²­í‚¹ ë° ì„ë² ë”© ì‹œì‘: {filename}\")\n",
        "                        \n",
        "                        chunker = HierarchicalChunker()\n",
        "                        embedding_service = TitanEmbeddingService()\n",
        "                        \n",
        "                        # ë§ˆí¬ë‹¤ìš´ ì²­í‚¹\n",
        "                        chunking_result = chunker.chunk_markdown(\n",
        "                            markdown_content=markdown_content,\n",
        "                            filename=filename,\n",
        "                        )\n",
        "                        \n",
        "                        # ë²¡í„°ìš© ì²­í¬ ìƒì„±\n",
        "                        vector_ready_chunks = chunker.create_vector_ready_chunks(chunking_result)\n",
        "                        \n",
        "                        # ì„ë² ë”© ìƒì„±\n",
        "                        embedded_chunks = await embedding_service.embed_chunked_documents(vector_ready_chunks)\n",
        "                        \n",
        "                        # ì²­í¬ ì €ì¥\n",
        "                        chunk_objects = []\n",
        "                        for i, chunk_data in enumerate(embedded_chunks):\n",
        "                            if chunk_data.get(\"content\", \"\").strip():\n",
        "                                chunk = Chunk(\n",
        "                                    document_id=document.id or 0,\n",
        "                                    content=chunk_data.get(\"content\", \"\"),\n",
        "                                    chunk_index=i,\n",
        "                                    header_1=chunk_data.get(\"headers\", {}).get(\"header_1\"),\n",
        "                                    header_2=chunk_data.get(\"headers\", {}).get(\"header_2\"),\n",
        "                                    header_3=chunk_data.get(\"headers\", {}).get(\"header_3\"),\n",
        "                                    header_4=chunk_data.get(\"headers\", {}).get(\"header_4\"),\n",
        "                                    parent_id=chunk_data.get(\"parent_id\"),\n",
        "                                    child_id=chunk_data.get(\"child_id\"),\n",
        "                                    chunk_type=chunk_data.get(\"chunk_type\", \"child\"),\n",
        "                                    embedding=chunk_data.get(\"embedding\"),\n",
        "                                    word_count=len(chunk_data.get(\"content\", \"\").split()),\n",
        "                                    char_count=len(chunk_data.get(\"content\", \"\")),\n",
        "                                    chunk_metadata=chunk_data.get(\"metadata\", {}),\n",
        "                                    auto_tags=chunk_data.get(\"auto_tags\", []),\n",
        "                                )\n",
        "                                chunk_objects.append(chunk)\n",
        "                        \n",
        "                        # ë°°ì¹˜ë¡œ ì²­í¬ ì €ì¥\n",
        "                        for chunk in chunk_objects:\n",
        "                            session.add(chunk)\n",
        "                        \n",
        "                        await session.commit()\n",
        "                        logger.info(f\"âœ… ì²­í¬ ì €ì¥ ì™„ë£Œ: {len(chunk_objects)}ê°œ\")\n",
        "                    \n",
        "                    # 5. ì²˜ë¦¬ ì™„ë£Œ ìƒíƒœë¡œ ë³€ê²½\n",
        "                    document.processing_status = \"completed\"\n",
        "                    session.add(document)\n",
        "                    await session.commit()\n",
        "                    \n",
        "                    logger.info(f\"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ: {filename} (Document ID: {document_id})\")\n",
        "                    return True\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    await session.rollback()\n",
        "                    raise e\n",
        "                    \n",
        "        except Exception as e:\n",
        "            logger.error(f\"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {str(e)}\")\n",
        "            import traceback\n",
        "            logger.error(f\"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}\")\n",
        "            return False\n",
        "    \n",
        "    # ë©”ì„œë“œë¥¼ í´ë˜ìŠ¤ì— ì¶”ê°€\n",
        "    client_class.save_to_database = save_to_database\n",
        "    return client_class\n",
        "\n",
        "# ExternalServiceClientì— ë©”ì„œë“œ ì¶”ê°€\n",
        "ExternalServiceClient = add_save_method(ExternalServiceClient)\n",
        "print(\"âœ… ExternalServiceClient save_to_database ë©”ì„œë“œ ì¶”ê°€ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAGPipeline í´ë˜ìŠ¤ ì •ì˜\n",
        "\n",
        "class RAGPipeline:\n",
        "    \"\"\"RAG íŒŒì´í”„ë¼ì¸ ë©”ì¸ í´ë˜ìŠ¤\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.client = ExternalServiceClient()\n",
        "        self.results: List[ProcessingResult] = []\n",
        "    \n",
        "    def find_all_documents(self) -> List[Path]:\n",
        "        \"\"\"ê·¼ê±° ìë£Œ í´ë”ì—ì„œë§Œ ë¬¸ì„œ íŒŒì¼ ì°¾ê¸°\"\"\"\n",
        "        supported_extensions = {'.pdf', '.docx', '.doc', '.hwp', '.xlsx', '.xls', '.pptx', '.ppt'}\n",
        "        document_files = []\n",
        "        \n",
        "        # ê·¼ê±° ìë£Œ í´ë”ë§Œ ê²€ìƒ‰\n",
        "        base_folder = DOCS_ROOT / \"ê·¼ê±° ìë£Œ\"\n",
        "        if base_folder.exists():\n",
        "            for ext in supported_extensions:\n",
        "                # ëª¨ë“  í•˜ìœ„ í´ë” ê²€ìƒ‰ (ë²•ë ¹, ì²´ê²°ê³„ì•½, í‘œì¤€ê³„ì•½ì„œ)\n",
        "                document_files.extend(base_folder.glob(f\"**/*{ext}\"))\n",
        "        \n",
        "        return sorted(set(document_files))  # ì¤‘ë³µ ì œê±° ë° ì •ë ¬\n",
        "\n",
        "    async def step1_store_reference_materials(self, force: bool = False):\n",
        "        \"\"\"Step 1: ê·¼ê±°ìë£Œ ì €ì¥\"\"\"\n",
        "        logger.info(\"ğŸš€ Step 1: ê·¼ê±°ìë£Œ ì €ì¥ ì‹œì‘\")\n",
        "        \n",
        "        # ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬\n",
        "        health_status = await self.client.health_check_all()\n",
        "        if not all(health_status.values()):\n",
        "            unhealthy_services = [k for k, v in health_status.items() if not v]\n",
        "            logger.error(f\"âŒ ë‹¤ìŒ ì„œë¹„ìŠ¤ë“¤ì´ ë¹„ì •ìƒì…ë‹ˆë‹¤: {unhealthy_services}\")\n",
        "            return False\n",
        "        \n",
        "        # ë¬¸ì„œ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°\n",
        "        document_files = self.find_all_documents()\n",
        "        if not document_files:\n",
        "            logger.warning(\"âš ï¸ docs í´ë”ì— ì§€ì›ë˜ëŠ” ë¬¸ì„œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "            return False\n",
        "        \n",
        "        logger.info(f\"ğŸ“ ë°œê²¬ëœ ë¬¸ì„œ íŒŒì¼: {len(document_files)}ê°œ\")\n",
        "        \n",
        "        # ë¬¸ì„œ íƒ€ì…ë³„ ë¶„ë¥˜ ì¶œë ¥\n",
        "        type_counts = {}\n",
        "        for file_path in document_files:\n",
        "            doc_type = self.client.determine_doc_type(file_path)\n",
        "            type_counts[doc_type] = type_counts.get(doc_type, 0) + 1\n",
        "        \n",
        "        logger.info(\"ğŸ“Š ë¬¸ì„œ íƒ€ì…ë³„ ë¶„ë¥˜:\")\n",
        "        for doc_type, count in type_counts.items():\n",
        "            logger.info(f\"  - {doc_type}: {count}ê°œ\")\n",
        "        \n",
        "        # ê° ë¬¸ì„œ íŒŒì¼ ì²˜ë¦¬\n",
        "        for i, file_path in enumerate(document_files, 1):\n",
        "            doc_type = self.client.determine_doc_type(file_path)\n",
        "            logger.info(f\"ğŸ“„ ì²˜ë¦¬ ì¤‘ ({i}/{len(document_files)}): {file_path.name} ({doc_type})\")\n",
        "            \n",
        "            # 1. JSON íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸ (ë¬¸ì„œ ë¶„ì„ ê±´ë„ˆë›°ê¸°ìš©)\n",
        "            doc_type_folder = PROCESSED_DIR / doc_type\n",
        "            result_file = doc_type_folder / f\"{file_path.stem}_parsed.json\"\n",
        "            old_format_file = PROCESSED_DIR / f\"{file_path.stem}_{doc_type}_parsed.json\"\n",
        "            \n",
        "            if not force and (result_file.exists() or old_format_file.exists()):\n",
        "                # ê¸°ì¡´ JSON íŒŒì¼ ë¡œë“œ\n",
        "                json_file = result_file if result_file.exists() else old_format_file\n",
        "                logger.info(f\"ğŸ“„ ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ì‚¬ìš©: {json_file.name}\")\n",
        "                \n",
        "                with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                    doc_parser_result = json.load(f)\n",
        "                \n",
        "                result = ProcessingResult(\n",
        "                    filename=file_path.name,\n",
        "                    doc_type=doc_type,\n",
        "                    folder_path=str(file_path.parent),\n",
        "                    success=True,\n",
        "                    processing_time=0,\n",
        "                    page_count=doc_parser_result.get(\"page_count\"),\n",
        "                    chunk_count=len(doc_parser_result.get(\"chunks\", [])),\n",
        "                    doc_parser_result=doc_parser_result\n",
        "                )\n",
        "            else:\n",
        "                # ìƒˆë¡œ ë¬¸ì„œ ë¶„ì„\n",
        "                logger.info(f\"ğŸ” ë¬¸ì„œ ë¶„ì„ ì‹œì‘: {file_path.name}\")\n",
        "                result = await self.client.analyze_document_file(file_path)\n",
        "            \n",
        "            self.results.append(result)\n",
        "            \n",
        "            if result.success:\n",
        "                # 2. íƒ€ì…ë³„ í´ë” ìƒì„±\n",
        "                import shutil\n",
        "                doc_type_folder = PROCESSED_DIR / doc_type\n",
        "                doc_type_folder.mkdir(exist_ok=True)\n",
        "                \n",
        "                # 3. ë¶„ì„ ê²°ê³¼ ì €ì¥ (íƒ€ì…ë³„ í´ë”ì— JSON ì €ì¥)\n",
        "                result_file = doc_type_folder / f\"{file_path.stem}_parsed.json\"\n",
        "                with open(result_file, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(result.doc_parser_result, f, ensure_ascii=False, indent=2)\n",
        "                logger.info(f\"ğŸ“„ JSON ì €ì¥ ì™„ë£Œ: {result_file}\")\n",
        "                \n",
        "                # 4. ì›ë³¸ íŒŒì¼ì„ íƒ€ì…ë³„ í´ë”ë¡œ ë³µì‚¬\n",
        "                copied_file = doc_type_folder / file_path.name\n",
        "                shutil.copy2(file_path, copied_file)\n",
        "                logger.info(f\"ğŸ“ íŒŒì¼ ë³µì‚¬ ì™„ë£Œ: {copied_file}\")\n",
        "                \n",
        "                # 5. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ (ì¤‘ë³µ ì²´í¬)\n",
        "                db_already_exists = False\n",
        "                if not force:\n",
        "                    try:\n",
        "                        from sqlmodel.ext.asyncio.session import AsyncSession\n",
        "                        from sqlalchemy.ext.asyncio import create_async_engine\n",
        "                        from src.models import Document\n",
        "                        from sqlmodel import select\n",
        "                        \n",
        "                        database_url = os.getenv(\"DATABASE_URL\") or f\"postgresql+asyncpg://{os.getenv('DATABASE_USER', 'postgres')}:{os.getenv('DATABASE_PASSWORD', 'postgres')}@{os.getenv('DATABASE_HOST', 'localhost')}:{os.getenv('DATABASE_PORT', '5434')}/{os.getenv('DATABASE_NAME', 'smartclm-poc')}\"\n",
        "                        async_engine = create_async_engine(database_url, echo=False)\n",
        "                        \n",
        "                        async with AsyncSession(async_engine) as session:\n",
        "                            query = select(Document).where(\n",
        "                                Document.filename == file_path.name,\n",
        "                                Document.doc_type == doc_type,\n",
        "                                Document.processing_status == \"completed\"\n",
        "                            )\n",
        "                            result_doc = await session.exec(query)\n",
        "                            existing_doc = result_doc.first()\n",
        "                            \n",
        "                            if existing_doc:\n",
        "                                logger.info(f\"ğŸ’¾ DB ì €ì¥ ê±´ë„ˆë›°ê¸°: {file_path.name} (ì´ë¯¸ ì €ì¥ë¨: ID {existing_doc.id})\")\n",
        "                                db_already_exists = True\n",
        "                    except Exception as e:\n",
        "                        logger.warning(f\"âš ï¸ DB ì¤‘ë³µ í™•ì¸ ì‹¤íŒ¨: {file_path.name} - {str(e)}\")\n",
        "                        logger.info(f\"ğŸ’¾ ì¤‘ë³µ í™•ì¸ ì‹¤íŒ¨ë¡œ ì¸í•´ ì €ì¥ì„ ì‹œë„í•©ë‹ˆë‹¤: {file_path.name}\")\n",
        "                \n",
        "                if not db_already_exists:\n",
        "                    db_success = await self.client.save_to_database(\n",
        "                        result.doc_parser_result, \n",
        "                        file_path,\n",
        "                        doc_type\n",
        "                    )\n",
        "                    \n",
        "                    if db_success:\n",
        "                        logger.info(f\"âœ… ì™„ë£Œ: {file_path.name} ({result.processing_time:.2f}ì´ˆ)\")\n",
        "                    else:\n",
        "                        logger.error(f\"âŒ DB ì €ì¥ ì‹¤íŒ¨: {file_path.name}\")\n",
        "                else:\n",
        "                    logger.info(f\"âœ… ì™„ë£Œ: {file_path.name} ({result.processing_time:.2f}ì´ˆ) - DBëŠ” ê¸°ì¡´ ì‚¬ìš©\")\n",
        "            else:\n",
        "                logger.error(f\"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {file_path.name} - {result.error_message}\")\n",
        "            \n",
        "            # ì²˜ë¦¬ ê°„ ì ì‹œ ëŒ€ê¸° (ì‹œìŠ¤í…œ ë¶€í•˜ ë°©ì§€)\n",
        "            await asyncio.sleep(1)\n",
        "        \n",
        "        # ê²°ê³¼ ìš”ì•½ ì €ì¥\n",
        "        await self._save_processing_summary()\n",
        "        \n",
        "        return True\n",
        "\n",
        "    async def step2_test_search(self):\n",
        "        \"\"\"Step 2: ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\"\"\"\n",
        "        logger.info(\"ğŸ” Step 2: ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹œì‘\")\n",
        "        \n",
        "        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì •ì˜\n",
        "        test_queries = [\n",
        "            # ë²•ë ¹ ê´€ë ¨ ì¿¼ë¦¬\n",
        "            {\n",
        "                \"query\": \"ê°œì¸ì •ë³´ë³´í˜¸ë²•ì˜ ì£¼ìš” ë‚´ìš©ì€?\",\n",
        "                \"category\": \"law\",\n",
        "                \"expected_doc_type\": \"law\"\n",
        "            },\n",
        "            {\n",
        "                \"query\": \"ì „ììƒê±°ë˜ ì†Œë¹„ìë³´í˜¸ì— ê´€í•œ ë²•ë¥ \",\n",
        "                \"category\": \"law\", \n",
        "                \"expected_doc_type\": \"law\"\n",
        "            },\n",
        "            # ê³„ì•½ì„œ ê´€ë ¨ ì¿¼ë¦¬\n",
        "            {\n",
        "                \"query\": \"ê³„ì•½í•´ì§€ ì¡°ê±´ê³¼ ì ˆì°¨\",\n",
        "                \"category\": \"contract\",\n",
        "                \"expected_doc_type\": [\"executed_contract\", \"standard_contract\"]\n",
        "            },\n",
        "            {\n",
        "                \"query\": \"ì†í•´ë°°ìƒ ì±…ì„ ë²”ìœ„\",\n",
        "                \"category\": \"contract\",\n",
        "                \"expected_doc_type\": [\"executed_contract\", \"standard_contract\"]\n",
        "            },\n",
        "            {\n",
        "                \"query\": \"ê³„ì•½ ê¸°ê°„ ë° ê°±ì‹  ì¡°ê±´\",\n",
        "                \"category\": \"contract\", \n",
        "                \"expected_doc_type\": [\"executed_contract\", \"standard_contract\"]\n",
        "            },\n",
        "            # ì¼ë°˜ì ì¸ ì§ˆë¬¸\n",
        "            {\n",
        "                \"query\": \"ê³„ì•½ì„œ ì‘ì„± ì‹œ ì£¼ì˜ì‚¬í•­\",\n",
        "                \"category\": \"general\",\n",
        "                \"expected_doc_type\": \"any\"\n",
        "            }\n",
        "        ]\n",
        "        \n",
        "        search_results = []\n",
        "        \n",
        "        # ê° ì¿¼ë¦¬ì— ëŒ€í•´ ê²€ìƒ‰ ìˆ˜í–‰\n",
        "        for i, test_case in enumerate(test_queries, 1):\n",
        "            logger.info(f\"ğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ {i}/{len(test_queries)}: {test_case['query']}\")\n",
        "            \n",
        "            try:\n",
        "                # ì§ì ‘ ê²€ìƒ‰ í˜¸ì¶œ\n",
        "                start_time = time.time()\n",
        "                result = await self.client.search_documents_direct(\n",
        "                    query=test_case[\"query\"],\n",
        "                    top_k=5,\n",
        "                    doc_types=None  # ëª¨ë“  ë¬¸ì„œ íƒ€ì…ì—ì„œ ê²€ìƒ‰\n",
        "                )\n",
        "                search_time = time.time() - start_time\n",
        "                \n",
        "                # ê²°ê³¼ ë¶„ì„\n",
        "                search_result = {\n",
        "                    \"query\": test_case[\"query\"],\n",
        "                    \"category\": test_case[\"category\"],\n",
        "                    \"expected_doc_type\": test_case[\"expected_doc_type\"],\n",
        "                    \"search_time\": search_time,\n",
        "                    \"total_results\": len(result.get(\"results\", [])),\n",
        "                    \"results\": result.get(\"results\", []),\n",
        "                    \"success\": True\n",
        "                }\n",
        "                \n",
        "                # ê²°ê³¼ í’ˆì§ˆ í‰ê°€\n",
        "                if result.get(\"results\"):\n",
        "                    # ë¬¸ì„œ íƒ€ì… ë¶„í¬ í™•ì¸\n",
        "                    doc_types = [r.get(\"doc_type\") for r in result[\"results\"]]\n",
        "                    search_result[\"doc_type_distribution\"] = {\n",
        "                        doc_type: doc_types.count(doc_type) \n",
        "                        for doc_type in set(doc_types) if doc_type\n",
        "                    }\n",
        "                    \n",
        "                    # í‰ê·  ì ìˆ˜ ê³„ì‚°\n",
        "                    scores = [r.get(\"similarity_score\", 0) for r in result[\"results\"]]\n",
        "                    search_result[\"avg_similarity_score\"] = sum(scores) / len(scores) if scores else 0\n",
        "                    search_result[\"max_similarity_score\"] = max(scores) if scores else 0\n",
        "                    search_result[\"min_similarity_score\"] = min(scores) if scores else 0\n",
        "                \n",
        "                logger.info(f\"  âœ… ê²€ìƒ‰ ì™„ë£Œ: {search_result['total_results']}ê°œ ê²°ê³¼, {search_time:.2f}ì´ˆ\")\n",
        "                if search_result.get(\"doc_type_distribution\"):\n",
        "                    for doc_type, count in search_result[\"doc_type_distribution\"].items():\n",
        "                        logger.info(f\"    - {doc_type}: {count}ê°œ\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                logger.error(f\"  âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}\")\n",
        "                search_result = {\n",
        "                    \"query\": test_case[\"query\"],\n",
        "                    \"category\": test_case[\"category\"],\n",
        "                    \"expected_doc_type\": test_case[\"expected_doc_type\"],\n",
        "                    \"error\": str(e),\n",
        "                    \"success\": False\n",
        "                }\n",
        "            \n",
        "            search_results.append(search_result)\n",
        "            \n",
        "            # ìš”ì²­ ê°„ ì ì‹œ ëŒ€ê¸°\n",
        "            await asyncio.sleep(0.5)\n",
        "        \n",
        "        # ê²°ê³¼ ìš”ì•½ ë° ì €ì¥\n",
        "        await self._save_search_test_results(search_results)\n",
        "        \n",
        "        logger.info(\"ğŸ‰ Step 2 ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
        "        return True\n",
        "\n",
        "print(\"âœ… RAGPipeline í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAGPipeline ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤ ì¶”ê°€\n",
        "\n",
        "def add_rag_utility_methods(rag_class):\n",
        "    \"\"\"RAGPipelineì— ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤ ì¶”ê°€\"\"\"\n",
        "    \n",
        "    async def _save_processing_summary(self):\n",
        "        \"\"\"ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì €ì¥\"\"\"\n",
        "        # íƒ€ì…ë³„ í†µê³„\n",
        "        type_stats = {}\n",
        "        for result in self.results:\n",
        "            doc_type = result.doc_type\n",
        "            if doc_type not in type_stats:\n",
        "                type_stats[doc_type] = {\"total\": 0, \"success\": 0, \"failed\": 0}\n",
        "            \n",
        "            type_stats[doc_type][\"total\"] += 1\n",
        "            if result.success:\n",
        "                type_stats[doc_type][\"success\"] += 1\n",
        "            else:\n",
        "                type_stats[doc_type][\"failed\"] += 1\n",
        "        \n",
        "        summary = {\n",
        "            \"total_files\": len(self.results),\n",
        "            \"successful\": len([r for r in self.results if r.success]),\n",
        "            \"failed\": len([r for r in self.results if not r.success]),\n",
        "            \"total_processing_time\": sum(r.processing_time for r in self.results),\n",
        "            \"type_statistics\": type_stats,\n",
        "            \"details\": [\n",
        "                {\n",
        "                    \"filename\": r.filename,\n",
        "                    \"doc_type\": r.doc_type,\n",
        "                    \"folder_path\": r.folder_path,\n",
        "                    \"success\": r.success,\n",
        "                    \"processing_time\": r.processing_time,\n",
        "                    \"page_count\": r.page_count,\n",
        "                    \"chunk_count\": r.chunk_count,\n",
        "                    \"error\": r.error_message\n",
        "                }\n",
        "                for r in self.results\n",
        "            ]\n",
        "        }\n",
        "        \n",
        "        summary_file = RESULTS_DIR / \"processing_summary.json\"\n",
        "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(summary, f, ensure_ascii=False, indent=2)\n",
        "        \n",
        "        logger.info(f\"ğŸ“Š ì²˜ë¦¬ ìš”ì•½:\")\n",
        "        logger.info(f\"  - ì „ì²´: {summary['total_files']}ê°œ\")\n",
        "        logger.info(f\"  - ì„±ê³µ: {summary['successful']}ê°œ\")\n",
        "        logger.info(f\"  - ì‹¤íŒ¨: {summary['failed']}ê°œ\")\n",
        "        logger.info(f\"  - ì´ ì²˜ë¦¬ ì‹œê°„: {summary['total_processing_time']:.2f}ì´ˆ\")\n",
        "        \n",
        "        logger.info(f\"ğŸ“‹ íƒ€ì…ë³„ í†µê³„:\")\n",
        "        for doc_type, stats in type_stats.items():\n",
        "            logger.info(f\"  - {doc_type}: {stats['success']}/{stats['total']} ì„±ê³µ\")\n",
        "        \n",
        "        logger.info(f\"ğŸ’¾ ìš”ì•½ ì €ì¥: {summary_file}\")\n",
        "\n",
        "    async def _save_search_test_results(self, search_results: List[Dict]):\n",
        "        \"\"\"ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥\"\"\"\n",
        "        # ê²°ê³¼ ìš”ì•½ ê³„ì‚°\n",
        "        total_queries = len(search_results)\n",
        "        successful_queries = len([r for r in search_results if r.get(\"success\", False)])\n",
        "        failed_queries = total_queries - successful_queries\n",
        "        \n",
        "        # ê²€ìƒ‰ ì‹œê°„ í†µê³„\n",
        "        search_times = [r.get(\"search_time\", 0) for r in search_results if r.get(\"success\")]\n",
        "        avg_search_time = sum(search_times) / len(search_times) if search_times else 0\n",
        "        \n",
        "        # ê²°ê³¼ ìˆ˜ í†µê³„\n",
        "        result_counts = [r.get(\"total_results\", 0) for r in search_results if r.get(\"success\")]\n",
        "        avg_results = sum(result_counts) / len(result_counts) if result_counts else 0\n",
        "        \n",
        "        # ì¹´í…Œê³ ë¦¬ë³„ ì„±ê³µë¥ \n",
        "        category_stats = {}\n",
        "        for result in search_results:\n",
        "            category = result.get(\"category\", \"unknown\")\n",
        "            if category not in category_stats:\n",
        "                category_stats[category] = {\"total\": 0, \"success\": 0}\n",
        "            category_stats[category][\"total\"] += 1\n",
        "            if result.get(\"success\", False):\n",
        "                category_stats[category][\"success\"] += 1\n",
        "        \n",
        "        summary = {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"test_summary\": {\n",
        "                \"total_queries\": total_queries,\n",
        "                \"successful_queries\": successful_queries,\n",
        "                \"failed_queries\": failed_queries,\n",
        "                \"success_rate\": successful_queries / total_queries if total_queries > 0 else 0,\n",
        "                \"avg_search_time_seconds\": round(avg_search_time, 3),\n",
        "                \"avg_results_per_query\": round(avg_results, 1)\n",
        "            },\n",
        "            \"category_performance\": {\n",
        "                category: {\n",
        "                    \"success_rate\": stats[\"success\"] / stats[\"total\"] if stats[\"total\"] > 0 else 0,\n",
        "                    \"success_count\": stats[\"success\"],\n",
        "                    \"total_count\": stats[\"total\"]\n",
        "                }\n",
        "                for category, stats in category_stats.items()\n",
        "            },\n",
        "            \"detailed_results\": search_results\n",
        "        }\n",
        "        \n",
        "        # ê²°ê³¼ ì €ì¥\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        results_file = RESULTS_DIR / f\"search_test_results_{timestamp}.json\"\n",
        "        \n",
        "        with open(results_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(summary, f, ensure_ascii=False, indent=2)\n",
        "        \n",
        "        # ë¡œê·¸ ì¶œë ¥\n",
        "        logger.info(\"=\" * 50)\n",
        "        logger.info(\"ğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½\")\n",
        "        logger.info(\"=\" * 50)\n",
        "        logger.info(f\"ğŸ“Š ì „ì²´ ì¿¼ë¦¬: {total_queries}ê°œ\")\n",
        "        logger.info(f\"âœ… ì„±ê³µ: {successful_queries}ê°œ ({successful_queries/total_queries*100:.1f}%)\")\n",
        "        logger.info(f\"âŒ ì‹¤íŒ¨: {failed_queries}ê°œ\")\n",
        "        logger.info(f\"â±ï¸ í‰ê·  ê²€ìƒ‰ ì‹œê°„: {avg_search_time:.3f}ì´ˆ\")\n",
        "        logger.info(f\"ğŸ“„ í‰ê·  ê²°ê³¼ ìˆ˜: {avg_results:.1f}ê°œ\")\n",
        "        \n",
        "        logger.info(\"\\nğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ì„±ê³µë¥ :\")\n",
        "        for category, stats in category_stats.items():\n",
        "            success_rate = stats[\"success\"] / stats[\"total\"] * 100\n",
        "            logger.info(f\"  - {category}: {stats['success']}/{stats['total']} ({success_rate:.1f}%)\")\n",
        "        \n",
        "        logger.info(f\"\\nğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥: {results_file}\")\n",
        "\n",
        "    # ë©”ì„œë“œë“¤ì„ í´ë˜ìŠ¤ì— ì¶”ê°€\n",
        "    rag_class._save_processing_summary = _save_processing_summary\n",
        "    rag_class._save_search_test_results = _save_search_test_results\n",
        "    return rag_class\n",
        "\n",
        "# RAGPipelineì— ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ ì¶”ê°€\n",
        "RAGPipeline = add_rag_utility_methods(RAGPipeline)\n",
        "print(\"âœ… RAGPipeline ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ ì¶”ê°€ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: ê³„ì•½ì„œ ê²€í†  í…ŒìŠ¤íŠ¸ (3ë‹¨ê³„ ì²´ì¸ ë°©ì‹) ë©”ì„œë“œ ì¶”ê°€\n",
        "\n",
        "def add_step3_methods(rag_class):\n",
        "    \"\"\"RAGPipelineì— Step 3 ê´€ë ¨ ë©”ì„œë“œë“¤ ì¶”ê°€\"\"\"\n",
        "    \n",
        "    async def step3_contract_review_test(self):\n",
        "        \"\"\"Step 3: ê³„ì•½ì„œ ê²€í†  ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ (3ë‹¨ê³„ ì²´ì¸ ë°©ì‹)\"\"\"\n",
        "        logger.info(\"ğŸ“ Step 3: ê³„ì•½ì„œ ê²€í†  ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘ (ì²´ì¸ ë°©ì‹)\")\n",
        "        \n",
        "        # 1. í…ŒìŠ¤íŠ¸ ëŒ€ìƒ ë¬¸ì„œ ì„ íƒ\n",
        "        test_document_path = DOCS_ROOT / \"1. NDA\" / \"ì´ˆì•ˆ_ë¹„ë°€ìœ ì§€ê³„ì•½ì„œ.docx\"\n",
        "        \n",
        "        if not test_document_path.exists():\n",
        "            logger.error(f\"âŒ í…ŒìŠ¤íŠ¸ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤: {test_document_path}\")\n",
        "            return False\n",
        "        \n",
        "        logger.info(f\"ğŸ“„ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ: {test_document_path.name}\")\n",
        "        \n",
        "        # 2. ê³„ì•½ì„œë¥¼ DBì— ì €ì¥ (ì¤‘ë³µ ì²´í¬ í¬í•¨)\n",
        "        logger.info(\"ğŸ’¾ ê³„ì•½ì„œ DB ì €ì¥ í™•ì¸/ì²˜ë¦¬ ì¤‘...\")\n",
        "        document_id = await self._ensure_contract_in_db(test_document_path)\n",
        "        \n",
        "        if not document_id:\n",
        "            logger.error(\"âŒ ê³„ì•½ì„œ DB ì €ì¥ ì‹¤íŒ¨\")\n",
        "            return False\n",
        "        \n",
        "        logger.info(f\"âœ… ê³„ì•½ì„œ DB ì €ì¥ ì™„ë£Œ (Document ID: {document_id})\")\n",
        "        \n",
        "        # 3. DBì—ì„œ ê³„ì•½ì„œ ì •ë³´ ì¡°íšŒ\n",
        "        document_info = await self._get_document_from_db(document_id)\n",
        "        if not document_info:\n",
        "            logger.error(\"âŒ DBì—ì„œ ê³„ì•½ì„œ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨\")\n",
        "            return False\n",
        "        \n",
        "        # 4. ê³„ì•½ì„œ ì¡°í•­ ì¡°íšŒ\n",
        "        chunks = await self._get_document_chunks_from_db(document_id)\n",
        "        parent_chunks = [c for c in chunks if c.get(\"chunk_type\") == \"parent\"]\n",
        "        \n",
        "        logger.info(f\"ğŸ” DBì—ì„œ ì¡°íšŒí•œ ì²­í¬ ìˆ˜: {len(chunks)}\")\n",
        "        logger.info(f\"ğŸ” Parent ì²­í¬ ìˆ˜: {len(parent_chunks)}\")\n",
        "        \n",
        "        if not parent_chunks:\n",
        "            logger.error(\"âŒ ë¶„ì„í•  ì¡°í•­ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "            return False\n",
        "        \n",
        "        logger.info(f\"ğŸ“Š ì´ {len(parent_chunks)}ê°œ ì¡°í•­ ë°œê²¬\")\n",
        "        \n",
        "        try:\n",
        "            # === 3ë‹¨ê³„ ì²´ì¸ ë¶„ì„ ì‹œì‘ ===\n",
        "            \n",
        "            # Chain 1: ì „ì²´ ê³„ì•½ì„œ ìŠ¤ìº” â†’ ìœ„ë²• ì¡°í•­ ì‹ë³„\n",
        "            logger.info(\"ğŸ” Chain 1: ì „ì²´ ê³„ì•½ì„œ ìœ„ë²• ì¡°í•­ ì‹ë³„ ì¤‘...\")\n",
        "            start_time = time.time()\n",
        "            \n",
        "            violation_candidates = await self._chain1_identify_violations(\n",
        "                document_name=test_document_path.name,\n",
        "                all_chunks=parent_chunks,\n",
        "                document_id=document_id\n",
        "            )\n",
        "            \n",
        "            chain1_time = time.time() - start_time\n",
        "            logger.info(f\"âœ… Chain 1 ì™„ë£Œ: {len(violation_candidates)}ê°œ ìœ„ë²• ì¡°í•­ í›„ë³´ ë°œê²¬ ({chain1_time:.2f}ì´ˆ)\")\n",
        "            \n",
        "            if not violation_candidates:\n",
        "                logger.info(\"âœ… ìœ„ë²• ì¡°í•­ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "                await self._save_chain_analysis_results(test_document_path.name, {\"violations\": []}, {\n",
        "                    \"chain1_time\": chain1_time,\n",
        "                    \"chain2_time\": 0,\n",
        "                    \"chain3_time\": 0,\n",
        "                    \"total_time\": chain1_time,\n",
        "                    \"violations_found\": 0,\n",
        "                    \"total_clauses\": len(parent_chunks)\n",
        "                })\n",
        "                return True\n",
        "            \n",
        "            # Chain 2: ê° ìœ„ë²• ì¡°í•­ë³„ ê´€ë ¨ ë²•ë ¹ ê²€ìƒ‰ (ë³‘ë ¬ ì²˜ë¦¬)\n",
        "            logger.info(\"ğŸ” Chain 2: ìœ„ë²• ì¡°í•­ë³„ ê´€ë ¨ ë²•ë ¹ ê²€ìƒ‰ ì¤‘...\")\n",
        "            start_time = time.time()\n",
        "            \n",
        "            violations_with_laws = await self._chain2_search_related_laws(violation_candidates)\n",
        "            \n",
        "            chain2_time = time.time() - start_time\n",
        "            logger.info(f\"âœ… Chain 2 ì™„ë£Œ: {len(violations_with_laws)}ê°œ ì¡°í•­ì˜ ë²•ë ¹ ê²€ìƒ‰ ì™„ë£Œ ({chain2_time:.2f}ì´ˆ)\")\n",
        "            \n",
        "            # Chain 3: ìµœì¢… ìƒì„¸ ë¶„ì„\n",
        "            logger.info(\"ğŸ” Chain 3: ìµœì¢… ìƒì„¸ ë¶„ì„ ì¤‘...\")\n",
        "            start_time = time.time()\n",
        "            \n",
        "            final_analysis = await self._chain3_detailed_analysis(\n",
        "                violations_with_laws=violations_with_laws,\n",
        "                document_name=test_document_path.name\n",
        "            )\n",
        "            \n",
        "            chain3_time = time.time() - start_time\n",
        "            logger.info(f\"âœ… Chain 3 ì™„ë£Œ: ìµœì¢… ë¶„ì„ ì™„ë£Œ ({chain3_time:.2f}ì´ˆ)\")\n",
        "            \n",
        "            # ê²°ê³¼ ì €ì¥\n",
        "            await self._save_chain_analysis_results(test_document_path.name, final_analysis, {\n",
        "                \"chain1_time\": chain1_time,\n",
        "                \"chain2_time\": chain2_time, \n",
        "                \"chain3_time\": chain3_time,\n",
        "                \"total_time\": chain1_time + chain2_time + chain3_time,\n",
        "                \"violations_found\": len(final_analysis.get(\"violations\", [])),\n",
        "                \"total_clauses\": len(parent_chunks)\n",
        "            })\n",
        "            \n",
        "            logger.info(\"ğŸ‰ Step 3 ê³„ì•½ì„œ ê²€í†  í…ŒìŠ¤íŠ¸ ì™„ë£Œ (ì²´ì¸ ë°©ì‹)!\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"âŒ ì²´ì¸ ë¶„ì„ ì‹¤íŒ¨: {str(e)}\")\n",
        "            return False\n",
        "    \n",
        "    # ë©”ì„œë“œë¥¼ í´ë˜ìŠ¤ì— ì¶”ê°€\n",
        "    rag_class.step3_contract_review_test = step3_contract_review_test\n",
        "    return rag_class\n",
        "\n",
        "# RAGPipelineì— Step 3 ë©”ì„œë“œ ì¶”ê°€\n",
        "RAGPipeline = add_step3_methods(RAGPipeline)\n",
        "print(\"âœ… RAGPipeline Step 3 ë©”ì„œë“œ ì¶”ê°€ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3 ì²´ì¸ ë¶„ì„ í—¬í¼ ë©”ì„œë“œë“¤ (Part 1: DB ê´€ë ¨)\n",
        "\n",
        "def add_chain_helper_methods_part1(rag_class):\n",
        "    \"\"\"RAGPipelineì— ì²´ì¸ ë¶„ì„ í—¬í¼ ë©”ì„œë“œë“¤ ì¶”ê°€ (Part 1: DB ê´€ë ¨)\"\"\"\n",
        "    \n",
        "    async def _ensure_contract_in_db(self, document_path: Path) -> Optional[int]:\n",
        "        \"\"\"ê³„ì•½ì„œê°€ DBì— ì—†ìœ¼ë©´ ì €ì¥í•˜ê³ , ìˆìœ¼ë©´ ê¸°ì¡´ ID ë°˜í™˜\"\"\"\n",
        "        try:\n",
        "            from sqlmodel.ext.asyncio.session import AsyncSession\n",
        "            from sqlalchemy.ext.asyncio import create_async_engine\n",
        "            from src.models import Document\n",
        "            from sqlmodel import select\n",
        "            \n",
        "            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°\n",
        "            database_url = os.getenv(\"DATABASE_URL\") or f\"postgresql+asyncpg://{os.getenv('DATABASE_USER', 'postgres')}:{os.getenv('DATABASE_PASSWORD', 'postgres')}@{os.getenv('DATABASE_HOST', 'localhost')}:{os.getenv('DATABASE_PORT', '5434')}/{os.getenv('DATABASE_NAME', 'smartclm-poc')}\"\n",
        "            async_engine = create_async_engine(database_url, echo=False)\n",
        "            \n",
        "            async with AsyncSession(async_engine) as session:\n",
        "                # 1. ê¸°ì¡´ ë¬¸ì„œ í™•ì¸\n",
        "                query = select(Document).where(\n",
        "                    Document.filename == document_path.name,\n",
        "                    Document.doc_type == \"contract\",\n",
        "                    Document.processing_status == \"completed\"\n",
        "                )\n",
        "                result = await session.exec(query)\n",
        "                existing_doc = result.first()\n",
        "                \n",
        "                if existing_doc:\n",
        "                    logger.info(f\"ğŸ—‘ï¸ ê¸°ì¡´ ê³„ì•½ì„œ ì‚­ì œ í›„ ì¬ìƒì„±: {document_path.name} (ID: {existing_doc.id})\")\n",
        "                    # ê¸°ì¡´ ì²­í¬ë“¤ ì‚­ì œ\n",
        "                    from sqlmodel import delete\n",
        "                    from src.models import Chunk\n",
        "                    chunk_delete_stmt = delete(Chunk).where(Chunk.document_id == existing_doc.id)\n",
        "                    await session.exec(chunk_delete_stmt)\n",
        "                    \n",
        "                    # ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ\n",
        "                    doc_delete_stmt = delete(Document).where(Document.id == existing_doc.id)\n",
        "                    await session.exec(doc_delete_stmt)\n",
        "                    await session.commit()\n",
        "                    logger.info(f\"âœ… ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ\")\n",
        "                \n",
        "                # 2. ìƒˆë¡œ ë¶„ì„ ë° ì €ì¥\n",
        "                logger.info(f\"ğŸ” ê³„ì•½ì„œ ë¶„ì„ ë° DB ì €ì¥: {document_path.name}\")\n",
        "                doc_data = await self.client.analyze_document_file(document_path)\n",
        "                \n",
        "                if not doc_data.success:\n",
        "                    logger.error(f\"âŒ ë¬¸ì„œ ë¶„ì„ ì‹¤íŒ¨: {doc_data.error_message}\")\n",
        "                    return None\n",
        "                \n",
        "                logger.info(f\"âœ… ë¬¸ì„œ ë¶„ì„ ì™„ë£Œ ({doc_data.processing_time:.2f}ì´ˆ)\")\n",
        "                \n",
        "                # 3. DB ì €ì¥\n",
        "                db_success = await self.client.save_to_database(\n",
        "                    doc_data.doc_parser_result,\n",
        "                    document_path,\n",
        "                    \"contract\"  # contract íƒ€ì…ìœ¼ë¡œ ì €ì¥\n",
        "                )\n",
        "                \n",
        "                if not db_success:\n",
        "                    logger.error(f\"âŒ DB ì €ì¥ ì‹¤íŒ¨: {document_path.name}\")\n",
        "                    return None\n",
        "                \n",
        "                # 4. ì €ì¥ëœ ë¬¸ì„œ ID ì¡°íšŒ\n",
        "                result2 = await session.exec(query)\n",
        "                new_doc = result2.first()\n",
        "                \n",
        "                if new_doc:\n",
        "                    logger.info(f\"âœ… ìƒˆ ê³„ì•½ì„œ ì €ì¥ ì™„ë£Œ: ID {new_doc.id}\")\n",
        "                    return new_doc.id\n",
        "                \n",
        "                return None\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"âŒ ê³„ì•½ì„œ DB ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}\")\n",
        "            return None\n",
        "    \n",
        "    async def _get_document_from_db(self, document_id: int) -> Optional[Dict]:\n",
        "        \"\"\"DBì—ì„œ ë¬¸ì„œ ì •ë³´ ì¡°íšŒ\"\"\"\n",
        "        try:\n",
        "            from sqlmodel.ext.asyncio.session import AsyncSession\n",
        "            from sqlalchemy.ext.asyncio import create_async_engine\n",
        "            from src.models import Document\n",
        "            from sqlmodel import select\n",
        "            \n",
        "            database_url = os.getenv(\"DATABASE_URL\") or f\"postgresql+asyncpg://{os.getenv('DATABASE_USER', 'postgres')}:{os.getenv('DATABASE_PASSWORD', 'postgres')}@{os.getenv('DATABASE_HOST', 'localhost')}:{os.getenv('DATABASE_PORT', '5434')}/{os.getenv('DATABASE_NAME', 'smartclm-poc')}\"\n",
        "            async_engine = create_async_engine(database_url, echo=False)\n",
        "            \n",
        "            async with AsyncSession(async_engine) as session:\n",
        "                query = select(Document).where(Document.id == document_id)\n",
        "                result = await session.exec(query)\n",
        "                document = result.first()\n",
        "                \n",
        "                if document:\n",
        "                    return {\n",
        "                        \"id\": document.id,\n",
        "                        \"filename\": document.filename,\n",
        "                        \"doc_type\": document.doc_type,\n",
        "                        \"markdown_content\": document.markdown_content,\n",
        "                        \"page_count\": document.page_count\n",
        "                    }\n",
        "                return None\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"âŒ ë¬¸ì„œ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n",
        "            return None\n",
        "    \n",
        "    async def _get_document_chunks_from_db(self, document_id: int) -> List[Dict]:\n",
        "        \"\"\"DBì—ì„œ ë¬¸ì„œì˜ ì²­í¬ë“¤ ì¡°íšŒ\"\"\"\n",
        "        try:\n",
        "            from sqlmodel.ext.asyncio.session import AsyncSession\n",
        "            from sqlalchemy.ext.asyncio import create_async_engine\n",
        "            from src.models import Chunk\n",
        "            from sqlmodel import select\n",
        "            \n",
        "            database_url = os.getenv(\"DATABASE_URL\") or f\"postgresql+asyncpg://{os.getenv('DATABASE_USER', 'postgres')}:{os.getenv('DATABASE_PASSWORD', 'postgres')}@{os.getenv('DATABASE_HOST', 'localhost')}:{os.getenv('DATABASE_PORT', '5434')}/{os.getenv('DATABASE_NAME', 'smartclm-poc')}\"\n",
        "            async_engine = create_async_engine(database_url, echo=False)\n",
        "            \n",
        "            async with AsyncSession(async_engine) as session:\n",
        "                query = select(Chunk).where(\n",
        "                    Chunk.document_id == document_id,\n",
        "                    Chunk.chunk_type == \"parent\"\n",
        "                ).order_by(Chunk.chunk_index)\n",
        "                \n",
        "                result = await session.exec(query)\n",
        "                chunks = result.all()\n",
        "                \n",
        "                # Chunk ëª¨ë¸ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜\n",
        "                chunk_list = []\n",
        "                for chunk in chunks:\n",
        "                    chunk_dict = {\n",
        "                        \"chunk_index\": chunk.chunk_index,\n",
        "                        \"chunk_type\": chunk.chunk_type,\n",
        "                        \"content\": chunk.content,\n",
        "                        \"header_1\": chunk.header_1,\n",
        "                        \"header_2\": chunk.header_2,\n",
        "                        \"header_3\": chunk.header_3,\n",
        "                        \"chunk_metadata\": chunk.chunk_metadata or {}\n",
        "                    }\n",
        "                    chunk_list.append(chunk_dict)\n",
        "                \n",
        "                logger.info(f\"ğŸ“Š DBì—ì„œ {len(chunk_list)}ê°œ parent ì²­í¬ ì¡°íšŒ\")\n",
        "                return chunk_list\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"âŒ ì²­í¬ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n",
        "            return []\n",
        "    \n",
        "    # ë©”ì„œë“œë“¤ì„ í´ë˜ìŠ¤ì— ì¶”ê°€\n",
        "    rag_class._ensure_contract_in_db = _ensure_contract_in_db\n",
        "    rag_class._get_document_from_db = _get_document_from_db\n",
        "    rag_class._get_document_chunks_from_db = _get_document_chunks_from_db\n",
        "    return rag_class\n",
        "\n",
        "# RAGPipelineì— ì²´ì¸ í—¬í¼ ë©”ì„œë“œ ì¶”ê°€ (Part 1)\n",
        "RAGPipeline = add_chain_helper_methods_part1(RAGPipeline)\n",
        "print(\"âœ… RAGPipeline ì²´ì¸ í—¬í¼ ë©”ì„œë“œ ì¶”ê°€ ì™„ë£Œ (Part 1: DB ê´€ë ¨)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3 ì²´ì¸ ë¶„ì„ í—¬í¼ ë©”ì„œë“œë“¤ (Part 2: ì²´ì¸ ë¶„ì„)\n",
        "\n",
        "def add_chain_helper_methods_part2(rag_class):\n",
        "    \"\"\"RAGPipelineì— ì²´ì¸ ë¶„ì„ ë©”ì„œë“œë“¤ ì¶”ê°€ (Part 2: ì²´ì¸ ë¶„ì„)\"\"\"\n",
        "    \n",
        "    async def _chain1_identify_violations(self, document_name: str, all_chunks: List[Dict], document_id: int) -> List[Dict]:\n",
        "        \"\"\"Chain 1: ì „ì²´ ê³„ì•½ì„œë¥¼ ìŠ¤ìº”í•˜ì—¬ ìœ„ë²• ì¡°í•­ë“¤ì„ ì‹ë³„\"\"\"\n",
        "        try:\n",
        "            # ì „ì²´ ê³„ì•½ì„œ êµ¬ì¡° ìƒì„± (ì›ë³¸ ì¡°í•­ ë²ˆí˜¸ ë³´ì¡´)\n",
        "            contract_structure = []\n",
        "            for i, chunk in enumerate(all_chunks, 1):\n",
        "                title = chunk.get(\"header_1\", f\"ì¡°í•­ {i}\")\n",
        "                content = chunk.get(\"content\", \"\")[:200] + \"...\" if len(chunk.get(\"content\", \"\")) > 200 else chunk.get(\"content\", \"\")\n",
        "                \n",
        "\n",
        "                # titleì´ ì´ë¯¸ \"ì œXì¡°\" í˜•íƒœì¸ì§€ í™•ì¸\n",
        "                if title.startswith(\"ì œ\") and \"ì¡°\" in title:\n",
        "                    # ì´ë¯¸ ì¡°í•­ ë²ˆí˜¸ê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
        "                    contract_structure.append(f\"{title}: {content}\")\n",
        "                else:\n",
        "                    # ì¡°í•­ ë²ˆí˜¸ê°€ ì—†ìœ¼ë©´ ìˆœì„œëŒ€ë¡œ ì¶”ê°€\n",
        "                    contract_structure.append(f\"ì œ{i}ì¡° {title}: {content}\")\n",
        "            \n",
        "            full_contract = \"\\n\\n\".join(contract_structure)\n",
        "            \n",
        "            # Chain 1 í”„ë¡¬í”„íŠ¸\n",
        "            chain1_prompt = f\"\"\"# ğŸ“‹ (ì£¼)ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ ê´€ì  ê³„ì•½ì„œ ìœ„í—˜ ì¡°í•­ ì‹ë³„\n",
        "\n",
        "**ê³„ì•½ì„œëª…:** {document_name}\n",
        "**ì´ ì¡°í•­ìˆ˜:** {len(all_chunks)}ê°œ\n",
        "**ê²€í†  ê´€ì :** **(ì£¼)ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ ì…ì¥ì—ì„œ ë¶ˆë¦¬í•œ ì¡°í•­ ì¤‘ì‹¬ ë¶„ì„**\n",
        "\n",
        "**ì „ì²´ ê³„ì•½ì„œ êµ¬ì¡°:**\n",
        "```\n",
        "{full_contract}\n",
        "```\n",
        "\n",
        "## ğŸ¯ ë¶„ì„ ìš”êµ¬ì‚¬í•­\n",
        "\n",
        "ì´ ê³„ì•½ì„œë¥¼ **(ì£¼)ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ ì…ì¥**ì—ì„œ ê²€í† í•˜ì—¬ **ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ì—ê²Œ ë¶ˆë¦¬í•˜ê±°ë‚˜ ìœ„í—˜í•œ ì¡°í•­ë“¤**ì„ ëª¨ë‘ ì°¾ì•„ì£¼ì„¸ìš”.\n",
        "\n",
        "### ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ ê´€ì  ìƒì„¸ ê²€í†  ê¸°ì¤€:\n",
        "\n",
        "**ğŸ” ë¹„ë°€ì •ë³´ ê´€ë ¨ ìœ„í—˜:**\n",
        "1. **ì •ë³´ ë³´í˜¸ ë²”ìœ„ì˜ ê³¼ë„í•œ ì¶•ì†Œ**: \"ë¹„ë°€\" í‘œì‹œ ëˆ„ë½ ì‹œ í•µì‹¬ì •ë³´ë„ ë³´í˜¸ë°›ì§€ ëª»í•˜ëŠ” ì¡°í•­\n",
        "2. **ì¼ë°©ì  ë¹„ë°€ìœ ì§€ ì˜ë¬´**: ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ì—ê²Œë§Œ ê³¼ë„í•œ ë¹„ë°€ìœ ì§€ ì˜ë¬´ ë¶€ê³¼\n",
        "3. **ë¹„ë°€ì •ë³´ ì •ì˜ì˜ ë¶ˆê· í˜•**: ìƒëŒ€ë°© ì •ë³´ëŠ” ë„“ê²Œ, ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ ì •ë³´ëŠ” ì¢ê²Œ ì •ì˜\n",
        "\n",
        "**âš–ï¸ ì±…ì„ ë° ë°°ìƒ ê´€ë ¨ ìœ„í—˜:**\n",
        "4. **ê³¼ë„í•œ ì†í•´ë°°ìƒ**: ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ì—ê²Œë§Œ ê³¼ì¤‘í•œ ë°°ìƒì±…ì„ ë¶€ê³¼\n",
        "5. **ë°°ìƒ í•œë„ì˜ ë¶ˆê· í˜•**: ìƒëŒ€ë°©ì€ ë¬´ì œí•œ, ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ëŠ” ì œí•œëœ ë°°ìƒ\n",
        "6. **ì…ì¦ì±…ì„ì˜ ì „ê°€**: ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ê°€ ë¬´ê³¼ì‹¤ì„ ì…ì¦í•´ì•¼ í•˜ëŠ” ì¡°í•­\n",
        "\n",
        "**ğŸšª ê³„ì•½ í•´ì§€ ë° ê¶Œë¦¬ ê´€ë ¨ ìœ„í—˜:**\n",
        "7. **ì¼ë°©ì  í•´ì§€ê¶Œ**: ìƒëŒ€ë°©ì—ê²Œë§Œ í•´ì§€ê¶Œì„ ë¶€ì—¬í•˜ëŠ” ë¶ˆí‰ë“± ì¡°í•­\n",
        "8. **ê¶Œë¦¬ í–‰ì‚¬ ì œí•œ**: ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ì˜ ì •ë‹¹í•œ ê¶Œë¦¬(ê°€ì²˜ë¶„, ì†í•´ë°°ìƒì²­êµ¬ ë“±) ì œí•œ\n",
        "9. **í†µì§€ ì˜ë¬´ì˜ ë¶ˆê· í˜•**: ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ì—ê²Œë§Œ ê¹Œë‹¤ë¡œìš´ í†µì§€ ì˜ë¬´ ë¶€ê³¼\n",
        "\n",
        "**ğŸ“‹ ì ˆì°¨ ë° ê¸°íƒ€ ìœ„í—˜:**\n",
        "10. **ê´€í•  ë²•ì›ì˜ ë¶ˆë¦¬í•¨**: ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ì—ê²Œ ë¶ˆë¦¬í•œ ì›ê±°ë¦¬ ê´€í•  ì§€ì •\n",
        "11. **ê³„ì•½ ê¸°ê°„ì˜ ë¶ˆê· í˜•**: ì˜ë¬´ëŠ” ê¸¸ê²Œ, ê¶Œë¦¬ëŠ” ì§§ê²Œ ì„¤ì •\n",
        "12. **ë²•ì  ìœ„í—˜**: ê°•í–‰ë²•ê·œ ìœ„ë°˜ìœ¼ë¡œ ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ê°€ ë¶ˆì´ìµì„ ë°›ì„ ìˆ˜ ìˆëŠ” ì¡°í•­\n",
        "\n",
        "**âš ï¸ íŠ¹íˆ ì£¼ì˜ê¹Šê²Œ ê²€í† í•  ì¡°í•­:**\n",
        "- ë¹„ë°€ì •ë³´ ì •ì˜ì—ì„œ \"í‘œì‹œ\" ìš”êµ¬ì‚¬í•­ (ì‹¤ë¬´ìƒ ëˆ„ë½ ìœ„í—˜ ë†’ìŒ)\n",
        "- ì†í•´ë°°ìƒ ì¡°í•­ì˜ ê¸ˆì•¡ ì œí•œ ë° ë©´ì±… ì¡°í•­\n",
        "- ê³„ì•½ ìœ„ë°˜ ì‹œ êµ¬ì œìˆ˜ë‹¨ ì œí•œ ì¡°í•­\n",
        "- ì¤€ê±°ë²• ë° ê´€í•  ì¡°í•­\n",
        "\n",
        "## ğŸ“ ì¶œë ¥ í˜•ì‹ (JSON)\n",
        "\n",
        "ìœ„ë²• ì¡°í•­ì´ ìˆë‹¤ë©´ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”:\n",
        "\n",
        "```json\n",
        "{{\n",
        "  \"violations_found\": true,\n",
        "  \"total_violations\": 3,\n",
        "  \"violations\": [\n",
        "    {{\n",
        "      \"clause_number\": \"ì œ3ì¡°\",\n",
        "      \"clause_title\": \"ë¹„ë°€ì •ë³´ì˜ ì •ì˜\",\n",
        "      \"risk_type\": \"ì •ë³´_ë³´í˜¸_ë²”ìœ„ì˜_ê³¼ë„í•œ_ì¶•ì†Œ\",\n",
        "      \"risk_level\": \"ë†’ìŒ\",\n",
        "      \"brief_reason\": \"ë¹„ë°€ í‘œì‹œ ëˆ„ë½ ì‹œ í•µì‹¬ì •ë³´ë„ ë³´í˜¸ë°›ì§€ ëª»í•´ ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ì—ê²Œ ë§¤ìš° ìœ„í—˜\"\n",
        "    }},\n",
        "    {{\n",
        "      \"clause_number\": \"ì œ9ì¡°\", \n",
        "      \"clause_title\": \"ì†í•´ë°°ìƒ\",\n",
        "      \"risk_type\": \"ê³¼ë„í•œ_ì†í•´ë°°ìƒ_ë°_ê¶Œë¦¬ì œí•œ\",\n",
        "      \"risk_level\": \"ë†’ìŒ\",\n",
        "      \"brief_reason\": \"ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ë§Œ ë°°ìƒ í•œë„ ì œí•œë˜ê³  ê°€ì²˜ë¶„ ì²­êµ¬ê¶Œë„ ë°•íƒˆë‹¹í•¨\"\n",
        "    }},\n",
        "    {{\n",
        "      \"clause_number\": \"ì œ8ì¡°\",\n",
        "      \"clause_title\": \"ê³„ì•½ê¸°ê°„\",\n",
        "      \"risk_type\": \"ê³„ì•½_ê¸°ê°„ì˜_ë¶ˆê· í˜•\",\n",
        "      \"risk_level\": \"ì¤‘ê°„\", \n",
        "      \"brief_reason\": \"ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ì˜ ì˜ë¬´ëŠ” 3ë…„+1ë…„, ìƒëŒ€ë°© ì˜ë¬´ëŠ” ê³„ì•½ ì¢…ë£Œì™€ í•¨ê»˜ ì†Œë©¸\"\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "```\n",
        "\n",
        "ìœ„ë²• ì¡°í•­ì´ ì—†ë‹¤ë©´:\n",
        "```json\n",
        "{{\n",
        "  \"violations_found\": false,\n",
        "  \"total_violations\": 0,\n",
        "  \"violations\": []\n",
        "}}\n",
        "```\n",
        "\n",
        "**ì¤‘ìš”**: ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ê³ , ì¶”ê°€ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.\"\"\"\n",
        "\n",
        "            # AI í˜¸ì¶œ\n",
        "            from src.aws.bedrock_service import BedrockService\n",
        "            bedrock_service = BedrockService()\n",
        "            \n",
        "            response = bedrock_service.invoke_model(\n",
        "                prompt=chain1_prompt,\n",
        "                max_tokens=2000,\n",
        "                temperature=0.0\n",
        "            )\n",
        "            \n",
        "            # JSON íŒŒì‹±\n",
        "            response_text = response.get(\"text\", \"\") if isinstance(response, dict) else str(response)\n",
        "            \n",
        "            import json\n",
        "            import re\n",
        "            \n",
        "            # JSON ì¶”ì¶œ (```json íƒœê·¸ ì œê±°)\n",
        "            json_match = re.search(r'```json\\s*(\\{.*?\\})\\s*```', response_text, re.DOTALL)\n",
        "            if json_match:\n",
        "                json_str = json_match.group(1)\n",
        "            else:\n",
        "                # JSON íƒœê·¸ ì—†ì´ ì§ì ‘ JSONì¸ ê²½ìš°\n",
        "                json_str = response_text.strip()\n",
        "            \n",
        "            try:\n",
        "                result = json.loads(json_str)\n",
        "                violations = result.get(\"violations\", [])\n",
        "                \n",
        "                # chunk_index ì¶”ê°€ (ì¡°í•­ ë²ˆí˜¸ì—ì„œ ì¶”ì¶œ)\n",
        "                for violation in violations:\n",
        "                    clause_num = violation.get(\"clause_number\", \"\")\n",
        "                    # \"ì œ3ì¡°\" -> 3 ì¶”ì¶œ\n",
        "                    import re\n",
        "                    match = re.search(r'ì œ(\\d+)ì¡°', clause_num)\n",
        "                    if match:\n",
        "                        violation[\"chunk_index\"] = int(match.group(1)) - 1  # 0-based\n",
        "                    else:\n",
        "                        violation[\"chunk_index\"] = 0\n",
        "                \n",
        "                logger.info(f\"ğŸ” Chain 1 ê²°ê³¼: {len(violations)}ê°œ ìœ„ë²• ì¡°í•­ í›„ë³´ ì‹ë³„\")\n",
        "                return violations\n",
        "                \n",
        "            except json.JSONDecodeError as e:\n",
        "                logger.error(f\"âŒ Chain 1 JSON íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
        "                logger.error(f\"ì‘ë‹µ í…ìŠ¤íŠ¸: {response_text[:500]}...\")\n",
        "                return []\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"âŒ Chain 1 ì‹¤íŒ¨: {str(e)}\")\n",
        "            return []\n",
        "    \n",
        "    async def _chain2_search_related_laws(self, violation_candidates: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"Chain 2: ê° ìœ„ë²• ì¡°í•­ë³„ë¡œ ê´€ë ¨ ë²•ë ¹ì„ ë³‘ë ¬ ê²€ìƒ‰\"\"\"\n",
        "        violations_with_laws = []\n",
        "        \n",
        "        # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ íƒœìŠ¤í¬ ìƒì„±\n",
        "        search_tasks = []\n",
        "        for violation in violation_candidates:\n",
        "            search_query = f\"{violation.get('clause_title', '')} {violation.get('brief_reason', '')}\"\n",
        "            task = self._search_laws_for_violation(violation, search_query)\n",
        "            search_tasks.append(task)\n",
        "        \n",
        "        # ë³‘ë ¬ ì‹¤í–‰\n",
        "        if search_tasks:\n",
        "            results = await asyncio.gather(*search_tasks, return_exceptions=True)\n",
        "            \n",
        "            for result in results:\n",
        "                if isinstance(result, Exception):\n",
        "                    logger.error(f\"âŒ ë²•ë ¹ ê²€ìƒ‰ ì‹¤íŒ¨: {result}\")\n",
        "                else:\n",
        "                    violations_with_laws.append(result)\n",
        "        \n",
        "        logger.info(f\"ğŸ” Chain 2 ê²°ê³¼: {len(violations_with_laws)}ê°œ ì¡°í•­ì˜ ë²•ë ¹ ê²€ìƒ‰ ì™„ë£Œ\")\n",
        "        return violations_with_laws\n",
        "    \n",
        "    async def _search_laws_for_violation(self, violation: Dict, search_query: str) -> Dict:\n",
        "        \"\"\"ê°œë³„ ìœ„ë²• ì¡°í•­ì— ëŒ€í•œ ë²•ë ¹ ê²€ìƒ‰\"\"\"\n",
        "        try:\n",
        "            # ê´€ë ¨ ë²•ë ¹ ê²€ìƒ‰\n",
        "            legal_docs = await self.client.search_documents_direct(\n",
        "                query=search_query,\n",
        "                top_k=3,\n",
        "                doc_types=[\"law\"]\n",
        "            )\n",
        "            \n",
        "            violation_with_laws = violation.copy()\n",
        "            violation_with_laws[\"related_laws\"] = legal_docs.get(\"results\", [])\n",
        "            violation_with_laws[\"laws_found\"] = len(legal_docs.get(\"results\", []))\n",
        "            \n",
        "            logger.info(f\"  ğŸ“– {violation.get('clause_title', '')}: {len(legal_docs.get('results', []))}ê°œ ê´€ë ¨ ë²•ë ¹ ë°œê²¬\")\n",
        "            return violation_with_laws\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"âŒ ë²•ë ¹ ê²€ìƒ‰ ì‹¤íŒ¨ ({violation.get('clause_title', '')}): {e}\")\n",
        "            violation_with_laws = violation.copy()\n",
        "            violation_with_laws[\"related_laws\"] = []\n",
        "            violation_with_laws[\"laws_found\"] = 0\n",
        "            return violation_with_laws\n",
        "    \n",
        "    # ë©”ì„œë“œë“¤ì„ í´ë˜ìŠ¤ì— ì¶”ê°€\n",
        "    rag_class._chain1_identify_violations = _chain1_identify_violations\n",
        "    rag_class._chain2_search_related_laws = _chain2_search_related_laws\n",
        "    rag_class._search_laws_for_violation = _search_laws_for_violation\n",
        "    return rag_class\n",
        "\n",
        "# RAGPipelineì— ì²´ì¸ í—¬í¼ ë©”ì„œë“œ ì¶”ê°€ (Part 2)\n",
        "RAGPipeline = add_chain_helper_methods_part2(RAGPipeline)\n",
        "print(\"âœ… RAGPipeline ì²´ì¸ í—¬í¼ ë©”ì„œë“œ ì¶”ê°€ ì™„ë£Œ (Part 2: Chain 1, 2)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3 ì²´ì¸ ë¶„ì„ í—¬í¼ ë©”ì„œë“œë“¤ (Part 3: Chain 3 ë° ê²°ê³¼ ì €ì¥)\n",
        "\n",
        "def add_chain_helper_methods_part3(rag_class):\n",
        "    \"\"\"RAGPipelineì— ì²´ì¸ ë¶„ì„ ë©”ì„œë“œë“¤ ì¶”ê°€ (Part 3: Chain 3 ë° ê²°ê³¼ ì €ì¥)\"\"\"\n",
        "    \n",
        "    async def _chain3_detailed_analysis(self, violations_with_laws: List[Dict], document_name: str) -> Dict:\n",
        "        \"\"\"Chain 3: ìœ„ë²• ì¡°í•­ê³¼ ë²•ë ¹ ê·¼ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ìƒì„¸ ë¶„ì„\"\"\"\n",
        "        try:\n",
        "            if not violations_with_laws:\n",
        "                return {\"violations\": []}\n",
        "            \n",
        "            final_violations = []\n",
        "            \n",
        "            for violation_data in violations_with_laws:\n",
        "                try:\n",
        "                    # ìƒì„¸ ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
        "                    clause_title = violation_data.get(\"clause_title\", \"\")\n",
        "                    clause_number = violation_data.get(\"clause_number\", \"\")\n",
        "                    risk_type = violation_data.get(\"risk_type\", \"\")\n",
        "                    brief_reason = violation_data.get(\"brief_reason\", \"\")\n",
        "                    related_laws = violation_data.get(\"related_laws\", [])\n",
        "                    \n",
        "                    # ê´€ë ¨ ë²•ë ¹ í…ìŠ¤íŠ¸ êµ¬ì„±\n",
        "                    laws_text = \"\"\n",
        "                    if related_laws:\n",
        "                        law_descriptions = []\n",
        "                        for i, law in enumerate(related_laws, 1):\n",
        "                            filename = law.get('filename', '').replace('.pdf', '')\n",
        "                            content = law.get('content', '')[:300] + \"...\" if len(law.get('content', '')) > 300 else law.get('content', '')\n",
        "                            similarity = f\"(ìœ ì‚¬ë„: {law.get('similarity_score', 0):.3f})\"\n",
        "                            law_descriptions.append(f\"{i}. {filename} {similarity}\\n{content}\")\n",
        "                        laws_text = \"\\n\\n\".join(law_descriptions)\n",
        "                    else:\n",
        "                        laws_text = \"ê´€ë ¨ ë²•ë ¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
        "                    \n",
        "                    chain3_prompt = f\"\"\"# ğŸ›ï¸ (ì£¼)ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ ê´€ì  ê³„ì•½ì„œ ì¡°í•­ ìƒì„¸ ë¶„ì„\n",
        "\n",
        "**ë¶„ì„ ëŒ€ìƒ ì¡°í•­:** {clause_number} {clause_title}\n",
        "**ê²€í†  ê´€ì :** (ì£¼)ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ ì…ì¥ì—ì„œì˜ ìœ„í—˜ì„± í‰ê°€\n",
        "**ì˜ˆë¹„ ìœ„í—˜ ìœ í˜•:** {risk_type}\n",
        "**ì˜ˆë¹„ íŒë‹¨:** {brief_reason}\n",
        "\n",
        "## ğŸ“š ê´€ë ¨ ë²•ë ¹ ê·¼ê±°\n",
        "\n",
        "{laws_text}\n",
        "\n",
        "## ğŸ¯ ìƒì„¸ ë¶„ì„ ìš”êµ¬ì‚¬í•­\n",
        "\n",
        "ìœ„ ì¡°í•­ê³¼ ê´€ë ¨ ë²•ë ¹ì„ ë°”íƒ•ìœ¼ë¡œ **(ì£¼)ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ ì…ì¥ì—ì„œ** ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìƒì„¸ ë¶„ì„í•´ì£¼ì„¸ìš”:\n",
        "\n",
        "### ì¶œë ¥ í˜•ì‹ (JSON):\n",
        "```json\n",
        "{{\n",
        "  \"ì¡°í•­_ìœ„ì¹˜\": \"{clause_number} ({clause_title})\",\n",
        "  \"ë¦¬ìŠ¤í¬_ìœ í˜•\": \"ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ ê´€ì ì˜ êµ¬ì²´ì ì¸ ìœ„í—˜ ìœ í˜• (ì˜ˆ: ì •ë³´ ë³´í˜¸ ë²”ìœ„ì˜ ê³¼ë„í•œ ì¶•ì†Œ, ê³¼ë„í•œ_ë°°ìƒì±…ì„, ë¶ˆë¦¬í•œ_í•´ì§€ì¡°ê±´)\",\n",
        "  \"íŒë‹¨_ê·¼ê±°\": \"ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ì—ê²Œ ì–´ë–¤ ë¶ˆì´ìµì´ ìˆëŠ”ì§€ ê´€ë ¨ ë²•ë ¹ê³¼ í•¨ê»˜ êµ¬ì²´ì  ì œì‹œ (ì˜ˆ: ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ì—ê²Œë§Œ ë¯¼ë²• ì œ398ì¡° ìœ„ë°˜ ìˆ˜ì¤€ì˜ ê³¼ë„í•œ ë°°ìƒì±…ì„ ë¶€ê³¼)\",\n",
        "  \"ì˜ê²¬\": \"ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ ì…ì¥ì—ì„œì˜ êµ¬ì²´ì ì¸ ê°œì„  ë°©ì•ˆ (ì˜ˆ: ìƒí˜¸ ë°°ìƒì±…ì„ìœ¼ë¡œ ë³€ê²½í•˜ê±°ë‚˜ ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ ë°°ìƒ í•œë„ ì„¤ì • í•„ìš”)\",\n",
        "  \"ê´€ë ¨_ë²•ë ¹\": [\"êµ¬ì²´ì ì¸ ë²•ë ¹ ì¡°í•­ëª…\"]\n",
        "}}\n",
        "```\n",
        "\n",
        "**ì¤‘ìš”**: \n",
        "1. ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥\n",
        "2. **(ì£¼)ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ ì…ì¥**ì—ì„œ ë¶ˆë¦¬í•œ ì ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„\n",
        "3. ê´€ë ¨ ë²•ë ¹ì´ ìˆë‹¤ë©´ êµ¬ì²´ì ì¸ ì¡°í•­ëª… ëª…ì‹œ  \n",
        "4. ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ì—ê²Œ ì‹¤ë¬´ì ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ” ê°œì„ ë°©ì•ˆ ì œì‹œ\n",
        "5. ì¶”ê°€ ì„¤ëª… ì—†ì´ JSONë§Œ ì¶œë ¥\"\"\"\n",
        "\n",
        "                    # AI í˜¸ì¶œ\n",
        "                    from src.aws.bedrock_service import BedrockService\n",
        "                    bedrock_service = BedrockService()\n",
        "                    \n",
        "                    response = bedrock_service.invoke_model(\n",
        "                        prompt=chain3_prompt,\n",
        "                        max_tokens=1500,\n",
        "                        temperature=0.0\n",
        "                    )\n",
        "                    \n",
        "                    # JSON íŒŒì‹±\n",
        "                    response_text = response.get(\"text\", \"\") if isinstance(response, dict) else str(response)\n",
        "                    \n",
        "                    import json\n",
        "                    import re\n",
        "                    \n",
        "                    # JSON ì¶”ì¶œ\n",
        "                    json_match = re.search(r'```json\\s*(\\{.*?\\})\\s*```', response_text, re.DOTALL)\n",
        "                    if json_match:\n",
        "                        json_str = json_match.group(1)\n",
        "                    else:\n",
        "                        json_str = response_text.strip()\n",
        "                    \n",
        "                    try:\n",
        "                        detailed_analysis = json.loads(json_str)\n",
        "                        final_violations.append(detailed_analysis)\n",
        "                        logger.info(f\"  âœ… {clause_title} ìƒì„¸ ë¶„ì„ ì™„ë£Œ\")\n",
        "                        \n",
        "                    except json.JSONDecodeError as e:\n",
        "                        logger.error(f\"âŒ Chain 3 JSON íŒŒì‹± ì‹¤íŒ¨ ({clause_title}): {e}\")\n",
        "                        # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í˜•ì‹ìœ¼ë¡œ ëŒ€ì²´\n",
        "                        final_violations.append({\n",
        "                            \"ì¡°í•­_ìœ„ì¹˜\": f\"{clause_number} ({clause_title})\",\n",
        "                            \"ë¦¬ìŠ¤í¬_ìœ í˜•\": risk_type,\n",
        "                            \"íŒë‹¨_ê·¼ê±°\": brief_reason,\n",
        "                            \"ì˜ê²¬\": \"ìƒì„¸ ë¶„ì„ ì‹¤íŒ¨ë¡œ ì¸í•´ ê¸°ë³¸ ì •ë³´ë§Œ ì œê³µ\",\n",
        "                            \"ê´€ë ¨_ë²•ë ¹\": [law.get('filename', '').replace('.pdf', '') for law in related_laws[:2]]\n",
        "                        })\n",
        "                \n",
        "                except Exception as e:\n",
        "                    logger.error(f\"âŒ ê°œë³„ ì¡°í•­ ë¶„ì„ ì‹¤íŒ¨ ({violation_data.get('clause_title', '')}): {e}\")\n",
        "                    continue\n",
        "            \n",
        "            return {\n",
        "                \"contract_analysis\": {\n",
        "                    \"document_name\": document_name,\n",
        "                    \"total_violations\": len(final_violations),\n",
        "                    \"analysis_method\": \"3_chain_analysis\",\n",
        "                    \"analysis_date\": datetime.now().isoformat()\n",
        "                },\n",
        "                \"violations\": final_violations\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"âŒ Chain 3 ì‹¤íŒ¨: {str(e)}\")\n",
        "            return {\"violations\": []}\n",
        "    \n",
        "    async def _save_chain_analysis_results(self, document_name: str, analysis_result: Dict, performance_stats: Dict):\n",
        "        \"\"\"ì²´ì¸ ë¶„ì„ ê²°ê³¼ ì €ì¥\"\"\"\n",
        "        try:\n",
        "            # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            \n",
        "            # ê²°ê³¼ íŒŒì¼ëª…\n",
        "            result_filename = f\"chain_analysis_{document_name.replace('.docx', '')}_{timestamp}.json\"\n",
        "            result_file = RESULTS_DIR / result_filename\n",
        "            \n",
        "            # ìµœì¢… ê²°ê³¼ êµ¬ì„±\n",
        "            final_result = {\n",
        "                \"metadata\": {\n",
        "                    \"timestamp\": datetime.now().isoformat(),\n",
        "                    \"document_name\": document_name,\n",
        "                    \"analysis_method\": \"3_chain_analysis\",\n",
        "                    \"performance\": performance_stats\n",
        "                },\n",
        "                \"summary\": {\n",
        "                    \"violations_found\": len(analysis_result.get(\"violations\", [])),\n",
        "                    \"total_analysis_time\": performance_stats.get(\"total_time\", 0),\n",
        "                    \"efficiency_gain\": f\"ê¸°ì¡´ ì¡°í•­ë³„ ë¶„ì„ ëŒ€ë¹„ {performance_stats.get('total_clauses', 0) * 30 - performance_stats.get('total_time', 0):.1f}ì´ˆ ë‹¨ì¶•\"\n",
        "                },\n",
        "                \"analysis_result\": analysis_result\n",
        "            }\n",
        "            \n",
        "            # JSON íŒŒì¼ ì €ì¥\n",
        "            with open(result_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(final_result, f, ensure_ascii=False, indent=2)\n",
        "            \n",
        "            # ë¡œê·¸ ì¶œë ¥\n",
        "            logger.info(\"=\" * 50)\n",
        "            logger.info(\"ğŸ“ ì²´ì¸ ë¶„ì„ ê²°ê³¼ ìš”ì•½\")\n",
        "            logger.info(\"=\" * 50)\n",
        "            logger.info(f\"ğŸ“„ ë¬¸ì„œëª…: {document_name}\")\n",
        "            logger.info(f\"ğŸ” ìœ„ë²• ì¡°í•­ ë°œê²¬: {len(analysis_result.get('violations', []))}ê°œ\")\n",
        "            logger.info(f\"â±ï¸ ì „ì²´ ë¶„ì„ ì‹œê°„: {performance_stats.get('total_time', 0):.2f}ì´ˆ\")\n",
        "            logger.info(f\"   - Chain 1 (ìœ„ë²• ì¡°í•­ ì‹ë³„): {performance_stats.get('chain1_time', 0):.2f}ì´ˆ\")\n",
        "            logger.info(f\"   - Chain 2 (ë²•ë ¹ ê²€ìƒ‰): {performance_stats.get('chain2_time', 0):.2f}ì´ˆ\")  \n",
        "            logger.info(f\"   - Chain 3 (ìƒì„¸ ë¶„ì„): {performance_stats.get('chain3_time', 0):.2f}ì´ˆ\")\n",
        "            \n",
        "            # ê°œë³„ ìœ„ë²• ì¡°í•­ ìš”ì•½\n",
        "            if analysis_result.get(\"violations\"):\n",
        "                logger.info(f\"ğŸ“Š ë°œê²¬ëœ ìœ„ë²• ì¡°í•­:\")\n",
        "                for i, violation in enumerate(analysis_result[\"violations\"], 1):\n",
        "                    logger.info(f\"   {i}. {violation.get('ì¡°í•­_ìœ„ì¹˜', '')} - {violation.get('ë¦¬ìŠ¤í¬_ìœ í˜•', '')}\")\n",
        "            \n",
        "            logger.info(f\"ğŸ’¾ ê²°ê³¼ ì €ì¥: {result_file}\")\n",
        "            logger.info(\"=\" * 50)\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"âŒ ì²´ì¸ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}\")\n",
        "    \n",
        "    # ë©”ì„œë“œë“¤ì„ í´ë˜ìŠ¤ì— ì¶”ê°€\n",
        "    rag_class._chain3_detailed_analysis = _chain3_detailed_analysis\n",
        "    rag_class._save_chain_analysis_results = _save_chain_analysis_results\n",
        "    return rag_class\n",
        "\n",
        "# RAGPipelineì— ì²´ì¸ í—¬í¼ ë©”ì„œë“œ ì¶”ê°€ (Part 3)\n",
        "RAGPipeline = add_chain_helper_methods_part3(RAGPipeline)\n",
        "print(\"âœ… RAGPipeline ì²´ì¸ í—¬í¼ ë©”ì„œë“œ ì¶”ê°€ ì™„ë£Œ (Part 3: Chain 3 ë° ê²°ê³¼ ì €ì¥)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì„¹ì…˜\n",
        "\n",
        "ì´ì œ ëª¨ë“  í´ë˜ìŠ¤ì™€ ë©”ì„œë“œê°€ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ ì…€ë“¤ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ì—¬ ê° ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° í™˜ê²½ í™•ì¸\n",
        "\n",
        "# ë¹„ë™ê¸° ì‹¤í–‰ì„ ìœ„í•œ nest_asyncio ì ìš©\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
        "pipeline = RAGPipeline()\n",
        "\n",
        "print(\"âœ… RAG íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ\")\n",
        "print(f\"ğŸ“ ë¬¸ì„œ ë£¨íŠ¸: {DOCS_ROOT}\")\n",
        "\n",
        "# í™˜ê²½ë³€ìˆ˜ í™•ì¸\n",
        "required_env_vars = [\n",
        "    \"DOC_CONVERTER_URL\", \"DOC_PARSER_URL\", \"API_URL\",\n",
        "    \"DATABASE_URL\", \"DATABASE_USER\", \"DATABASE_PASSWORD\", \n",
        "    \"DATABASE_HOST\", \"DATABASE_PORT\", \"DATABASE_NAME\"\n",
        "]\n",
        "\n",
        "print(\"\\nğŸ“‹ í™˜ê²½ë³€ìˆ˜ í™•ì¸:\")\n",
        "for var in required_env_vars:\n",
        "    value = os.getenv(var)\n",
        "    if value:\n",
        "        # ë¹„ë°€ë²ˆí˜¸ëŠ” ë§ˆìŠ¤í‚¹\n",
        "        if \"PASSWORD\" in var:\n",
        "            value = \"*\" * len(value)\n",
        "        print(f\"  âœ… {var}: {value}\")\n",
        "    else:\n",
        "        print(f\"  âš ï¸ {var}: ì„¤ì •ë˜ì§€ ì•ŠìŒ\")\n",
        "\n",
        "# ë¬¸ì„œ íŒŒì¼ í™•ì¸\n",
        "document_files = pipeline.find_all_documents()\n",
        "print(f\"\\nğŸ“ ë°œê²¬ëœ ë¬¸ì„œ íŒŒì¼: {len(document_files)}ê°œ\")\n",
        "\n",
        "if document_files:\n",
        "    # íŒŒì¼ íƒ€ì…ë³„ ë¶„ë¥˜\n",
        "    type_counts = {}\n",
        "    for file_path in document_files:\n",
        "        doc_type = pipeline.client.determine_doc_type(file_path)\n",
        "        type_counts[doc_type] = type_counts.get(doc_type, 0) + 1\n",
        "    \n",
        "    print(\"ğŸ“Š ë¬¸ì„œ íƒ€ì…ë³„ ë¶„ë¥˜:\")\n",
        "    for doc_type, count in type_counts.items():\n",
        "        print(f\"  - {doc_type}: {count}ê°œ\")\n",
        "    \n",
        "    print(\"\\nğŸ“„ ë¬¸ì„œ íŒŒì¼ ëª©ë¡ (ì²˜ìŒ 5ê°œ):\")\n",
        "    for i, file_path in enumerate(document_files[:5], 1):\n",
        "        doc_type = pipeline.client.determine_doc_type(file_path)\n",
        "        print(f\"  {i}. {file_path.name} ({doc_type})\")\n",
        "    \n",
        "    if len(document_files) > 5:\n",
        "        print(f\"  ... ë° {len(document_files) - 5}ê°œ íŒŒì¼ ë”\")\n",
        "else:\n",
        "    print(\"âš ï¸ ì²˜ë¦¬í•  ë¬¸ì„œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    print(f\"   ë‹¤ìŒ í´ë”ì— ë¬¸ì„œ íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”: {DOCS_ROOT / 'ê·¼ê±° ìë£Œ'}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Step 1: ê·¼ê±°ìë£Œ ì €ì¥\n",
        "\n",
        "ë¬¸ì„œ íŒŒì¼ë“¤ì„ ë¶„ì„í•˜ê³  ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1 ì‹¤í–‰: ê·¼ê±°ìë£Œ ì €ì¥\n",
        "\n",
        "FORCE_REPROCESS = False  # ê°•ì œ ì¬ì²˜ë¦¬ ì—¬ë¶€ (Trueë¡œ ì„¤ì •í•˜ë©´ ê¸°ì¡´ íŒŒì¼ë„ ë‹¤ì‹œ ì²˜ë¦¬)\n",
        "\n",
        "print(\"ğŸš€ Step 1: ê·¼ê±°ìë£Œ ì €ì¥ ì‹œì‘\")\n",
        "print(f\"âš™ï¸ ê°•ì œ ì¬ì²˜ë¦¬: {'ì˜ˆ' if FORCE_REPROCESS else 'ì•„ë‹ˆì˜¤'}\")\n",
        "\n",
        "try:\n",
        "    success = await pipeline.step1_store_reference_materials(force=FORCE_REPROCESS)\n",
        "    \n",
        "    if success:\n",
        "        print(\"\\nğŸ‰ Step 1 ì™„ë£Œ!\")\n",
        "        print(f\"ğŸ“„ ì²˜ë¦¬ëœ íŒŒì¼: {len(pipeline.results)}ê°œ\")\n",
        "        \n",
        "        # ì„±ê³µ/ì‹¤íŒ¨ í†µê³„\n",
        "        successful = len([r for r in pipeline.results if r.success])\n",
        "        failed = len([r for r in pipeline.results if not r.success])\n",
        "        \n",
        "        print(f\"âœ… ì„±ê³µ: {successful}ê°œ\")\n",
        "        print(f\"âŒ ì‹¤íŒ¨: {failed}ê°œ\")\n",
        "        \n",
        "        if failed > 0:\n",
        "            print(\"\\nâŒ ì‹¤íŒ¨í•œ íŒŒì¼ë“¤:\")\n",
        "            for result in pipeline.results:\n",
        "                if not result.success:\n",
        "                    print(f\"  - {result.filename}: {result.error_message}\")\n",
        "        \n",
        "        # ì´ ì²˜ë¦¬ ì‹œê°„\n",
        "        total_time = sum(r.processing_time for r in pipeline.results)\n",
        "        print(f\"\\nâ±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ\")\n",
        "        \n",
        "        # íƒ€ì…ë³„ í†µê³„\n",
        "        type_stats = {}\n",
        "        for result in pipeline.results:\n",
        "            doc_type = result.doc_type\n",
        "            if doc_type not in type_stats:\n",
        "                type_stats[doc_type] = {\"total\": 0, \"success\": 0}\n",
        "            type_stats[doc_type][\"total\"] += 1\n",
        "            if result.success:\n",
        "                type_stats[doc_type][\"success\"] += 1\n",
        "        \n",
        "        print(\"\\nğŸ“Š íƒ€ì…ë³„ ì„±ê³µë¥ :\")\n",
        "        for doc_type, stats in type_stats.items():\n",
        "            success_rate = stats[\"success\"] / stats[\"total\"] * 100\n",
        "            print(f\"  - {doc_type}: {stats['success']}/{stats['total']} ({success_rate:.1f}%)\")\n",
        "        \n",
        "    else:\n",
        "        print(\"âŒ Step 1 ì‹¤íŒ¨!\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Step 1 ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Step 2: ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
        "\n",
        "ì €ì¥ëœ ë¬¸ì„œë“¤ì— ëŒ€í•œ ê²€ìƒ‰ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2 ì‹¤í–‰: ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
        "\n",
        "print(\"ğŸ” Step 2: ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹œì‘\")\n",
        "\n",
        "try:\n",
        "    success = await pipeline.step2_test_search()\n",
        "    \n",
        "    if success:\n",
        "        print(\"ğŸ‰ Step 2 ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
        "    else:\n",
        "        print(\"âŒ Step 2 ì‹¤íŒ¨!\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Step 2 ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ê°œë³„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (ì›í•˜ëŠ” ì¿¼ë¦¬ë¡œ ì§ì ‘ í…ŒìŠ¤íŠ¸)\n",
        "\n",
        "SEARCH_QUERY = \"ê°œì¸ì •ë³´ë³´í˜¸ë²•\"  # ì›í•˜ëŠ” ê²€ìƒ‰ì–´ë¡œ ë³€ê²½\n",
        "TOP_K = 5  # ë°˜í™˜í•  ê²°ê³¼ ìˆ˜\n",
        "DOC_TYPES = None  # íŠ¹ì • ë¬¸ì„œ íƒ€ì…ë§Œ ê²€ìƒ‰í•˜ë ¤ë©´ [\"law\", \"standard_contract\"] ë“±ìœ¼ë¡œ ì„¤ì •\n",
        "\n",
        "print(f\"ğŸ” ê°œë³„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\")\n",
        "print(f\"ê²€ìƒ‰ì–´: '{SEARCH_QUERY}'\")\n",
        "print(f\"ê²°ê³¼ ìˆ˜: {TOP_K}ê°œ\")\n",
        "print(f\"ë¬¸ì„œ íƒ€ì… í•„í„°: {DOC_TYPES if DOC_TYPES else 'ëª¨ë“  íƒ€ì…'}\")\n",
        "\n",
        "try:\n",
        "    # ê²€ìƒ‰ ì‹¤í–‰\n",
        "    start_time = time.time()\n",
        "    result = await pipeline.client.search_documents_direct(\n",
        "        query=SEARCH_QUERY,\n",
        "        top_k=TOP_K,\n",
        "        doc_types=DOC_TYPES\n",
        "    )\n",
        "    search_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"\\nâ±ï¸ ê²€ìƒ‰ ì‹œê°„: {search_time:.3f}ì´ˆ\")\n",
        "    print(f\"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {len(result.get('results', []))}ê°œ\")\n",
        "    \n",
        "    # ê²°ê³¼ ì¶œë ¥\n",
        "    for i, doc in enumerate(result.get(\"results\", []), 1):\n",
        "        print(f\"\\nğŸ“„ {i}. {doc.get('filename', 'Unknown')}\")\n",
        "        print(f\"   ğŸ“‚ íƒ€ì…: {doc.get('doc_type', 'Unknown')}\")\n",
        "        print(f\"   ğŸ¯ ìœ ì‚¬ë„: {doc.get('similarity_score', 0):.4f}\")\n",
        "        print(f\"   ğŸ“ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:\")\n",
        "        content = doc.get('content', '')[:200] + \"...\" if len(doc.get('content', '')) > 200 else doc.get('content', '')\n",
        "        print(f\"   {content}\")\n",
        "    \n",
        "    if not result.get(\"results\"):\n",
        "        print(\"âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Step 3: ê³„ì•½ì„œ ê²€í†  í…ŒìŠ¤íŠ¸ (3ë‹¨ê³„ ì²´ì¸ ë°©ì‹)\n",
        "\n",
        "3ë‹¨ê³„ ì²´ì¸ ë°©ì‹ìœ¼ë¡œ ê³„ì•½ì„œì˜ ìœ„ë²• ì¡°í•­ì„ ë¶„ì„í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3 ì‹¤í–‰: ê³„ì•½ì„œ ê²€í†  í…ŒìŠ¤íŠ¸ (3ë‹¨ê³„ ì²´ì¸ ë°©ì‹)\n",
        "\n",
        "print(\"ğŸ“ Step 3: ê³„ì•½ì„œ ê²€í†  í…ŒìŠ¤íŠ¸ ì‹œì‘\")\n",
        "\n",
        "try:\n",
        "    success = await pipeline.step3_contract_review_test()\n",
        "    \n",
        "    if success:\n",
        "        print(\"ğŸ‰ Step 3 ê³„ì•½ì„œ ê²€í†  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
        "    else:\n",
        "        print(\"âŒ Step 3 ì‹¤íŒ¨!\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Step 3 ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”\n",
        "\n",
        "ì‹¤í–‰ëœ ë‹¨ê³„ë“¤ì˜ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ì‹œê°í™”í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "\n",
        "# í•œê¸€ í°íŠ¸ ì„¤ì • (ì‹œìŠ¤í…œì— ë”°ë¼ ì¡°ì • í•„ìš”)\n",
        "plt.rcParams['font.family'] = ['DejaVu Sans', 'Malgun Gothic', 'NanumGothic']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "print(\"ğŸ“Š ê²°ê³¼ ë¶„ì„ ì‹œì‘\")\n",
        "\n",
        "# 1. ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ íŒŒì¼ í™•ì¸\n",
        "summary_file = RESULTS_DIR / \"processing_summary.json\"\n",
        "search_results_files = list(RESULTS_DIR.glob(\"search_test_results_*.json\"))\n",
        "chain_results_files = list(RESULTS_DIR.glob(\"chain_analysis_*.json\"))\n",
        "\n",
        "if summary_file.exists():\n",
        "    # ì²˜ë¦¬ ê²°ê³¼ ë¶„ì„\n",
        "    with open(summary_file, 'r', encoding='utf-8') as f:\n",
        "        summary = json.load(f)\n",
        "    \n",
        "    print(f\"\\nğŸ“„ ë¬¸ì„œ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½:\")\n",
        "    print(f\"  ì „ì²´ íŒŒì¼: {summary['total_files']}ê°œ\")\n",
        "    print(f\"  ì„±ê³µ: {summary['successful']}ê°œ\")\n",
        "    print(f\"  ì‹¤íŒ¨: {summary['failed']}ê°œ\")\n",
        "    print(f\"  ì´ ì²˜ë¦¬ ì‹œê°„: {summary['total_processing_time']:.2f}ì´ˆ\")\n",
        "    \n",
        "    # ë¬¸ì„œ íƒ€ì…ë³„ ì„±ê³µë¥  ì‹œê°í™”\n",
        "    if summary.get('type_statistics'):\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "        \n",
        "        # íƒ€ì…ë³„ íŒŒì¼ ìˆ˜\n",
        "        doc_types = list(summary['type_statistics'].keys())\n",
        "        totals = [summary['type_statistics'][dt]['total'] for dt in doc_types]\n",
        "        \n",
        "        ax1.bar(doc_types, totals, color='skyblue')\n",
        "        ax1.set_title('ë¬¸ì„œ íƒ€ì…ë³„ íŒŒì¼ ìˆ˜')\n",
        "        ax1.set_xlabel('ë¬¸ì„œ íƒ€ì…')\n",
        "        ax1.set_ylabel('íŒŒì¼ ìˆ˜')\n",
        "        ax1.tick_params(axis='x', rotation=45)\n",
        "        \n",
        "        # íƒ€ì…ë³„ ì„±ê³µë¥ \n",
        "        success_rates = [summary['type_statistics'][dt]['success'] / summary['type_statistics'][dt]['total'] * 100 \n",
        "                        for dt in doc_types]\n",
        "        \n",
        "        ax2.bar(doc_types, success_rates, color='lightgreen')\n",
        "        ax2.set_title('ë¬¸ì„œ íƒ€ì…ë³„ ì„±ê³µë¥ ')\n",
        "        ax2.set_xlabel('ë¬¸ì„œ íƒ€ì…')\n",
        "        ax2.set_ylabel('ì„±ê³µë¥  (%)')\n",
        "        ax2.set_ylim(0, 100)\n",
        "        ax2.tick_params(axis='x', rotation=45)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # ìƒì„¸ í†µê³„ ì¶œë ¥\n",
        "        print(f\"\\nğŸ“Š ë¬¸ì„œ íƒ€ì…ë³„ í†µê³„:\")\n",
        "        for doc_type, stats in summary['type_statistics'].items():\n",
        "            success_rate = stats['success'] / stats['total'] * 100\n",
        "            print(f\"  {doc_type}: {stats['success']}/{stats['total']} ({success_rate:.1f}%)\")\n",
        "    \n",
        "else:\n",
        "    print(\"âš ï¸ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. Step 1ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
        "\n",
        "# 2. ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„\n",
        "if search_results_files:\n",
        "    # ê°€ì¥ ìµœê·¼ ê²€ìƒ‰ ê²°ê³¼ íŒŒì¼ ì‚¬ìš©\n",
        "    latest_search_file = sorted(search_results_files)[-1]\n",
        "    \n",
        "    with open(latest_search_file, 'r', encoding='utf-8') as f:\n",
        "        search_data = json.load(f)\n",
        "    \n",
        "    print(f\"\\nğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½:\")\n",
        "    test_summary = search_data['test_summary']\n",
        "    print(f\"  ì „ì²´ ì¿¼ë¦¬: {test_summary['total_queries']}ê°œ\")\n",
        "    print(f\"  ì„±ê³µ: {test_summary['successful_queries']}ê°œ\")\n",
        "    print(f\"  ì‹¤íŒ¨: {test_summary['failed_queries']}ê°œ\")\n",
        "    print(f\"  ì„±ê³µë¥ : {test_summary['success_rate']*100:.1f}%\")\n",
        "    print(f\"  í‰ê·  ê²€ìƒ‰ ì‹œê°„: {test_summary['avg_search_time_seconds']:.3f}ì´ˆ\")\n",
        "    print(f\"  í‰ê·  ê²°ê³¼ ìˆ˜: {test_summary['avg_results_per_query']:.1f}ê°œ\")\n",
        "    \n",
        "    # ì¹´í…Œê³ ë¦¬ë³„ ì„±ê³µë¥  ì‹œê°í™”\n",
        "    if search_data.get('category_performance'):\n",
        "        categories = list(search_data['category_performance'].keys())\n",
        "        success_rates = [search_data['category_performance'][cat]['success_rate'] * 100 \n",
        "                        for cat in categories]\n",
        "        \n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.bar(categories, success_rates, color='orange')\n",
        "        plt.title('ê²€ìƒ‰ ì¹´í…Œê³ ë¦¬ë³„ ì„±ê³µë¥ ')\n",
        "        plt.xlabel('ì¹´í…Œê³ ë¦¬')\n",
        "        plt.ylabel('ì„±ê³µë¥  (%)')\n",
        "        plt.ylim(0, 100)\n",
        "        plt.tick_params(axis='x', rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        print(f\"\\nğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ì„±ê³µë¥ :\")\n",
        "        for category, performance in search_data['category_performance'].items():\n",
        "            success_rate = performance['success_rate'] * 100\n",
        "            print(f\"  {category}: {performance['success_count']}/{performance['total_count']} ({success_rate:.1f}%)\")\n",
        "    \n",
        "else:\n",
        "    print(\"âš ï¸ ê²€ìƒ‰ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. Step 2ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
        "\n",
        "# 3. ì²´ì¸ ë¶„ì„ ê²°ê³¼\n",
        "if chain_results_files:\n",
        "    latest_chain_file = sorted(chain_results_files)[-1]\n",
        "    \n",
        "    with open(latest_chain_file, 'r', encoding='utf-8') as f:\n",
        "        chain_data = json.load(f)\n",
        "    \n",
        "    print(f\"\\nğŸ“ ì²´ì¸ ë¶„ì„ ê²°ê³¼ ìš”ì•½:\")\n",
        "    summary_data = chain_data['summary']\n",
        "    performance = chain_data['metadata']['performance']\n",
        "    \n",
        "    print(f\"  ë¬¸ì„œëª…: {chain_data['metadata']['document_name']}\")\n",
        "    print(f\"  ìœ„ë²• ì¡°í•­ ë°œê²¬: {summary_data['violations_found']}ê°œ\")\n",
        "    print(f\"  ì „ì²´ ë¶„ì„ ì‹œê°„: {summary_data['total_analysis_time']:.2f}ì´ˆ\")\n",
        "    print(f\"  Chain 1 ì‹œê°„: {performance['chain1_time']:.2f}ì´ˆ\")\n",
        "    print(f\"  Chain 2 ì‹œê°„: {performance['chain2_time']:.2f}ì´ˆ\")\n",
        "    print(f\"  Chain 3 ì‹œê°„: {performance['chain3_time']:.2f}ì´ˆ\")\n",
        "    \n",
        "    # ì²´ì¸ë³„ ì‹œê°„ ë¶„í¬ ì‹œê°í™”\n",
        "    chain_times = [performance['chain1_time'], performance['chain2_time'], performance['chain3_time']]\n",
        "    chain_labels = ['Chain 1\\n(ìœ„ë²•ì¡°í•­ ì‹ë³„)', 'Chain 2\\n(ë²•ë ¹ ê²€ìƒ‰)', 'Chain 3\\n(ìƒì„¸ ë¶„ì„)']\n",
        "    \n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.bar(chain_labels, chain_times, color=['red', 'blue', 'green'])\n",
        "    plt.title('ì²´ì¸ë³„ ë¶„ì„ ì‹œê°„')\n",
        "    plt.xlabel('ë¶„ì„ ë‹¨ê³„')\n",
        "    plt.ylabel('ì‹œê°„ (ì´ˆ)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # ë°œê²¬ëœ ìœ„ë²• ì¡°í•­ ìš”ì•½\n",
        "    violations = chain_data['analysis_result'].get('violations', [])\n",
        "    if violations:\n",
        "        print(f\"\\nğŸ“Š ë°œê²¬ëœ ìœ„ë²• ì¡°í•­:\")\n",
        "        for i, violation in enumerate(violations, 1):\n",
        "            print(f\"  {i}. {violation.get('ì¡°í•­_ìœ„ì¹˜', '')} - {violation.get('ë¦¬ìŠ¤í¬_ìœ í˜•', '')}\")\n",
        "    \n",
        "else:\n",
        "    print(\"âš ï¸ ì²´ì¸ ë¶„ì„ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. Step 3ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
        "\n",
        "# 4. ê²°ê³¼ íŒŒì¼ ëª©ë¡\n",
        "print(f\"\\nğŸ“ ìƒì„±ëœ ê²°ê³¼ íŒŒì¼ë“¤:\")\n",
        "result_files = list(RESULTS_DIR.glob(\"*\"))\n",
        "for result_file in sorted(result_files):\n",
        "    file_size = result_file.stat().st_size\n",
        "    file_size_mb = file_size / (1024 * 1024)\n",
        "    print(f\"  ğŸ“„ {result_file.name} ({file_size_mb:.2f} MB)\")\n",
        "\n",
        "print(f\"\\nğŸ“Š ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ëŠ” {RESULTS_DIR} í´ë”ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### ì¶”ê°€ ìœ í‹¸ë¦¬í‹°\n",
        "\n",
        "ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸, íŒŒì¼ ì •ë¦¬ ë“± ìœ ìš©í•œ ìœ í‹¸ë¦¬í‹° ê¸°ëŠ¥ë“¤ì…ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸\n",
        "\n",
        "async def check_database_status():\n",
        "    \"\"\"ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ ë° ì €ì¥ëœ ë¬¸ì„œ í™•ì¸\"\"\"\n",
        "    try:\n",
        "        from sqlmodel.ext.asyncio.session import AsyncSession\n",
        "        from sqlalchemy.ext.asyncio import create_async_engine\n",
        "        from src.models import Document, Chunk\n",
        "        from sqlmodel import select, func\n",
        "        \n",
        "        database_url = os.getenv(\"DATABASE_URL\") or f\"postgresql+asyncpg://{os.getenv('DATABASE_USER', 'postgres')}:{os.getenv('DATABASE_PASSWORD', 'postgres')}@{os.getenv('DATABASE_HOST', 'localhost')}:{os.getenv('DATABASE_PORT', '5434')}/{os.getenv('DATABASE_NAME', 'smartclm-poc')}\"\n",
        "        async_engine = create_async_engine(database_url, echo=False)\n",
        "        \n",
        "        async with AsyncSession(async_engine) as session:\n",
        "            # ë¬¸ì„œ ìˆ˜ í™•ì¸\n",
        "            doc_count_query = select(func.count(Document.id))\n",
        "            doc_result = await session.exec(doc_count_query)\n",
        "            total_docs = doc_result.one()\n",
        "            \n",
        "            # íƒ€ì…ë³„ ë¬¸ì„œ ìˆ˜\n",
        "            type_query = select(Document.doc_type, func.count(Document.id)).group_by(Document.doc_type)\n",
        "            type_result = await session.exec(type_query)\n",
        "            type_counts = {doc_type: count for doc_type, count in type_result.all()}\n",
        "            \n",
        "            # ì²­í¬ ìˆ˜ í™•ì¸\n",
        "            chunk_count_query = select(func.count(Chunk.id))\n",
        "            chunk_result = await session.exec(chunk_count_query)\n",
        "            total_chunks = chunk_result.one()\n",
        "            \n",
        "            # ì²˜ë¦¬ ìƒíƒœë³„ ë¬¸ì„œ ìˆ˜\n",
        "            status_query = select(Document.processing_status, func.count(Document.id)).group_by(Document.processing_status)\n",
        "            status_result = await session.exec(status_query)\n",
        "            status_counts = {status: count for status, count in status_result.all()}\n",
        "            \n",
        "            print(\"ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ:\")\n",
        "            print(f\"  ì´ ë¬¸ì„œ ìˆ˜: {total_docs}ê°œ\")\n",
        "            print(f\"  ì´ ì²­í¬ ìˆ˜: {total_chunks}ê°œ\")\n",
        "            \n",
        "            print(\"\\nğŸ“‹ ë¬¸ì„œ íƒ€ì…ë³„ ë¶„í¬:\")\n",
        "            for doc_type, count in type_counts.items():\n",
        "                print(f\"  - {doc_type}: {count}ê°œ\")\n",
        "            \n",
        "            print(\"\\nâš™ï¸ ì²˜ë¦¬ ìƒíƒœë³„ ë¶„í¬:\")\n",
        "            for status, count in status_counts.items():\n",
        "                print(f\"  - {status}: {count}ê°œ\")\n",
        "            \n",
        "            # ìµœê·¼ ì¶”ê°€ëœ ë¬¸ì„œë“¤\n",
        "            recent_query = select(Document.filename, Document.doc_type, Document.processing_status).order_by(Document.id.desc()).limit(5)\n",
        "            recent_result = await session.exec(recent_query)\n",
        "            recent_docs = recent_result.all()\n",
        "            \n",
        "            if recent_docs:\n",
        "                print(\"\\nğŸ†• ìµœê·¼ ì¶”ê°€ëœ ë¬¸ì„œ (ìµœëŒ€ 5ê°œ):\")\n",
        "                for i, (filename, doc_type, status) in enumerate(recent_docs, 1):\n",
        "                    print(f\"  {i}. {filename} ({doc_type}) - {status}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)}\")\n",
        "\n",
        "# ì‹¤í–‰\n",
        "print(\"ğŸ” ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸ ì¤‘...\")\n",
        "await check_database_status()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# íŒŒì¼ ì •ë¦¬ ë° ìœ í‹¸ë¦¬í‹°\n",
        "\n",
        "def cleanup_temporary_files():\n",
        "    \"\"\"ì„ì‹œ íŒŒì¼ë“¤ ì •ë¦¬\"\"\"\n",
        "    try:\n",
        "        import shutil\n",
        "        \n",
        "        # ë³€í™˜ëœ PDF íŒŒì¼ë“¤ ì •ë¦¬\n",
        "        converted_pdfs = list(PROCESSED_DIR.glob(\"*_converted.pdf\"))\n",
        "        for pdf_file in converted_pdfs:\n",
        "            pdf_file.unlink()\n",
        "            print(f\"ğŸ—‘ï¸ ì„ì‹œ PDF ì‚­ì œ: {pdf_file.name}\")\n",
        "        \n",
        "        print(f\"âœ… ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ: {len(converted_pdfs)}ê°œ íŒŒì¼ ì‚­ì œ\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {str(e)}\")\n",
        "\n",
        "def show_results_summary():\n",
        "    \"\"\"ê²°ê³¼ íŒŒì¼ë“¤ ìš”ì•½ ì¶œë ¥\"\"\"\n",
        "    print(\"ğŸ“ ê²°ê³¼ íŒŒì¼ ìš”ì•½:\")\n",
        "    \n",
        "    # ì²˜ë¦¬ ìš”ì•½\n",
        "    summary_file = RESULTS_DIR / \"processing_summary.json\"\n",
        "    if summary_file.exists():\n",
        "        print(f\"  âœ… ì²˜ë¦¬ ìš”ì•½: {summary_file.name}\")\n",
        "    \n",
        "    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ê²°ê³¼\n",
        "    search_files = list(RESULTS_DIR.glob(\"search_test_results_*.json\"))\n",
        "    print(f\"  ğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ê²°ê³¼: {len(search_files)}ê°œ íŒŒì¼\")\n",
        "    \n",
        "    # ì²´ì¸ ë¶„ì„ ê²°ê³¼\n",
        "    chain_files = list(RESULTS_DIR.glob(\"chain_analysis_*.json\"))\n",
        "    print(f\"  ğŸ“ ì²´ì¸ ë¶„ì„ ê²°ê³¼: {len(chain_files)}ê°œ íŒŒì¼\")\n",
        "    \n",
        "    # ì²˜ë¦¬ëœ ë¬¸ì„œë“¤\n",
        "    processed_folders = [f for f in PROCESSED_DIR.iterdir() if f.is_dir()]\n",
        "    print(f\"  ğŸ“„ ì²˜ë¦¬ëœ ë¬¸ì„œ íƒ€ì…: {len(processed_folders)}ê°œ í´ë”\")\n",
        "    for folder in processed_folders:\n",
        "        doc_count = len(list(folder.glob(\"*\")))\n",
        "        print(f\"    - {folder.name}: {doc_count}ê°œ íŒŒì¼\")\n",
        "\n",
        "# ì „ì²´ íŒŒì´í”„ë¼ì¸ ìƒíƒœ ìš”ì•½\n",
        "def show_pipeline_status():\n",
        "    \"\"\"ì „ì²´ íŒŒì´í”„ë¼ì¸ ìƒíƒœ ìš”ì•½\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ğŸ¯ Smart CLM RAG íŒŒì´í”„ë¼ì¸ ìƒíƒœ ìš”ì•½\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Step 1 ìƒíƒœ\n",
        "    if hasattr(pipeline, 'results') and pipeline.results:\n",
        "        successful = len([r for r in pipeline.results if r.success])\n",
        "        total = len(pipeline.results)\n",
        "        print(f\"ğŸ“„ Step 1 (ë¬¸ì„œ ì €ì¥): {successful}/{total} ì„±ê³µ\")\n",
        "    else:\n",
        "        print(\"ğŸ“„ Step 1 (ë¬¸ì„œ ì €ì¥): ë¯¸ì‹¤í–‰\")\n",
        "    \n",
        "    # Step 2 ìƒíƒœ\n",
        "    search_files = list(RESULTS_DIR.glob(\"search_test_results_*.json\"))\n",
        "    if search_files:\n",
        "        print(f\"ğŸ” Step 2 (ê²€ìƒ‰ í…ŒìŠ¤íŠ¸): ì™„ë£Œ ({len(search_files)}ê°œ ê²°ê³¼)\")\n",
        "    else:\n",
        "        print(\"ğŸ” Step 2 (ê²€ìƒ‰ í…ŒìŠ¤íŠ¸): ë¯¸ì‹¤í–‰\")\n",
        "    \n",
        "    # Step 3 ìƒíƒœ\n",
        "    chain_files = list(RESULTS_DIR.glob(\"chain_analysis_*.json\"))\n",
        "    if chain_files:\n",
        "        print(f\"ğŸ“ Step 3 (ê³„ì•½ì„œ ê²€í† ): ì™„ë£Œ ({len(chain_files)}ê°œ ê²°ê³¼)\")\n",
        "    else:\n",
        "        print(\"ğŸ“ Step 3 (ê³„ì•½ì„œ ê²€í† ): ë¯¸ì‹¤í–‰\")\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "\n",
        "# ì‹¤í–‰\n",
        "print(\"ğŸ§¹ íŒŒì¼ ì •ë¦¬ ì¤‘...\")\n",
        "cleanup_temporary_files()\n",
        "\n",
        "print(\"\\nğŸ“Š ê²°ê³¼ ìš”ì•½:\")\n",
        "show_results_summary()\n",
        "\n",
        "print(\"\\nğŸ“ˆ íŒŒì´í”„ë¼ì¸ ìƒíƒœ:\")\n",
        "show_pipeline_status()\n",
        "\n",
        "print(f\"\\nğŸ‰ ë…¸íŠ¸ë¶ ì‹¤í–‰ ì™„ë£Œ! ëª¨ë“  ê²°ê³¼ëŠ” {RESULTS_DIR} í´ë”ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
