"""
ğŸ“„ í†µí•© ë¬¸ì„œ ê´€ë¦¬ ë¦¬í¬ì§€í† ë¦¬

ê¸°ì¡´ legalê³¼ contracts ë¦¬í¬ì§€í† ë¦¬ë¥¼ ìƒˆë¡œìš´ Document, Chunk ëª¨ë¸ë¡œ í†µí•©
"""

from typing import Any, Dict, List, Optional

from sqlalchemy import text
from sqlmodel import desc, select
from sqlmodel.ext.asyncio.session import AsyncSession

from src.core.query_utils import (
    create_soft_delete_query,
    create_soft_delete_query_with_conditions,
)
from src.models import Chunk, Document


class DocumentRepository:
    """
    í†µí•© ë¬¸ì„œ ê´€ë¦¬ ë¦¬í¬ì§€í† ë¦¬
    """

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ“š Document CRUD
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    async def create_document(
        self, document: Document, session: AsyncSession
    ) -> Document:
        """ë¬¸ì„œ ìƒì„±"""
        session.add(document)
        await session.commit()
        await session.refresh(document)
        return document

    async def get_document_by_id(
        self, document_id: int, session: AsyncSession, include_deleted: bool = False
    ) -> Optional[Document]:
        """IDë¡œ ë¬¸ì„œ ì¡°íšŒ"""
        query = create_soft_delete_query_with_conditions(
            Document, include_deleted_records=include_deleted, id=document_id
        )
        result = await session.exec(query)
        return result.first()

    async def get_documents_by_room(
        self, room_id: int, session: AsyncSession, include_deleted: bool = False
    ) -> List[Document]:
        """ë°©ë³„ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
        query = create_soft_delete_query_with_conditions(
            Document, include_deleted_records=include_deleted, room_id=room_id
        ).order_by(desc(Document.created_at))
        result = await session.exec(query)
        return list(result.all())

    async def get_document_by_filename_and_room(
        self,
        filename: str,
        room_id: int,
        session: AsyncSession,
        include_deleted: bool = False,
    ) -> Optional[Document]:
        """ê°™ì€ ë°©ì—ì„œ ê°™ì€ íŒŒì¼ëª…ì˜ ìµœì‹  ë¬¸ì„œ ì¡°íšŒ"""
        base_query = create_soft_delete_query(
            Document, include_deleted_records=include_deleted
        )
        query = base_query.where(
            Document.filename == filename, Document.room_id == room_id
        ).order_by(desc(Document.created_at))
        result = await session.exec(query)
        return result.first()

    async def get_latest_contract_in_room(
        self, room_id: int, session: AsyncSession, include_deleted: bool = False
    ) -> Optional[Document]:
        """í•´ë‹¹ ë°©ì—ì„œ ê°€ì¥ ìµœì‹  ê³„ì•½ì„œ ì¡°íšŒ (ë²„ì „ ê´€ë¦¬ìš©)"""
        base_query = create_soft_delete_query(
            Document, include_deleted_records=include_deleted
        )
        query = base_query.where(
            Document.room_id == room_id, Document.doc_type == "contract"
        ).order_by(desc(Document.created_at))
        result = await session.exec(query)
        return result.first()

    async def get_global_documents(
        self,
        doc_type: Optional[str] = None,
        category: Optional[str] = None,
        processing_status: Optional[str] = None,
        session: AsyncSession = None,
        include_deleted: bool = False,
    ) -> List[Document]:
        """ì „ì—­ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ (room_id = NULL)"""
        base_query = create_soft_delete_query(
            Document, include_deleted_records=include_deleted
        )
        query = base_query.where(Document.room_id == None)

        if doc_type:
            query = query.where(Document.doc_type == doc_type)
        if category:
            query = query.where(Document.category == category)
        if processing_status:
            query = query.where(Document.processing_status == processing_status)

        query = query.order_by(desc(Document.created_at))
        result = await session.exec(query)
        documents = list(result.all())
        return documents

    async def get_documents_by_type(
        self, doc_type: str, session: AsyncSession, include_deleted: bool = False
    ) -> List[Document]:
        """ë¬¸ì„œ ìœ í˜•ë³„ ì¡°íšŒ"""
        base_query = create_soft_delete_query(
            Document, include_deleted_records=include_deleted
        )
        query = base_query.where(
            text("document_metadata->>'type' = :doc_type")
        ).order_by(desc(Document.created_at))
        result = await session.exec(query, {"doc_type": doc_type})
        return list(result.all())

    async def update_document(
        self, document: Document, session: AsyncSession
    ) -> Document:
        """ë¬¸ì„œ ì—…ë°ì´íŠ¸"""
        session.add(document)
        await session.commit()
        await session.refresh(document)
        return document

    async def soft_delete_document(
        self, document_id: int, session: AsyncSession
    ) -> bool:
        """ë¬¸ì„œ Soft Delete"""
        document = await self.get_document_by_id(document_id, session)
        if document:
            document.soft_delete()
            await session.commit()
            return True
        return False

    async def restore_document(self, document_id: int, session: AsyncSession) -> bool:
        """ì‚­ì œëœ ë¬¸ì„œ ë³µêµ¬"""
        document = await self.get_document_by_id(
            document_id, session, include_deleted=True
        )
        if document and document.is_deleted:
            document.restore()
            await session.commit()
            return True
        return False

    async def hard_delete_document(
        self, document_id: int, session: AsyncSession
    ) -> bool:
        """ë¬¸ì„œ Hard Delete (ì˜êµ¬ ì‚­ì œ)"""
        document = await self.get_document_by_id(
            document_id, session, include_deleted=True
        )
        if document:
            await session.delete(document)
            await session.commit()
            return True
        return False

    async def get_deleted_documents(
        self, room_id: Optional[int] = None, session: AsyncSession = None
    ) -> List[Document]:
        """ì‚­ì œëœ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
        base_query = create_soft_delete_query(Document, include_deleted_records=True)
        query = base_query.where(Document.deleted_at != None)

        if room_id is not None:
            query = query.where(Document.room_id == room_id)

        query = query.order_by(desc(Document.deleted_at))
        result = await session.exec(query)
        return list(result.all())

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ” Chunk CRUD
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    async def create_chunks(
        self, chunks: List[Chunk], session: AsyncSession
    ) -> List[Chunk]:
        """ì²­í¬ ì¼ê´„ ìƒì„±"""
        session.add_all(chunks)
        await session.commit()
        for chunk in chunks:
            await session.refresh(chunk)
        return chunks

    async def get_chunks_by_document(
        self, document_id: int, session: AsyncSession
    ) -> List[Chunk]:
        """ë¬¸ì„œë³„ ì²­í¬ ì¡°íšŒ"""
        query = (
            select(Chunk)
            .where(Chunk.document_id == document_id)
            .order_by(Chunk.chunk_index)
        )
        result = await session.exec(query)
        return list(result.all())

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ” ë²¡í„° ê²€ìƒ‰
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    async def search_documents_by_vector(
        self,
        query_embedding: List[float],
        session: AsyncSession,
        doc_types: Optional[List[str]] = None,
        top_k: int = 5,
    ) -> List[Dict[str, Any]]:
        """ë²¡í„° ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ (ì‹œìŠ¤í…œ RAGìš©)"""

        # ê¸°ë³¸ ì¿¼ë¦¬ êµ¬ì„± - ì „ì—­ ë¬¸ì„œë§Œ ê²€ìƒ‰
        base_query = """
        SELECT 
            c.id as chunk_id,
            c.content,
            c.chunk_type,
            c.parent_id,
            c.child_id,
            c.header_1,
            c.header_2,
            c.header_3,
            c.header_4,
            d.id as document_id,
            d.filename,
            d.doc_type,
            d.category,
            d.auto_tags,
            (1 - (c.embedding <=> :query_embedding)) as similarity_score
        FROM chunks c
        JOIN documents d ON c.document_id = d.id
        WHERE c.embedding IS NOT NULL
        AND d.room_id IS NULL 
        """

        # ë²¡í„°ë¥¼ pgvector í˜•ì‹ ë¬¸ìì—´ë¡œ ë³€í™˜
        vector_str = "[" + ",".join(map(str, query_embedding)) + "]"
        params: Dict[str, Any] = {"query_embedding": vector_str}

        # ë¬¸ì„œ ìœ í˜• í•„í„°
        if doc_types:
            type_conditions = []
            for i, doc_type in enumerate(doc_types):
                param_name = f"doc_type_{i}"
                type_conditions.append(f"d.doc_type = :{param_name}")
                params[param_name] = doc_type
            base_query += f" AND ({' OR '.join(type_conditions)})"

        # ìœ ì‚¬ë„ ì •ë ¬ ë° ì œí•œ
        base_query += " ORDER BY similarity_score DESC LIMIT :top_k"
        params["top_k"] = top_k

        # ì¿¼ë¦¬ ì‹¤í–‰
        connection = await session.connection()
        result = await connection.execute(text(base_query), params)
        rows = result.fetchall()

        # ê²°ê³¼ ë³€í™˜
        search_results = []
        for row in rows:
            search_results.append(
                {
                    "chunk_id": row.chunk_id,
                    "content": row.content,
                    "similarity_score": round(row.similarity_score, 4),
                    "chunk_type": row.chunk_type,
                    "parent_id": row.parent_id,
                    "child_id": row.child_id,
                    "headers": {
                        "header_1": row.header_1,
                        "header_2": row.header_2,
                        "header_3": row.header_3,
                        "header_4": row.header_4,
                    },
                    "document_id": row.document_id,
                    "filename": row.filename,
                    "doc_type": row.doc_type,
                    "category": row.category,
                    "auto_tags": row.auto_tags or [],
                }
            )

        return search_results

    async def get_parent_chunk_content(
        self, parent_id: str, session: AsyncSession
    ) -> Optional[Dict[str, Any]]:
        """Parent ì²­í¬ ë‚´ìš© ì¡°íšŒ"""
        try:
            parent_query = """
            SELECT content, header_1, header_2, header_3, header_4
            FROM chunks 
            WHERE chunk_type = 'parent' 
            AND parent_id = :parent_id
            LIMIT 1
            """
            connection = await session.connection()
            result = await connection.execute(
                text(parent_query), {"parent_id": parent_id}
            )
            row = result.fetchone()

            if row:
                return {
                    "content": row.content,
                    "headers": {
                        "header_1": row.header_1,
                        "header_2": row.header_2,
                        "header_3": row.header_3,
                        "header_4": row.header_4,
                    },
                }
            return None
        except Exception:
            return None

    async def search_documents_by_vector_advanced(
        self,
        query_embedding: List[float],
        session: AsyncSession,
        doc_types: Optional[List[str]] = None,
        categories: Optional[List[str]] = None,
        tags: Optional[List[str]] = None,
        top_k: int = 10,
    ) -> List[Dict[str, Any]]:
        """ê³ ê¸‰ ë²¡í„° ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ (ì¶”ê°€ í•„í„°ë§)"""

        # ê¸°ë³¸ ì¿¼ë¦¬ êµ¬ì„± - ì „ì—­ ë¬¸ì„œë§Œ ê²€ìƒ‰
        base_query = """
        SELECT 
            c.id as chunk_id,
            c.content,
            c.chunk_type,
            c.parent_id,
            c.child_id,
            c.header_1,
            c.header_2,
            c.header_3,
            c.header_4,
            d.id as document_id,
            d.filename,
            d.doc_type,
            d.category,
            d.auto_tags,
            (1 - (c.embedding <=> :query_embedding)) as similarity_score
        FROM chunks c
        JOIN documents d ON c.document_id = d.id
        WHERE c.embedding IS NOT NULL
        AND d.room_id IS NULL  
        """

        # ë²¡í„°ë¥¼ pgvector í˜•ì‹ ë¬¸ìì—´ë¡œ ë³€í™˜
        vector_str = "[" + ",".join(map(str, query_embedding)) + "]"
        params: Dict[str, Any] = {"query_embedding": vector_str}

        # ë¬¸ì„œ ìœ í˜• í•„í„°
        if doc_types:
            type_conditions = []
            for i, doc_type in enumerate(doc_types):
                param_name = f"doc_type_{i}"
                type_conditions.append(f"d.doc_type = :{param_name}")
                params[param_name] = doc_type
            base_query += f" AND ({' OR '.join(type_conditions)})"

        # ì¹´í…Œê³ ë¦¬ í•„í„°
        if categories:
            category_conditions = []
            for i, category in enumerate(categories):
                param_name = f"category_{i}"
                category_conditions.append(f"d.category = :{param_name}")
                params[param_name] = category
            base_query += f" AND ({' OR '.join(category_conditions)})"

        # íƒœê·¸ í•„í„°
        if tags:
            tag_conditions = []
            for i, tag in enumerate(tags):
                param_name = f"tag_{i}"
                tag_conditions.append(f":{param_name} = ANY(d.auto_tags)")
                params[param_name] = tag
            base_query += f" AND ({' OR '.join(tag_conditions)})"

        # ìœ ì‚¬ë„ ì •ë ¬ ë° ì œí•œ
        base_query += " ORDER BY similarity_score DESC LIMIT :top_k"
        params["top_k"] = top_k

        # ì¿¼ë¦¬ ì‹¤í–‰
        connection = await session.connection()
        result = await connection.execute(text(base_query), params)
        rows = result.fetchall()

        # ê²°ê³¼ ë³€í™˜
        search_results = []
        for row in rows:
            search_results.append(
                {
                    "chunk_id": row.chunk_id,
                    "content": row.content,
                    "similarity_score": round(row.similarity_score, 4),
                    "chunk_type": row.chunk_type,
                    "parent_id": row.parent_id,
                    "child_id": row.child_id,
                    "headers": {
                        "header_1": row.header_1,
                        "header_2": row.header_2,
                        "header_3": row.header_3,
                        "header_4": row.header_4,
                    },
                    "document_id": row.document_id,
                    "filename": row.filename,
                    "doc_type": row.doc_type,
                    "category": row.category,
                    "auto_tags": row.auto_tags or [],
                }
            )

        return search_results

    async def search_documents_by_vector_with_room_context(
        self,
        query_embedding: List[float],
        session: AsyncSession,
        room_id: Optional[int] = None,
        doc_types: Optional[List[str]] = None,
        top_k: int = 10,
    ) -> List[Dict[str, Any]]:
        """ë°©ë³„ ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ë²¡í„° ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ (í•˜ì´ë¸Œë¦¬ë“œ RAGìš©)"""

        # ë°©ë³„ ë¬¸ì„œì™€ ì‹œìŠ¤í…œ ë¬¸ì„œë¥¼ ëª¨ë‘ ê²€ìƒ‰
        base_query = """
        SELECT 
            c.id as chunk_id,
            c.content,
            c.chunk_type,
            c.parent_id,
            c.child_id,
            c.header_1,
            c.header_2,
            c.header_3,
            c.header_4,
            d.id as document_id,
            d.filename,
            d.doc_type,
            d.category,
            d.auto_tags,
            d.room_id,
            (1 - (c.embedding <=> :query_embedding)) as similarity_score
        FROM chunks c
        JOIN documents d ON c.document_id = d.id
        WHERE c.embedding IS NOT NULL
        """

        # ë²¡í„°ë¥¼ pgvector í˜•ì‹ ë¬¸ìì—´ë¡œ ë³€í™˜
        vector_str = "[" + ",".join(map(str, query_embedding)) + "]"
        params: Dict[str, Any] = {"query_embedding": vector_str}

        # ë°©ë³„ í•„í„°ë§
        if room_id is not None:
            # íŠ¹ì • ë°©ì˜ ë¬¸ì„œ + ì‹œìŠ¤í…œ ë¬¸ì„œ
            base_query += " AND (d.room_id = :room_id OR d.room_id IS NULL)"
            params["room_id"] = room_id
        else:
            # ì‹œìŠ¤í…œ ë¬¸ì„œë§Œ
            base_query += " AND d.room_id IS NULL"

        # ë¬¸ì„œ ìœ í˜• í•„í„°
        if doc_types:
            type_conditions = []
            for i, doc_type in enumerate(doc_types):
                param_name = f"doc_type_{i}"
                type_conditions.append(f"d.doc_type = :{param_name}")
                params[param_name] = doc_type
            base_query += f" AND ({' OR '.join(type_conditions)})"

        # ìœ ì‚¬ë„ ì •ë ¬ ë° ì œí•œ
        base_query += " ORDER BY similarity_score DESC LIMIT :top_k"
        params["top_k"] = top_k

        # ì¿¼ë¦¬ ì‹¤í–‰
        connection = await session.connection()
        result = await connection.execute(text(base_query), params)
        rows = result.fetchall()

        # ê²°ê³¼ ë³€í™˜
        search_results = []
        for row in rows:
            search_results.append(
                {
                    "chunk_id": row.chunk_id,
                    "content": row.content,
                    "similarity_score": round(row.similarity_score, 4),
                    "chunk_type": row.chunk_type,
                    "parent_id": row.parent_id,
                    "child_id": row.child_id,
                    "headers": {
                        "header_1": row.header_1,
                        "header_2": row.header_2,
                        "header_3": row.header_3,
                        "header_4": row.header_4,
                    },
                    "document_id": row.document_id,
                    "filename": row.filename,
                    "doc_type": row.doc_type,
                    "category": row.category,
                    "auto_tags": row.auto_tags or [],
                    "room_id": row.room_id,
                    "is_room_document": row.room_id is not None,
                }
            )

        return search_results

    async def search_documents_hybrid(
        self,
        query_embedding: List[float],
        session: AsyncSession,
        room_id: Optional[int] = None,
        doc_types: Optional[List[str]] = None,
        top_k: int = 15,
        system_weight: float = 0.6,
        room_weight: float = 0.4,
    ) -> List[Dict[str, Any]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ì‹œìŠ¤í…œ ë¬¸ì„œ + ë°©ë³„ ë¬¸ì„œ ê°€ì¤‘ì¹˜ ì ìš©)"""

        try:
            # 1. ì‹œìŠ¤í…œ ë¬¸ì„œ ê²€ìƒ‰ (ì „ì—­ ë¬¸ì„œ)
            system_results = await self.search_documents_by_vector(
                query_embedding=query_embedding,
                session=session,
                doc_types=doc_types,
                top_k=top_k,
            )

            # 2. ë°©ë³„ ë¬¸ì„œ ê²€ìƒ‰ (í•´ë‹¹ ë°©ì˜ ë¬¸ì„œ)
            room_results = []
            if room_id is not None:
                room_results = await self.search_documents_by_vector_with_room_context(
                    query_embedding=query_embedding,
                    session=session,
                    room_id=room_id,
                    doc_types=doc_types,
                    top_k=top_k,
                )
                # ë°©ë³„ ë¬¸ì„œë§Œ í•„í„°ë§
                room_results = [
                    r for r in room_results if r.get("is_room_document", False)
                ]

            # 3. ê°€ì¤‘ì¹˜ ì ìš© ë° ê²°ê³¼ ë³‘í•©
            combined_results = []

            # ì‹œìŠ¤í…œ ë¬¸ì„œì— ê°€ì¤‘ì¹˜ ì ìš©
            for result in system_results:
                result["weighted_score"] = result["similarity_score"] * system_weight
                result["source_type"] = "system"
                combined_results.append(result)

            # ë°©ë³„ ë¬¸ì„œì— ê°€ì¤‘ì¹˜ ì ìš©
            for result in room_results:
                result["weighted_score"] = result["similarity_score"] * room_weight
                result["source_type"] = "room"
                combined_results.append(result)

            # 4. ê°€ì¤‘ì¹˜ ì ìˆ˜ë¡œ ì •ë ¬
            combined_results.sort(key=lambda x: x["weighted_score"], reverse=True)

            # 5. ìƒìœ„ ê²°ê³¼ ë°˜í™˜
            return combined_results[:top_k]

        except Exception as e:
            print(f"âš ï¸ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            # ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±
            return await self.search_documents_by_vector(
                query_embedding=query_embedding,
                session=session,
                doc_types=doc_types,
                top_k=top_k,
            )
