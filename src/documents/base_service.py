"""
ğŸ“„ ê³µí†µ ë¬¸ì„œ ì—…ë¡œë“œ ì„œë¹„ìŠ¤

ì–´ë“œë¯¼ê³¼ ì‚¬ìš©ì ë¬¸ì„œ ì—…ë¡œë“œì˜ ê³µí†µ ë¡œì§ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
"""

import os
import time
import traceback
import uuid
from datetime import datetime
from typing import Dict, Optional

from fastapi import HTTPException, UploadFile
from sqlmodel.ext.asyncio.session import AsyncSession

from src.aws.bedrock_service import BedrockService
from src.aws.dependency import get_s3_service
from src.aws.embedding_service import TitanEmbeddingService
from src.documents.chunking import HierarchicalChunker
from src.documents.repository import DocumentRepository
from src.external_services.doc_converter.client import doc_converter_client
from src.external_services.doc_parser.client import doc_parser_client
from src.models import Chunk, Document


class BaseDocumentService:
    """ê³µí†µ ë¬¸ì„œ ì—…ë¡œë“œ ì„œë¹„ìŠ¤"""

    def __init__(self):
        self.embedding_service = TitanEmbeddingService()
        self.chunker = HierarchicalChunker()
        self.bedrock_service = BedrockService()
        self.s3_service = get_s3_service()
        self.repository = DocumentRepository()

    async def _analyze_document_comprehensive(
        self,
        file: UploadFile,
        analysis_result: Dict,
        doc_type: str,
        session: AsyncSession = None,
        user_id: Optional[int] = None,
        is_admin: bool = False,
    ) -> Dict:
        """í†µí•© ë¬¸ì„œ ë¶„ì„ (ë¬¸ì„œ ìœ í˜• ë¶„ë¥˜ + ìƒì„¸ ë¶„ì„ + ê³„ì•½ì„œ ê²€í† )"""
        filename = file.filename or ""

        # doc_parser ë¶„ì„ ê²°ê³¼ì—ì„œ í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ
        markdown_content = analysis_result.get("markdown_content", "")
        file_size = file.size or 0
        page_count = analysis_result.get("page_count", 0)

        # ì‚¬ìš©ì ì—…ë¡œë“œ ì‹œì—ë§Œ í¬ê¸° ì œí•œ ì²´í¬
        if not is_admin:
            if page_count > 80:
                raise HTTPException(
                    status_code=400,
                    detail=f"ë¬¸ì„œê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. í˜„ì¬ {page_count}í˜ì´ì§€, ìµœëŒ€ 80í˜ì´ì§€ê¹Œì§€ ì§€ì›í•©ë‹ˆë‹¤. (Claude 200K í† í° ì œí•œ ê¸°ì¤€)",
                )
            if file_size > 50 * 1024 * 1024:  # 50MB
                raise HTTPException(
                    status_code=400,
                    detail=f"íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤. í˜„ì¬ {file_size / (1024 * 1024):.1f}MB, ìµœëŒ€ 50MBê¹Œì§€ ì§€ì›í•©ë‹ˆë‹¤.",
                )

        print(
            f"ğŸ“„ ë¬¸ì„œ í¬ê¸°: {file_size / (1024 * 1024):.2f}MB, í˜ì´ì§€: {page_count}ì¥"
        )

        # ì „ì²´ ë¬¸ì„œ ë‚´ìš© ì‚¬ìš© (ì œí•œ ì—†ìŒ)
        content_for_analysis = markdown_content if markdown_content else ""
        print(f"âœ… ì „ì²´ ë¬¸ì„œ ì‚¬ìš©: {len(content_for_analysis)}ì")

        # í†µí•© ë¶„ì„ í”„ë¡¬í”„íŠ¸ íŒŒì¼ ì½ê¸°
        prompt_path = "src/documents/prompts/comprehensive_analysis_prompt.txt"
        with open(prompt_path, "r", encoding="utf-8") as f:
            prompt_template = f.read()

        # RAG ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
        rag_context = (
            "ê´€ë ¨ ì°¸ê³ ìë£Œ ì—†ìŒ" if is_admin else "ê´€ë ¨ ì°¸ê³ ìë£Œ ì—†ìŒ"
        )  # TODO: ì‹¤ì œ RAG êµ¬í˜„ ì‹œ ìˆ˜ì •

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ê°’ ì¹˜í™˜
        prompt = prompt_template.format(
            filename=filename,
            content_for_analysis=content_for_analysis,
            rag_context=rag_context,
        )

        # Bedrock LLM í˜¸ì¶œ
        print(f"ğŸ¤– í†µí•© ë¬¸ì„œ ë¶„ì„ ì¤‘: {filename} ({doc_type})")
        llm_response = self.bedrock_service.invoke_model(
            prompt=prompt, max_tokens=4000, temperature=0.1
        )

        # ì‹¤ì œ í† í° ì‚¬ìš©ëŸ‰ ë° ì‹œê°„ ì •ë³´ ì¶”ì¶œ
        response_text = llm_response["text"]
        token_usage = llm_response["usage"]
        timing_info = llm_response.get("timing", {})

        print(
            f"ğŸ“Š ì‹¤ì œ í† í° ì‚¬ìš©ëŸ‰ - ì…ë ¥: {token_usage['input_tokens']:,}, ì¶œë ¥: {token_usage['output_tokens']:,}, ì´í•©: {token_usage['total_tokens']:,}"
        )
        if timing_info.get("processing_time_ms"):
            print(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {timing_info['processing_time_ms']}ms")

        # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì  (ì„¸ì…˜ê³¼ user_idê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ)
        if session and user_id:
            try:
                from src.monitoring import TokenTracker

                # ë©”íƒ€ë°ì´í„° êµ¬ì„±
                usage_metadata = {
                    "filename": file.filename,
                    "doc_type": doc_type,
                    "content_length": len(content_for_analysis),
                    "response_length": len(response_text),
                    "is_admin": is_admin,
                }

                operation_type = (
                    "admin_document_analysis" if is_admin else "user_document_analysis"
                )

                await TokenTracker.record_bedrock_usage(
                    session=session,
                    user_id=user_id,
                    operation_type=operation_type,
                    model_name="claude-3-sonnet-20240229-v1:0",
                    usage=token_usage,
                    usage_metadata=usage_metadata,
                    request_started_at=timing_info.get("request_started_at"),
                    response_received_at=timing_info.get("response_received_at"),
                    processing_time_ms=timing_info.get("processing_time_ms"),
                )
            except Exception as e:
                print(f"âš ï¸ í† í° ì‚¬ìš©ëŸ‰ ì¶”ì  ì‹¤íŒ¨: {str(e)}")
                # í† í° ì¶”ì  ì‹¤íŒ¨ëŠ” ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¤‘ë‹¨í•˜ì§€ ì•ŠìŒ

        # JSON ì‘ë‹µ íŒŒì‹±
        import json
        import re

        try:
            # ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            response_text = response_text.strip()

            # ```json ... ``` ë¸”ë¡ ì°¾ê¸°
            json_match = re.search(r"```json\s*(.*?)\s*```", response_text, re.DOTALL)
            if json_match:
                json_content = json_match.group(1).strip()
            else:
                # ```json ë¸”ë¡ì´ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ JSON ê°ì²´ ì°¾ê¸°
                json_match = re.search(r"\{.*\}", response_text, re.DOTALL)
                if json_match:
                    json_content = json_match.group(0)
                else:
                    # ë§ˆì§€ë§‰ ì‹œë„: ì²« ë²ˆì§¸ {ë¶€í„° ë§ˆì§€ë§‰ }ê¹Œì§€
                    start = response_text.find("{")
                    end = response_text.rfind("}") + 1
                    if start != -1 and end != 0:
                        json_content = response_text[start:end]
                    else:
                        raise json.JSONDecodeError(
                            "JSON ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", response_text, 0
                        )

            comprehensive_result = json.loads(json_content)

            # ì–´ë“œë¯¼ì´ ì§€ì •í•œ ë¬¸ì„œ ìœ í˜•ê³¼ LLM ë¶„ë¥˜ ê²°ê³¼ ë¹„êµ
            llm_doc_type = comprehensive_result.get("doc_type", "contract")
            if doc_type != llm_doc_type:
                print(f"âš ï¸ ê²½ê³ : ì§€ì •({doc_type})ê³¼ LLM ë¶„ë¥˜({llm_doc_type})ê°€ ë‹¤ë¦…ë‹ˆë‹¤")
                # ì§€ì • ìœ í˜•ì„ ìš°ì„  ì‚¬ìš©
                comprehensive_result["doc_type"] = doc_type

            # í† í° ì‚¬ìš©ëŸ‰ ì¶”ê°€
            comprehensive_result["token_usage"] = token_usage

            print(
                f"âœ… í†µí•© ë¶„ì„ ì™„ë£Œ: {comprehensive_result.get('title', 'ì œëª© ì—†ìŒ')}"
            )
            return comprehensive_result

        except json.JSONDecodeError as e:
            print(f"âŒ ë¬¸ì„œ ë¶„ì„ JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            print(f"LLM ì‘ë‹µ: {llm_response}")
            raise HTTPException(
                status_code=500, detail="ë¬¸ì„œ ìƒì„¸ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: JSON íŒŒì‹± ì˜¤ë¥˜"
            )
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            raise HTTPException(
                status_code=500, detail=f"ë¬¸ì„œ ìƒì„¸ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}"
            )

    async def _process_document_chunks(
        self, document: Document, analysis_result: Dict, session: AsyncSession
    ):
        """ë¬¸ì„œ ì²­í‚¹ ë° ì„ë² ë”© ì²˜ë¦¬"""
        try:
            print(f"âœ‚ï¸ ë¬¸ì„œ ì²­í‚¹ ì¤‘: {document.filename}")

            # 1. ë§ˆí¬ë‹¤ìš´ ì²­í‚¹
            markdown_content = analysis_result.get("markdown_content", "")
            chunking_result = self.chunker.chunk_markdown(
                markdown_content=markdown_content,
                filename=document.filename,
            )

            # 2. ë²¡í„°ìš© ì²­í¬ ìƒì„±
            vector_ready_chunks = self.chunker.create_vector_ready_chunks(
                chunking_result
            )

            # 3. ì„ë² ë”© ìƒì„±
            embedded_chunks = await self.embedding_service.embed_chunked_documents(
                vector_ready_chunks
            )

            # 4. ì²­í¬ ì €ì¥
            chunk_objects = []
            for i, chunk_data in enumerate(embedded_chunks):
                if chunk_data.get("content", "").strip():
                    chunk = Chunk(
                        document_id=document.id or 0,
                        content=chunk_data.get("content", ""),
                        chunk_index=i,
                        header_1=chunk_data.get("headers", {}).get("header_1"),
                        header_2=chunk_data.get("headers", {}).get("header_2"),
                        header_3=chunk_data.get("headers", {}).get("header_3"),
                        header_4=chunk_data.get("headers", {}).get("header_4"),
                        parent_id=chunk_data.get("parent_id"),
                        child_id=chunk_data.get("child_id"),
                        chunk_type=chunk_data.get("chunk_type", "child"),
                        embedding=chunk_data.get("embedding"),
                        word_count=len(chunk_data.get("content", "").split()),
                        char_count=len(chunk_data.get("content", "")),
                        chunk_metadata=chunk_data.get("metadata", {}),
                        auto_tags=chunk_data.get("auto_tags", []),
                    )
                    chunk_objects.append(chunk)

            await self.repository.create_chunks(chunk_objects, session)
            print(f"âœ… ì²­í¬ ì €ì¥ ì™„ë£Œ: {len(chunk_objects)}ê°œ")

        except Exception as e:
            print(f"âŒ ì²­í‚¹ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            # ì²­í‚¹ ì‹¤íŒ¨í•´ë„ ë¬¸ì„œëŠ” ì €ì¥ë¨

    async def _upload_document_common(
        self,
        file: UploadFile,
        doc_type: str,
        room_id: Optional[int],
        user_id: int,
        session: AsyncSession,
        is_admin: bool = False,
    ) -> Dict:
        """ê³µí†µ ë¬¸ì„œ ì—…ë¡œë“œ ë¡œì§"""
        start_time = time.time()

        try:
            # 1. íŒŒì¼ í˜•ì‹ í™•ì¸ ë° PDF ë³€í™˜
            print(f"ğŸ“„ íŒŒì¼ í˜•ì‹ í™•ì¸ ì¤‘: {file.filename}")
            print(f"ğŸ“„ ì›ë³¸ íŒŒì¼ í¬ê¸°: {file.size} bytes")
            file_extension = (
                file.filename.lower().split(".")[-1] if file.filename else ""
            )

            # PDFê°€ ì•„ë‹Œ ê²½ìš° ë³€í™˜
            if file_extension != "pdf":
                print(f"ğŸ”„ PDF ë³€í™˜ ì¤‘: {file.filename}")
                pdf_filename = file.filename.replace(f".{file_extension}", ".pdf")
                pdf_tmp_path = None
                pdf_bytes = None

                try:
                    # doc_converter_clientë¥¼ ì‚¬ìš©í•˜ì—¬ PDF ë³€í™˜
                    pdf_bytes = await doc_converter_client.convert_to_pdf(file)

                    # ë³€í™˜ëœ PDF ë°”ì´íŠ¸ ë°ì´í„° ê²€ì¦
                    if not pdf_bytes or len(pdf_bytes) == 0:
                        raise Exception("ë³€í™˜ëœ PDFê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

                    print(f"âœ… PDF ë³€í™˜ ì™„ë£Œ: {pdf_filename} ({len(pdf_bytes)} bytes)")

                    # ì„ì‹œ íŒŒì¼ë¡œ PDF ì €ì¥
                    import tempfile

                    with tempfile.NamedTemporaryFile(
                        delete=False, suffix=".pdf"
                    ) as pdf_tmp:
                        pdf_tmp.write(pdf_bytes)
                        pdf_tmp_path = pdf_tmp.name

                    # ë³€í™˜ëœ PDFë¥¼ UploadFileë¡œ ìƒì„± (BytesIO ì‚¬ìš©)
                    from io import BytesIO

                    from fastapi import UploadFile as FastAPIUploadFile

                    pdf_file_io = BytesIO(pdf_bytes)
                    pdf_file_io.seek(0)  # [ì¶”ê°€] S3 ì—…ë¡œë“œ ì „ í¬ì¸í„° ì´ˆê¸°í™”
                    converted_pdf_file = FastAPIUploadFile(
                        filename=pdf_filename,
                        file=pdf_file_io,
                    )
                    converted_pdf_file.size = len(pdf_bytes)

                except Exception as e:
                    print(f"âŒ PDF ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
                    raise HTTPException(
                        status_code=400, detail=f"PDF ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}"
                    )
            else:
                converted_pdf_file = file
                pdf_filename = file.filename
                pdf_tmp_path = None
                pdf_bytes = None

            # 2. PDF ë¶„ì„ (doc_parser_client ì‚¬ìš©)
            print(f"ğŸ“„ PDF ë¶„ì„ ì¤‘: {pdf_filename}")
            print(f"ğŸ“„ ë³€í™˜ëœ PDF íŒŒì¼ í¬ê¸°: {converted_pdf_file.size} bytes")
            analysis_result = await doc_parser_client.analyze_pdf(converted_pdf_file)

            if not analysis_result["success"]:
                raise HTTPException(status_code=400, detail="PDF ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")

            # 3. LLM ìƒì„¸ ë¶„ì„
            print(f"ğŸ¤– LLM ë¶„ì„ ì‹œì‘: {file.filename}")
            doc_analysis = await self._analyze_document_comprehensive(
                file, analysis_result, doc_type, session, user_id, is_admin
            )

            title = doc_analysis.get("title", file.filename or "ì œëª© ì—†ìŒ")
            category = doc_analysis.get("category")
            description = doc_analysis.get("description")
            confidence = doc_analysis.get("confidence", 0.0)
            review_analysis = doc_analysis.get("review_analysis")

            # 4. ë²„ì „ ê´€ë¦¬ (ê³„ì•½ì„œì¸ ê²½ìš°ì—ë§Œ)
            new_version = ""
            if doc_type == "contract" and not is_admin and room_id is not None:
                # í•´ë‹¹ ë°©ì—ì„œ ê°€ì¥ ìµœì‹  ê³„ì•½ì„œ ì¡°íšŒ
                existing_doc = await self.repository.get_latest_contract_in_room(
                    room_id=room_id,
                    session=session,
                )

                if existing_doc:
                    new_version = self._increment_version(existing_doc.version or "v1")
                    print(
                        f"ğŸ“ ê¸°ì¡´ ê³„ì•½ì„œ ë°œê²¬: {existing_doc.filename} (ë²„ì „ {existing_doc.version} â†’ {new_version})"
                    )
                else:
                    new_version = "v1"
                    print(f"ğŸ“„ ìƒˆë¡œìš´ ê³„ì•½ì„œ: {title} (ë²„ì „ {new_version})")
            else:
                print(f"ğŸ“š ì°¸ê³  ë¬¸ì„œ ì—…ë¡œë“œ: {file.filename} (ë²„ì „ ê´€ë¦¬ ì—†ìŒ)")

            # 5. S3 ê²½ë¡œ ì„¤ì •
            s3_prefix = "system" if is_admin else "contract"
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            unique_id = str(uuid.uuid4())[:8]
            safe_filename = file.filename or "unknown.pdf"

            # 6. S3ì— ì›ë³¸ íŒŒì¼ ì—…ë¡œë“œ
            print(f"ğŸ“¤ S3 ì›ë³¸ íŒŒì¼ ì—…ë¡œë“œ ì¤‘: {file.filename}")
            original_s3_key = f"{s3_prefix}/original/{doc_type}/{timestamp}_{unique_id}_{safe_filename}"

            # ì›ë³¸ íŒŒì¼ í¬ì¸í„° ë¦¬ì…‹
            await file.seek(0)
            original_s3_result = await self.s3_service.upload_file(
                file=file, s3_key=original_s3_key
            )
            print(
                f"âœ… ì›ë³¸ íŒŒì¼ S3 ì—…ë¡œë“œ ì™„ë£Œ: {original_s3_result['file_size']} bytes"
            )

            # 7. S3ì— ë³€í™˜ëœ PDF ì—…ë¡œë“œ
            print(f"ğŸ“¤ S3 PDF íŒŒì¼ ì—…ë¡œë“œ ì¤‘: {pdf_filename}")
            pdf_s3_key = (
                f"{s3_prefix}/pdf/{doc_type}/{timestamp}_{unique_id}_{pdf_filename}"
            )

            # ë³€í™˜ëœ PDF íŒŒì¼ì˜ ê²½ìš° ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ ì§ì ‘ ì „ë‹¬
            if pdf_bytes is not None:
                # ë³€í™˜ëœ PDF ë°”ì´íŠ¸ ë°ì´í„° ì‚¬ìš©
                from io import BytesIO

                from fastapi import UploadFile as FastAPIUploadFile

                pdf_file_obj = FastAPIUploadFile(
                    filename=pdf_filename,
                    file=BytesIO(pdf_bytes),
                )
                pdf_file_obj.size = len(pdf_bytes)
                pdf_file_obj.file.seek(0)  # [ì¶”ê°€] S3 ì—…ë¡œë“œ ì „ í¬ì¸í„° ì´ˆê¸°í™”

                print(
                    f"ğŸ“¤ ë³€í™˜ëœ PDF S3 ì—…ë¡œë“œ: {pdf_filename} ({len(pdf_bytes)} bytes)"
                )
                pdf_s3_result = await self.s3_service.upload_file(
                    file=pdf_file_obj, s3_key=pdf_s3_key, content_type="application/pdf"
                )
                print(
                    f"âœ… ë³€í™˜ëœ PDF S3 ì—…ë¡œë“œ ì™„ë£Œ: {pdf_s3_result['file_size']} bytes"
                )
            else:
                # ì´ë¯¸ PDFì¸ ê²½ìš°
                await converted_pdf_file.seek(0)
                print(f"ğŸ“¤ ì›ë³¸ PDF S3 ì—…ë¡œë“œ: {pdf_filename}")
                pdf_s3_result = await self.s3_service.upload_file(
                    file=converted_pdf_file,
                    s3_key=pdf_s3_key,
                    content_type="application/pdf",
                )
                print(f"âœ… ì›ë³¸ PDF S3 ì—…ë¡œë“œ ì™„ë£Œ: {pdf_s3_result['file_size']} bytes")

            # 8. ë¬¸ì„œ ë©”íƒ€ë°ì´í„° êµ¬ì„±
            metadata = {
                "description": description,
                "analysis_info": {
                    "processing_time": time.time() - start_time,
                    "analysis_method": "comprehensive",
                    "confidence": confidence,
                },
                "token_usage": doc_analysis.get("token_usage", {}),
            }

            # ì–´ë“œë¯¼ ì—…ë¡œë“œ í‘œì‹œ
            if is_admin:
                metadata["uploaded_by_admin"] = True

            # ê³„ì•½ì„œì¸ ê²½ìš° ê²€í†  ë¶„ì„ ê²°ê³¼ ì¶”ê°€
            if doc_type == "contract" and review_analysis:
                metadata["review_analysis"] = review_analysis

            # 9. auto_tags ì¶”ì¶œ
            auto_tags = doc_analysis.get("auto_tags", [])
            if not auto_tags:
                # auto_tagsê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ íƒœê·¸ ìƒì„±
                auto_tags = [doc_type, category] if category else [doc_type]
                auto_tags = [tag for tag in auto_tags if tag]  # ë¹ˆ ê°’ ì œê±°

            # 10. Document ëª¨ë¸ ìƒì„±
            document = Document(
                room_id=room_id,  # None = ì „ì—­ ë¬¸ì„œ, ìˆ«ì = ë°©ë³„ ë¬¸ì„œ
                doc_type=doc_type,
                category=category,
                processing_status="completed",
                filename=file.filename or "unknown.pdf",
                version=new_version,
                s3_bucket=original_s3_result["bucket"],
                s3_key=original_s3_result["key"],
                pdf_s3_bucket=pdf_s3_result["bucket"],
                pdf_s3_key=pdf_s3_result["key"],
                file_size=file.size,
                pdf_file_size=converted_pdf_file.size,
                page_count=analysis_result.get("page_count"),
                document_metadata=metadata,
                auto_tags=auto_tags,
                html_content=analysis_result.get("html_content"),
                markdown_content=analysis_result.get("markdown_content"),
            )

            # 11. ë¬¸ì„œ ì €ì¥
            created_document = await self.repository.create_document(document, session)

            # 12. ì²­í‚¹ ë° ì„ë² ë”©
            await self._process_document_chunks(
                created_document, analysis_result, session
            )

            # 13. ë°© ì •ë³´ ì—…ë°ì´íŠ¸ (ê³„ì•½ì„œ v1ì¸ ê²½ìš°ì—ë§Œ)
            if doc_type == "contract" and room_id is not None and new_version == "v1":
                await self._update_room_info(room_id, title, description, session)

            # 14. ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if pdf_tmp_path and os.path.exists(pdf_tmp_path):
                try:
                    os.remove(pdf_tmp_path)
                    print(f"ğŸ§¹ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ: {pdf_tmp_path}")
                except Exception as e:
                    print(f"âš ï¸ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {str(e)}")

            return {
                "document": created_document,
                "title": title,
                "version": new_version,
                "review_analysis": review_analysis,
            }

        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            traceback.print_exc()
            raise HTTPException(
                status_code=500, detail=f"ë¬¸ì„œ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            )

    def _increment_version(self, current_version: str) -> str:
        """ë²„ì „ ë²ˆí˜¸ ì¦ê°€ (v1 â†’ v2 â†’ v3...)"""
        if not current_version or not current_version.startswith("v"):
            return "v1"

        try:
            version_num = int(current_version[1:])
            return f"v{version_num + 1}"
        except ValueError:
            return "v1"

    async def _update_room_info(
        self,
        room_id: int,
        title: str,
        description: Optional[str],
        session: AsyncSession,
    ):
        """ë°© ì •ë³´ ì—…ë°ì´íŠ¸ (ì œëª©ê³¼ ì„¤ëª…)"""
        try:
            from src.rooms.repository import room_repository

            # ë°© ì¡°íšŒ
            room = await room_repository.get_room_by_id(room_id, session)
            if not room:
                print(f"âš ï¸ ë°©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: room_id={room_id}")
                return

            # ë°© ì œëª©ì´ "Untitled"ì¸ ê²½ìš°ì—ë§Œ ì—…ë°ì´íŠ¸
            if room.name == "Untitled" and title:
                room.name = title
                print(f"ğŸ  ë°© ì œëª© ì—…ë°ì´íŠ¸: {title}")

            # ë°© ì„¤ëª…ì´ ì—†ëŠ” ê²½ìš°ì—ë§Œ ì—…ë°ì´íŠ¸
            if not room.description and description:
                room.description = description
                print(f"ğŸ  ë°© ì„¤ëª… ì—…ë°ì´íŠ¸: {description}")

            # ë³€ê²½ì‚¬í•­ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì €ì¥
            if room.name != "Untitled" or room.description:
                await room_repository.update_room(room, session)
                print(f"âœ… ë°© ì •ë³´ ì—…ë°ì´íŠ¸ ì™„ë£Œ: room_id={room_id}")

        except Exception as e:
            print(f"âš ï¸ ë°© ì •ë³´ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
            # ë°© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ëŠ” ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¤‘ë‹¨í•˜ì§€ ì•ŠìŒ
