from typing import Any, Dict, List, Optional

from langchain_text_splitters import (
    MarkdownHeaderTextSplitter,
    RecursiveCharacterTextSplitter,
)


class HierarchicalChunker:
    """
    Doclingì—ì„œ ì¶”ì¶œí•œ ë§ˆí¬ë‹¤ìš´ì„ ê¸°ë°˜ìœ¼ë¡œ ê³„ì¸µì  ì²­í‚¹ì„ ìˆ˜í–‰í•˜ëŠ” í´ë˜ìŠ¤
    """

    def __init__(self):
        # í—¤ë” ì •ì˜ (í—¤ë” ë ˆë²¨ê³¼ ë©”íƒ€ë°ì´í„° í‚¤ ì´ë¦„)
        self.headers_to_split_on = [
            ("#", "Header 1"),
            ("##", "Header 2"),
            ("###", "Header 3"),
            ("####", "Header 4"),
        ]

        # Markdown í—¤ë” ê¸°ì¤€ ë¶„í• ê¸°
        self.markdown_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=self.headers_to_split_on
        )

        # Child ë¬¸ì„œìš© í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì •
        self.chunk_size = 500
        self.chunk_overlap = 50
        self._create_child_splitter()

    def _create_child_splitter(self):
        """Child splitterë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        self.child_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            length_function=len,
        )

    def update_chunk_settings(self, chunk_size: int, chunk_overlap: int):
        """ì²­í‚¹ ì„¤ì •ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self._create_child_splitter()
        print(f"ğŸ“ ì²­í‚¹ ì„¤ì • ì—…ë°ì´íŠ¸: í¬ê¸°={chunk_size}, ì˜¤ë²„ë©={chunk_overlap}")

    def chunk_markdown(
        self, markdown_content: str, filename: str = "document"
    ) -> Dict[str, Any]:
        """
        ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸ ë¥¼ ê³„ì¸µì ìœ¼ë¡œ ì²­í‚¹í•©ë‹ˆë‹¤.
        í•œêµ­ì–´ ì¡°í•­ íŒ¨í„´(ì œXì¡°)ë„ ì¸ì‹í•˜ì—¬ ì›ë³¸ ë²ˆí˜¸ë¥¼ ë³´ì¡´í•©ë‹ˆë‹¤.

        Args:
            markdown_content: ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ë¬¸ì„œ ë‚´ìš©
            filename: íŒŒì¼ëª… (ë©”íƒ€ë°ì´í„°ìš©)

        Returns:
            ì²­í‚¹ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print(f"ğŸ“„ ë§ˆí¬ë‹¤ìš´ ê³„ì¸µì  ì²­í‚¹ ì‹œì‘: {filename}")

        try:
            import re
            
            # 1ë‹¨ê³„: í•œêµ­ì–´ ì¡°í•­ íŒ¨í„´ ì „ì²˜ë¦¬ - ì œXì¡°ë¥¼ ë§ˆí¬ë‹¤ìš´ í—¤ë”ë¡œ ë³€í™˜
            lines = markdown_content.split('\n')
            preprocessed_lines = []
            
            for line in lines:
                # ì œXì¡° íŒ¨í„´ì„ ## í—¤ë”ë¡œ ë³€í™˜í•˜ì—¬ MarkdownHeaderTextSplitterê°€ ì¸ì‹í•˜ë„ë¡ í•¨
                if re.match(r'^ì œ\s*\d+\s*ì¡°', line.strip()):
                    # "ì œ8ì¡° (ê³„ì•½ê¸°ê°„)" ë˜ëŠ” "ì œ8ì¡°" â†’ "## ì œ8ì¡° (ê³„ì•½ê¸°ê°„)"
                    preprocessed_lines.append(f"## {line.strip()}")
                    print(f"ğŸ” ì¡°í•­ íŒ¨í„´ ë°œê²¬: {line.strip()} â†’ ## {line.strip()}")
                else:
                    preprocessed_lines.append(line)
            
            preprocessed_markdown = '\n'.join(preprocessed_lines)
            
            # 2ë‹¨ê³„: ì „ì²˜ë¦¬ëœ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ í—¤ë” ê¸°ì¤€ ë¶„í•  (Parent chunks)
            parent_chunks = self.markdown_splitter.split_text(preprocessed_markdown)

            print(f"ğŸ“Š í—¤ë” ë¶„í•  ê²°ê³¼: {len(parent_chunks)}ê°œ ì„¹ì…˜")
            print(
                f"ğŸ“ ì„¹ì…˜ë³„ í¬ê¸°: {[len(chunk.page_content) for chunk in parent_chunks]}"
            )

            # 3ë‹¨ê³„: Parent chunksì—ì„œ ì›ë³¸ ì¡°í•­ ë²ˆí˜¸ ë³µì›
            for idx, parent_chunk in enumerate(parent_chunks):
                # Header 2ì—ì„œ "ì œXì¡°" íŒ¨í„´ì´ ìˆë‹¤ë©´ ì›ë³¸ í˜•íƒœë¡œ ë³µì›
                header_2 = parent_chunk.metadata.get("Header 2", "")
                if header_2 and header_2.startswith("ì œ") and "ì¡°" in header_2:
                    # "ì œ8ì¡° (ê³„ì•½ê¸°ê°„)" í˜•íƒœ ê·¸ëŒ€ë¡œ ìœ ì§€
                    parent_chunk.metadata["Header 1"] = header_2
                    print(f"ğŸ”„ ì¡°í•­ ë²ˆí˜¸ ë³µì›: {header_2}")
                
                preview = parent_chunk.page_content[:200].replace("\n", "\\n")

            # 4ë‹¨ê³„: ê° parent chunkë¥¼ child chunksë¡œ ì„¸ë¶„í™”
            all_child_chunks = []
            parent_child_mapping = {}

            for parent_idx, parent_chunk in enumerate(parent_chunks):
                parent_id = f"{filename}_parent_{parent_idx}"

                # Parent chunk ë©”íƒ€ë°ì´í„° ë³´ê°•
                parent_chunk.metadata.update(
                    {
                        "source": filename,
                        "chunk_type": "parent",
                        "parent_id": parent_id,
                        "parent_index": parent_idx,
                        "content_length": len(parent_chunk.page_content),
                    }
                )

                # Parent chunkë¥¼ child chunksë¡œ ë¶„í• 
                if len(parent_chunk.page_content) > 500:  # í° ì„¹ì…˜ë§Œ ì¶”ê°€ ë¶„í• 
                    child_docs = self.child_splitter.split_documents([parent_chunk])

                    # Child chunks ë©”íƒ€ë°ì´í„° ì„¤ì •
                    for child_idx, child_doc in enumerate(child_docs):
                        child_id = f"{parent_id}_child_{child_idx}"
                        child_doc.metadata.update(
                            {
                                "source": filename,
                                "chunk_type": "child",
                                "parent_id": parent_id,
                                "child_id": child_id,
                                "child_index": child_idx,
                                "content_length": len(child_doc.page_content),
                            }
                        )
                        all_child_chunks.append(child_doc)

                    parent_child_mapping[parent_id] = [
                        doc.metadata["child_id"] for doc in child_docs
                    ]
                    print(f"  ğŸ“‚ ì„¹ì…˜ {parent_idx}: {len(child_docs)}ê°œ í•˜ìœ„ ì²­í¬ ìƒì„±")

                    # Child chunks ìƒì„¸ ì •ë³´ ë¡œê·¸
                    for child_idx, child_doc in enumerate(child_docs):
                        preview = child_doc.page_content[:100].replace("\n", "\\n")
                else:
                    # ì‘ì€ ì„¹ì…˜(500ì ì´í•˜)ì€ Parentë§Œ ì €ì¥í•˜ê³  ChildëŠ” ìƒì„±í•˜ì§€ ì•ŠìŒ
                    # Parent chunk ë©”íƒ€ë°ì´í„°ì— ë‹¨ë… ì €ì¥ í‘œì‹œ
                    parent_chunk.metadata.update(
                        {
                            "is_standalone": True,  # Parentë§Œ ë‹¨ë… ì €ì¥ë˜ëŠ” ì²­í¬ì„ì„ í‘œì‹œ
                            "skip_child_creation": True,  # Child ìƒì„± ìŠ¤í‚µ í”Œë˜ê·¸
                        }
                    )
                    parent_child_mapping[parent_id] = []  # Child ì—†ìŒ

            # ê²°ê³¼ ì •ë¦¬
            result = {
                "success": True,
                "filename": filename,
                "parent_chunks": parent_chunks,
                "child_chunks": all_child_chunks,
                "parent_child_mapping": parent_child_mapping,
                "summary": {
                    "total_parent_chunks": len(parent_chunks),
                    "total_child_chunks": len(all_child_chunks),
                    "average_parent_size": sum(
                        len(chunk.page_content) for chunk in parent_chunks
                    )
                    // len(parent_chunks)
                    if parent_chunks
                    else 0,
                    "average_child_size": sum(
                        len(chunk.page_content) for chunk in all_child_chunks
                    )
                    // len(all_child_chunks)
                    if all_child_chunks
                    else 0,
                },
            }

            print("\n" + "=" * 80)
            print("âœ… ê³„ì¸µì  ì²­í‚¹ ì™„ë£Œ - ìµœì¢… ê²°ê³¼ ìš”ì•½")
            print("=" * 80)
            print(f"ğŸ“Š Parent chunks: {result['summary']['total_parent_chunks']}ê°œ")
            print(f"ğŸ“Š Child chunks: {result['summary']['total_child_chunks']}ê°œ")
            print(f"ğŸ“ í‰ê·  Parent í¬ê¸°: {result['summary']['average_parent_size']}ì")
            print(f"ğŸ“ í‰ê·  Child í¬ê¸°: {result['summary']['average_child_size']}ì")

            return result

        except Exception as e:
            print(f"âŒ ì²­í‚¹ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return {"success": False, "error": str(e), "filename": filename}

    def create_vector_ready_chunks(
        self, chunking_result: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        ë²¡í„° DB ì €ì¥ìš© ì²­í¬ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        Parent chunk (ì „ì²´ ë§¥ë½ìš©)ì™€ Child chunk (ê²€ìƒ‰ìš©) ëª¨ë‘ í¬í•¨í•©ë‹ˆë‹¤.

        Args:
            chunking_result: chunk_markdown ê²°ê³¼

        Returns:
            ë²¡í„° DB ì €ì¥ìš© ì²­í¬ ë¦¬ìŠ¤íŠ¸ (Parent + Child)
        """
        if not chunking_result.get("success"):
            return []

        vector_ready_chunks = []

        # 1. Parent chunks ì¶”ê°€ (ì „ì²´ ë§¥ë½ ë³´ì¡´ìš©)
        print("ğŸ“‚ Parent chunksë¥¼ DB ì €ì¥ìš©ìœ¼ë¡œ ë³€í™˜ ì¤‘...")
        for parent_chunk in chunking_result["parent_chunks"]:
            parent_id = parent_chunk.metadata.get("parent_id")
            is_standalone = parent_chunk.metadata.get("is_standalone", False)

            parent_data = {
                "content": parent_chunk.page_content,
                "metadata": parent_chunk.metadata,
                "chunk_id": parent_id,  # Parent ê³ ìœ  ID
                "parent_id": parent_id,  # ìì‹ ì˜ ID (ë” ì§ê´€ì )
                "child_id": parent_id,  # DB ì €ì¥ì‹œ child_id í•„ë“œì— ìì‹ ì˜ ID ì €ì¥
                "chunk_type": "parent",  # DB êµ¬ë¶„ìš©
                "headers": {
                    "header_1": parent_chunk.metadata.get("Header 1", ""),
                    "header_2": parent_chunk.metadata.get("Header 2", ""),
                    "header_3": parent_chunk.metadata.get("Header 3", ""),
                    "header_4": parent_chunk.metadata.get("Header 4", ""),
                },
                "source_file": parent_chunk.metadata.get("source", ""),
                "content_length": len(parent_chunk.page_content),
                "is_standalone": is_standalone,  # ë‹¨ë… ì €ì¥ ì—¬ë¶€ í‘œì‹œ
            }

            # ì‘ì€ ì„¹ì…˜(ë‹¨ë… ì €ì¥)ì˜ ê²½ìš° ì„ë² ë”©ë„ í¬í•¨ (ê²€ìƒ‰ ê°€ëŠ¥í•˜ë„ë¡)
            if is_standalone:
                parent_data.update(
                    {
                        "embedding": None,  # ì„ë² ë”©ì€ ë‚˜ì¤‘ì— ìƒì„±
                        "embedding_success": False,
                        "is_for_embedding": True,  # ë‹¨ë… ì €ì¥ë˜ëŠ” ParentëŠ” ì„ë² ë”© ëŒ€ìƒ
                    }
                )
            else:
                # í° ì„¹ì…˜ì˜ ParentëŠ” ì„ë² ë”© ì—†ìŒ (Childê°€ ê²€ìƒ‰ ë‹´ë‹¹)
                parent_data.update(
                    {
                        "embedding": None,
                        "embedding_success": False,
                        "is_for_embedding": False,  # ì„ë² ë”© ëŒ€ìƒ ì•„ë‹˜
                    }
                )

            # print(
            #     f"   ğŸ“‚ Parent ë²¡í„° ë³€í™˜: parent_id={parent_id} (ìì‹ ), child_id={parent_id}, standalone={is_standalone}"
            # )
            vector_ready_chunks.append(parent_data)

        print(
            f"   âœ… {len(chunking_result['parent_chunks'])}ê°œ Parent chunks ì¤€ë¹„ ì™„ë£Œ"
        )

        # 2. Child chunks ì¶”ê°€ (ê²€ìƒ‰ìš© - ì„ë² ë”© ëŒ€ìƒ)
        print("ğŸ“„ Child chunksë¥¼ ë²¡í„° DB ì €ì¥ìš©ìœ¼ë¡œ ë³€í™˜ ì¤‘...")
        for child_chunk in chunking_result["child_chunks"]:
            parent_id = child_chunk.metadata.get("parent_id")
            child_id = child_chunk.metadata.get("child_id")

            child_data = {
                "content": child_chunk.page_content,
                "metadata": child_chunk.metadata,
                "chunk_id": child_id,
                "parent_id": parent_id,
                "child_id": child_id,  # Child ìì‹ ì˜ IDë„ ì €ì¥
                "chunk_type": "child",  # DB êµ¬ë¶„ìš©
                "headers": {
                    "header_1": child_chunk.metadata.get("Header 1", ""),
                    "header_2": child_chunk.metadata.get("Header 2", ""),
                    "header_3": child_chunk.metadata.get("Header 3", ""),
                    "header_4": child_chunk.metadata.get("Header 4", ""),
                },
                "source_file": child_chunk.metadata.get("source", ""),
                "content_length": len(child_chunk.page_content),
                # ChildëŠ” ì„ë² ë”© ëŒ€ìƒ
                "is_for_embedding": True,  # ì„ë² ë”© ëŒ€ìƒì„ì„ ëª…ì‹œ
            }

            # print(f"   ğŸ”— ë²¡í„° ë³€í™˜: parent_id={parent_id}, child_id={child_id}")
            vector_ready_chunks.append(child_data)

        print(f"   âœ… {len(chunking_result['child_chunks'])}ê°œ Child chunks ì¤€ë¹„ ì™„ë£Œ")

        total_parent = len(chunking_result["parent_chunks"])
        total_child = len(chunking_result["child_chunks"])

        print(f"ğŸ”„ ë²¡í„° DB ì €ì¥ìš© ì²­í¬ ì´ {len(vector_ready_chunks)}ê°œ ìƒì„±")
        print(f"   ğŸ“‚ Parent chunks: {total_parent}ê°œ (ì „ì²´ ë§¥ë½ ë³´ì¡´ìš©)")
        print(f"   ğŸ“„ Child chunks: {total_child}ê°œ (ë²¡í„° ê²€ìƒ‰ìš©)")

        return vector_ready_chunks

    def get_parent_context(
        self, chunking_result: Dict[str, Any], child_chunk_id: str
    ) -> Optional[str]:
        """
        Child chunk IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•´ë‹¹í•˜ëŠ” Parent chunkì˜ ì „ì²´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        Args:
            chunking_result: chunk_markdown ê²°ê³¼
            child_chunk_id: Child chunk ID

        Returns:
            Parent chunkì˜ ì „ì²´ ë‚´ìš©
        """
        if not chunking_result.get("success"):
            return None

        # Child chunkì—ì„œ parent_id ì°¾ê¸°
        parent_id = None
        for child_chunk in chunking_result["child_chunks"]:
            if child_chunk.metadata.get("child_id") == child_chunk_id:
                parent_id = child_chunk.metadata.get("parent_id")
                break

        if not parent_id:
            return None

        # Parent chunk ì°¾ê¸°
        for parent_chunk in chunking_result["parent_chunks"]:
            if parent_chunk.metadata.get("parent_id") == parent_id:
                return parent_chunk.page_content

        return None

    def integrate_image_ocr(self, markdown_content: str, images_data: list) -> str:
        """ì´ë¯¸ì§€ OCR í…ìŠ¤íŠ¸ë¥¼ ë§ˆí¬ë‹¤ìš´ì— í†µí•©"""
        enhanced_markdown = markdown_content

        for image_info in images_data:
            if image_info.get("ocr_text") and image_info.get("is_info_rich"):
                ocr_text = image_info["ocr_text"]
                enhanced_markdown = enhanced_markdown.replace(
                    "<!-- image -->",
                    f"\n### ì´ë¯¸ì§€ {image_info['image_index']} ë‚´ìš©\n{ocr_text}\n",
                    1,  # ì²« ë²ˆì§¸ ë§¤ì¹­ë§Œ êµì²´
                )

        return enhanced_markdown


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
hierarchical_chunker = HierarchicalChunker()
