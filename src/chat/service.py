"""
ğŸ¤– ì±„íŒ… ì„œë¹„ìŠ¤

AI ì±—ë´‡ ë° ëŒ€í™” ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.
"""

import os
from typing import List, Optional

from sqlmodel.ext.asyncio.session import AsyncSession

from src.aws.bedrock_service import BedrockService
from src.aws.embedding_service import TitanEmbeddingService
from src.documents.repository import DocumentRepository
from src.documents.service import document_service

from .schema import (
    StreamingChatRequest,
    StreamingChatResponse,
    StreamingContractDraftImprovementRequest,
    StreamingContractLegalAnalysisRequest,
    StreamingContractResponse,
)


class ChatService:
    """ì±„íŒ… ì„œë¹„ìŠ¤"""

    def __init__(self):
        self.embedding_service = TitanEmbeddingService()
        self.bedrock_service = BedrockService()
        self.repository = DocumentRepository()

    async def streaming_chat(
        self, request: StreamingChatRequest, user_id: int, session: AsyncSession
    ):
        """ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… (ë¬¸ì„œ ì „ì²´ ê¸°ë°˜)"""
        try:
            if request.document_id is None:
                raise ValueError("document_idëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.")
            # 1. í”„ë¡¬í”„íŠ¸ txt íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
            prompt_path = os.path.join(
                os.path.dirname(__file__), "prompts", "chat_stream_prompt.txt"
            )
            with open(prompt_path, "r", encoding="utf-8") as f:
                prompt_template = f.read()

            # 2. ë¬¸ì„œ ì „ì²´(Parent ì²­í¬) context ìƒì„±
            chunks = await document_service.repository.get_chunks_by_document(
                request.document_id, session
            )
            parent_chunks = [
                c for c in chunks if getattr(c, "chunk_type", None) == "parent"
            ]
            if not parent_chunks:
                raise ValueError("í•´ë‹¹ ë¬¸ì„œì— ì¡°í•­(Parent ì²­í¬)ì´ ì—†ìŠµë‹ˆë‹¤.")
            context_text = "\n\n".join(
                f"[{c.header_1}]\n{c.content}" for c in parent_chunks
            )

            # 3. í”„ë¡¬í”„íŠ¸ì™€ ì»¨í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
            full_prompt = prompt_template.replace("{context}", context_text)

            # 4. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
            is_first_chunk = True
            async for chunk in self.bedrock_service.stream_model(
                prompt=full_prompt,
                max_tokens=request.max_tokens or 2000,
                temperature=0.0,
            ):
                yield StreamingChatResponse(
                    chunk=chunk["text"],
                    is_complete=chunk.get("is_complete", False),
                    token_usage=chunk.get("usage"),
                    sources=None,
                )
                is_first_chunk = False

        except Exception as e:
            print(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì‹¤íŒ¨: {str(e)}")
            yield StreamingChatResponse(
                chunk="ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                is_complete=True,
                token_usage=None,
                sources=None,
            )

    async def streaming_contract_draft_improvement(
        self,
        request: StreamingContractDraftImprovementRequest,
        user_id: int,
        session: AsyncSession,
    ):
        """ê³„ì•½ì„œ ì´ˆì•ˆ ì‘ì„± ë° ì¡°í•­ ê°œì„  ìŠ¤íŠ¸ë¦¬ë°"""
        try:
            if request.document_id is None:
                raise ValueError("document_idëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.")
            if request.section_id is None:
                raise ValueError("section_idëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.")
            # 1. í”„ë¡¬í”„íŠ¸ txt íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
            prompt_path = os.path.join(
                os.path.dirname(__file__),
                "prompts",
                "contract_draft_improvement_prompt.txt",
            )
            with open(prompt_path, "r", encoding="utf-8") as f:
                prompt_template = f.read()

            # 2. ì¡°í•­+ì•ë’¤ ë§¥ë½ context ìƒì„±
            section_context = await document_service.get_section_with_neighbors_context(
                document_id=request.document_id,
                section_id=request.section_id,
                session=session,
                neighbor_count=1,  # ì•ë’¤ 1ê°œì”© í¬í•¨(ì¡°ì • ê°€ëŠ¥)
            )
            context_text = section_context["context_text"]

            # 3. í”„ë¡¬í”„íŠ¸ì™€ ì»¨í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
            full_prompt = prompt_template.replace("{context}", context_text)

            # 4. LLM ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
            is_first_chunk = True
            async for chunk in self.bedrock_service.stream_model(
                prompt=full_prompt,
                max_tokens=request.max_tokens or 2000,
                temperature=0.0,
            ):
                yield StreamingContractResponse(
                    chunk=chunk["text"],
                    is_complete=chunk.get("is_complete", False),
                    token_usage=chunk.get("usage"),
                    sources=None,
                )
                is_first_chunk = False
        except Exception as e:
            print(f"âŒ ê³„ì•½ì„œ ì´ˆì•ˆ ì‘ì„±/ê°œì„  ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: {str(e)}")
            yield StreamingContractResponse(
                chunk="ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                is_complete=True,
                token_usage=None,
                sources=None,
            )

    async def streaming_contract_legal_analysis(
        self,
        request: StreamingContractLegalAnalysisRequest,
        user_id: int,
        session: AsyncSession,
    ):
        """ê³„ì•½ì„œ ì¡°í•­ ë¶„ì„ ë° ë²•ì  ìœ„í—˜ ë¶„ì„ ìŠ¤íŠ¸ë¦¬ë°"""
        try:
            if request.document_id is None:
                raise ValueError("document_idëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.")
            if request.section_id is None:
                raise ValueError("section_idëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.")
            # 1. í”„ë¡¬í”„íŠ¸ txt íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
            prompt_path = os.path.join(
                os.path.dirname(__file__),
                "prompts",
                "contract_legal_analysis_prompt.txt",
            )
            with open(prompt_path, "r", encoding="utf-8") as f:
                prompt_template = f.read()

            # 2. ì¡°í•­+ì•ë’¤ ë§¥ë½ context ìƒì„±
            section_context = await document_service.get_section_with_neighbors_context(
                document_id=request.document_id,
                section_id=request.section_id,
                session=session,
                neighbor_count=1,  # ì•ë’¤ 1ê°œì”© í¬í•¨(ì¡°ì • ê°€ëŠ¥)
            )
            context_text = section_context["context_text"]

            # 3. í”„ë¡¬í”„íŠ¸ì™€ ì»¨í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
            full_prompt = prompt_template.replace("{context}", context_text)

            # 4. LLM ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
            is_first_chunk = True
            async for chunk in self.bedrock_service.stream_model(
                prompt=full_prompt,
                max_tokens=request.max_tokens or 2000,
                temperature=0.0,
            ):
                yield StreamingContractResponse(
                    chunk=chunk["text"],
                    is_complete=chunk.get("is_complete", False),
                    token_usage=chunk.get("usage"),
                    sources=None,
                )
                is_first_chunk = False
        except Exception as e:
            print(f"âŒ ê³„ì•½ì„œ ì¡°í•­ ë¶„ì„/ë²•ì  ìœ„í—˜ ë¶„ì„ ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: {str(e)}")
            yield StreamingContractResponse(
                chunk="ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                is_complete=True,
                token_usage=None,
                sources=None,
            )

    async def _search_relevant_documents(
        self,
        query: str,
        room_id: Optional[int],
        doc_types: Optional[List[str]],
        db_session: AsyncSession,
    ) -> List[dict]:
        """ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (ì¼ë°˜ ì±„íŒ…ìš©)"""
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = await self.embedding_service.create_single_embedding(
                query
            )

            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë°©ë³„ ë¬¸ì„œ + ì‹œìŠ¤í…œ ë¬¸ì„œ)
            search_results = await self.repository.search_documents_hybrid(
                query_embedding=query_embedding,
                session=db_session,
                room_id=room_id,
                doc_types=doc_types,
                top_k=8,
                system_weight=0.6,
                room_weight=0.4,
            )

            # ê²°ê³¼ ë³€í™˜
            relevant_docs = []
            for result in search_results:
                relevant_docs.append(
                    {
                        "document_id": result["document_id"],
                        "title": result["filename"],
                        "content": result["content"],
                        "similarity_score": result["similarity_score"],
                        "doc_type": result["doc_type"],
                        "source_type": result.get("source_type", "system"),
                    }
                )

            return relevant_docs

        except Exception as e:
            print(f"âš ï¸ ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []

    async def _search_relevant_documents_for_draft_improvement(
        self,
        query: str,
        document_id: Optional[int],
        room_id: Optional[int],
        db_session: AsyncSession,
    ) -> List[dict]:
        """ê³„ì•½ì„œ ì´ˆì•ˆ ì‘ì„± ë° ì¡°í•­ ê°œì„ ìš© ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"""
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = await self.embedding_service.create_single_embedding(
                query
            )

            # ì´ˆì•ˆ ì‘ì„± ê´€ë ¨ ë¬¸ì„œ ìœ í˜•
            draft_doc_types = ["standard_contract", "guideline", "law"]

            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë°©ë³„ ë¬¸ì„œ + ì‹œìŠ¤í…œ ë¬¸ì„œ)
            search_results = await self.repository.search_documents_hybrid(
                query_embedding=query_embedding,
                session=db_session,
                room_id=room_id,
                doc_types=draft_doc_types,
                top_k=10,
                system_weight=0.7,
                room_weight=0.3,
            )

            # ê²°ê³¼ ë³€í™˜
            relevant_docs = []
            for result in search_results:
                relevant_docs.append(
                    {
                        "document_id": result["document_id"],
                        "title": result["filename"],
                        "content": result["content"],
                        "similarity_score": result["similarity_score"],
                        "doc_type": result["doc_type"],
                        "source_type": result.get("source_type", "system"),
                    }
                )

            return relevant_docs

        except Exception as e:
            print(f"âš ï¸ ì´ˆì•ˆ ì‘ì„± ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []

    async def _search_relevant_documents_for_legal_analysis(
        self,
        query: str,
        document_id: Optional[int],
        room_id: Optional[int],
        db_session: AsyncSession,
    ) -> List[dict]:
        """ê³„ì•½ì„œ ë²•ì  ë¶„ì„ìš© ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"""
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = await self.embedding_service.create_single_embedding(
                query
            )

            # ë²•ì  ë¶„ì„ ê´€ë ¨ ë¬¸ì„œ ìœ í˜•
            legal_doc_types = ["law", "precedent", "guideline"]

            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë°©ë³„ ë¬¸ì„œ + ì‹œìŠ¤í…œ ë¬¸ì„œ)
            search_results = await self.repository.search_documents_hybrid(
                query_embedding=query_embedding,
                session=db_session,
                room_id=room_id,
                doc_types=legal_doc_types,
                top_k=10,
                system_weight=0.8,
                room_weight=0.2,
            )

            # ê²°ê³¼ ë³€í™˜
            relevant_docs = []
            for result in search_results:
                relevant_docs.append(
                    {
                        "document_id": result["document_id"],
                        "title": result["filename"],
                        "content": result["content"],
                        "similarity_score": result["similarity_score"],
                        "doc_type": result["doc_type"],
                        "source_type": result.get("source_type", "system"),
                    }
                )

            return relevant_docs

        except Exception as e:
            print(f"âš ï¸ ë²•ì  ë¶„ì„ ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []

    def _build_chat_context(
        self,
        question: str,
        relevant_docs: List[dict],
    ) -> str:
        """ì±„íŒ… ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ì¼ë°˜)"""
        context_parts = []

        # ê´€ë ¨ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸
        if relevant_docs:
            docs_context = "ì°¸ê³  ë¬¸ì„œ:\n"
            for i, doc in enumerate(relevant_docs, 1):
                source_type = doc.get("source_type", "system")
                source_label = "ë°© ë¬¸ì„œ" if source_type == "room" else "ì‹œìŠ¤í…œ ë¬¸ì„œ"
                docs_context += f"{i}. {doc['title']} ({doc['doc_type']}, {source_label}): {doc['content'][:500]}...\n"
            context_parts.append(docs_context)

        # í˜„ì¬ ì§ˆë¬¸
        context_parts.append(f"ì‚¬ìš©ì ì§ˆë¬¸: {question}")

        return "\n\n".join(context_parts)

    def _build_draft_improvement_context(
        self,
        question: str,
        context_text: Optional[str],
        document_id: Optional[int],
        room_id: Optional[int],
        relevant_docs: List[dict],
    ) -> str:
        """ê³„ì•½ì„œ ì´ˆì•ˆ ì‘ì„± ë° ì¡°í•­ ê°œì„  ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        context_parts = []

        # ì‘ì„±/ê°œì„ í•˜ë ¤ëŠ” ì¡°í•­
        if context_text:
            context_parts.append(f"[ì‘ì„±/ê°œì„ í•˜ë ¤ëŠ” ì¡°í•­]\n{context_text}")

        # ë¬¸ì„œ ë° ë°© ì •ë³´
        if document_id:
            context_parts.append(f"[í‘œì¤€ê³„ì•½ì„œ ì›ë³¸ ë¬¸ì„œ ID: {document_id}]")
        if room_id:
            context_parts.append(f"[ë°© ID: {room_id}]")

        # ì°¸ê³  ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸
        if relevant_docs:
            docs_context = "ì°¸ê³  ë¬¸ì„œ (í‘œì¤€ê³„ì•½ì„œ, ê°€ì´ë“œë¼ì¸, ë²•ë ¹):\n"
            for i, doc in enumerate(relevant_docs, 1):
                source_type = doc.get("source_type", "system")
                source_label = "ë°© ë¬¸ì„œ" if source_type == "room" else "ì‹œìŠ¤í…œ ë¬¸ì„œ"
                docs_context += f"{i}. {doc['title']} ({doc['doc_type']}, {source_label}): {doc['content'][:500]}...\n"
            context_parts.append(docs_context)

        # ì§ˆë¬¸/ìš”ì²­
        context_parts.append(f"[ì§ˆë¬¸/ìš”ì²­]\n{question}")

        return "\n\n".join(context_parts)

    def _build_legal_analysis_context(
        self,
        question: str,
        context_text: Optional[str],
        document_id: Optional[int],
        room_id: Optional[int],
        relevant_docs: List[dict],
    ) -> str:
        """ê³„ì•½ì„œ ë²•ì  ë¶„ì„ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        context_parts = []

        # ë¶„ì„í•  ì¡°í•­/ë¶€ë¶„
        if context_text:
            context_parts.append(f"[ë¶„ì„í•  ì¡°í•­/ë¶€ë¶„]\n{context_text}")

        # ë¶„ì„í•  ê³„ì•½ì„œ ì •ë³´
        if document_id:
            context_parts.append(f"[ë¶„ì„í•  ê³„ì•½ì„œ ë¬¸ì„œ ID: {document_id}]")
        if room_id:
            context_parts.append(f"[ë°© ID: {room_id}]")

        # ì°¸ê³  ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸
        if relevant_docs:
            docs_context = "ì°¸ê³  ë¬¸ì„œ (ë²•ë ¹, íŒë¡€, ê°€ì´ë“œë¼ì¸):\n"
            for i, doc in enumerate(relevant_docs, 1):
                source_type = doc.get("source_type", "system")
                source_label = "ë°© ë¬¸ì„œ" if source_type == "room" else "ì‹œìŠ¤í…œ ë¬¸ì„œ"
                docs_context += f"{i}. {doc['title']} ({doc['doc_type']}, {source_label}): {doc['content'][:500]}...\n"
            context_parts.append(docs_context)

        # ì§ˆë¬¸/ìš”ì²­
        context_parts.append(f"[ì§ˆë¬¸/ìš”ì²­]\n{question}")

        return "\n\n".join(context_parts)


# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
chat_service = ChatService()
