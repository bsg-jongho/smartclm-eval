#!/usr/bin/env python3
"""
Smart CLM RAG íŒŒì´í”„ë¼ì¸
ê·¼ê±°ìë£Œ ì €ì¥ ë° ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ í†µí•© ìŠ¤í¬ë¦½íŠ¸

ì‹¤í–‰ ë°©ë²•:
python rag_pipeline.py --step=1  # ê·¼ê±°ìë£Œ ì €ì¥
python rag_pipeline.py --step=2  # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
python rag_pipeline.py --step=3  # ê³„ì•½ì„œ ê²€í†  í…ŒìŠ¤íŠ¸
python rag_pipeline.py --step=4  # í‰ê°€
python rag_pipeline.py --step=5  # Streamlit ê²°ê³¼ ì‹œê°í™”
"""

import asyncio
import argparse
import json
import logging
import os
import sys
import time
from datetime import datetime
from pathlib import Path

# í™˜ê²½ë³€ìˆ˜ ë¡œë”©
from dotenv import load_dotenv
load_dotenv()
from typing import Dict, List, Any, Optional
import httpx
from dataclasses import dataclass
import pandas as pd

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler("rag_pipeline.log")
    ]
)
logger = logging.getLogger(__name__)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent
DOCS_ROOT = PROJECT_ROOT / "docs"
RESULTS_DIR = PROJECT_ROOT / "results"
PROCESSED_DIR = PROJECT_ROOT / "processed"

# ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
RESULTS_DIR.mkdir(exist_ok=True)
PROCESSED_DIR.mkdir(exist_ok=True)

# ë¬¸ì„œ íƒ€ì… ë§¤í•‘ (í´ë” ê²½ë¡œ -> ë¬¸ì„œ íƒ€ì…)
DOCUMENT_TYPE_MAPPING = {
    "ê·¼ê±° ìë£Œ/ë²•ë ¹": "law",  # ë²•ë ¹ íŒŒì¼ë“¤
    "ê·¼ê±°ìë£Œ/ë²•ë ¹": "law",  # ë²•ë ¹ íŒŒì¼ë“¤ (ê³µë°± ì—†ëŠ” ë²„ì „)
    "ê·¼ê±° ìë£Œ/ì²´ê²°ê³„ì•½": "executed_contract",  # ì²´ê²°ëœ ê³„ì•½ì„œë“¤
    "ê·¼ê±°ìë£Œ/ì²´ê²°ê³„ì•½": "executed_contract",  # ì²´ê²°ëœ ê³„ì•½ì„œë“¤ (ê³µë°± ì—†ëŠ” ë²„ì „)
    "ê·¼ê±° ìë£Œ/í‘œì¤€ê³„ì•½ì„œ": "standard_contract",  # í‘œì¤€ ê³„ì•½ì„œ í…œí”Œë¦¿ë“¤
    "ê·¼ê±°ìë£Œ/í‘œì¤€ê³„ì•½ì„œ": "standard_contract",  # í‘œì¤€ ê³„ì•½ì„œ í…œí”Œë¦¿ë“¤ (ê³µë°± ì—†ëŠ” ë²„ì „)
    "1. NDA": "standard_contract",  # NDA í‘œì¤€ê³„ì•½ì„œ (ê¸°ì¡´ í´ë”)
    "2. ì œí’ˆ(ì„œë¹„ìŠ¤) íŒë§¤": "standard_contract",  # íŒë§¤ê³„ì•½ì„œ í‘œì¤€ (ê¸°ì¡´ í´ë”)
}


@dataclass
class ProcessingResult:
    """ë¬¸ì„œ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    filename: str
    doc_type: str
    folder_path: str
    success: bool
    processing_time: float
    page_count: Optional[int] = None
    chunk_count: Optional[int] = None
    error_message: Optional[str] = None
    doc_parser_result: Optional[Dict] = None


class ExternalServiceClient:
    """ì™¸ë¶€ ì„œë¹„ìŠ¤ í´ë¼ì´ì–¸íŠ¸ í†µí•© í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.doc_converter_url = os.getenv("DOC_CONVERTER_URL", "http://localhost:8001")
        self.doc_parser_url = os.getenv("DOC_PARSER_URL", "http://localhost:8002")
        self.api_url = os.getenv("API_URL", "http://localhost:8000")
        self.timeout = 600.0
    
    async def health_check_all(self) -> Dict[str, bool]:
        """ëª¨ë“  ì„œë¹„ìŠ¤ì˜ í—¬ìŠ¤ì²´í¬"""
        services = {
            "doc-converter": f"{self.doc_converter_url}/health",
            "doc-parser": f"{self.doc_parser_url}/health",
            "api": f"{self.api_url}/health"
        }
        
        results = {}
        async with httpx.AsyncClient(timeout=10.0) as client:
            for service, url in services.items():
                try:
                    response = await client.get(url)
                    results[service] = response.status_code == 200
                    logger.info(f"âœ… {service} ì„œë¹„ìŠ¤ ì •ìƒ")
                except Exception as e:
                    results[service] = False
                    logger.error(f"âŒ {service} ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {e}")
        
        return results
    
    def determine_doc_type(self, file_path: Path) -> str:
        """íŒŒì¼ ê²½ë¡œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œ íƒ€ì… ê²°ì •"""
        # ìƒëŒ€ ê²½ë¡œ ê³„ì‚° (docs í´ë” ê¸°ì¤€)
        relative_path = file_path.relative_to(DOCS_ROOT)
        folder_path = str(relative_path.parent)
        
        # í´ë” ê²½ë¡œ ë§¤í•‘ì—ì„œ ë¬¸ì„œ íƒ€ì… ì°¾ê¸°
        for pattern, doc_type in DOCUMENT_TYPE_MAPPING.items():
            if folder_path.startswith(pattern) or folder_path == pattern:
                return doc_type
        
        # ê¸°ë³¸ê°’: law (ê·¼ê±°ìë£Œ)
        return "law"
    
    async def search_documents_direct(self, query: str, top_k: int = 5, doc_types: List[str] = None) -> Dict:
        """ë°ì´í„°ë² ì´ìŠ¤ ì§ì ‘ ì ‘ê·¼ìœ¼ë¡œ ë¬¸ì„œ ê²€ìƒ‰"""
        try:
            # ì§ì ‘ ì„ë² ë”© ì„œë¹„ìŠ¤ ë° ê²€ìƒ‰ ë¡œì§ êµ¬í˜„
            from sqlmodel.ext.asyncio.session import AsyncSession
            from sqlalchemy.ext.asyncio import create_async_engine
            from src.aws.embedding_service import TitanEmbeddingService
            from sqlalchemy import text
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
            database_url = os.getenv("DATABASE_URL") or f"postgresql+asyncpg://{os.getenv('DATABASE_USER', 'postgres')}:{os.getenv('DATABASE_PASSWORD', 'postgres')}@{os.getenv('DATABASE_HOST', 'localhost')}:{os.getenv('DATABASE_PORT', '5434')}/{os.getenv('DATABASE_NAME', 'smartclm-poc')}"
            async_engine = create_async_engine(database_url, echo=False)
            
            # ì„ë² ë”© ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
            embedding_service = TitanEmbeddingService()
            
            async with AsyncSession(async_engine) as session:
                # 1. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
                query_embedding = await embedding_service.create_single_embedding(query)
                
                # 2. ë²¡í„° ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
                base_query = """
                SELECT 
                    c.id as chunk_id,
                    c.content,
                    c.chunk_type,
                    c.parent_id,
                    d.id as document_id,
                    d.filename,
                    d.doc_type,
                    d.category,
                    (1 - (c.embedding <=> :query_embedding)) as similarity_score
                FROM chunks c
                JOIN documents d ON c.document_id = d.id
                WHERE c.embedding IS NOT NULL
                AND d.room_id IS NULL 
                """
                
                # ë²¡í„°ë¥¼ pgvector í˜•ì‹ ë¬¸ìì—´ë¡œ ë³€í™˜
                vector_str = "[" + ",".join(map(str, query_embedding)) + "]"
                params = {"query_embedding": vector_str}
                
                # ë¬¸ì„œ ìœ í˜• í•„í„°
                if doc_types:
                    type_conditions = []
                    for i, doc_type in enumerate(doc_types):
                        param_name = f"doc_type_{i}"
                        type_conditions.append(f"d.doc_type = :{param_name}")
                        params[param_name] = doc_type
                    base_query += f" AND ({' OR '.join(type_conditions)})"
                
                # ìœ ì‚¬ë„ ì •ë ¬ ë° ì œí•œ
                base_query += " ORDER BY similarity_score DESC LIMIT :top_k"
                params["top_k"] = top_k
                
                # 3. ì¿¼ë¦¬ ì‹¤í–‰
                connection = await session.connection()
                result = await connection.execute(text(base_query), params)
                rows = result.fetchall()
                
                # 4. ê²°ê³¼ ë³€í™˜
                search_results = []
                for row in rows:
                    search_results.append({
                        "chunk_id": row.chunk_id,
                        "content": row.content,
                        "similarity_score": round(row.similarity_score, 4),
                        "chunk_type": row.chunk_type,
                        "parent_id": row.parent_id,
                        "document_id": row.document_id,
                        "filename": row.filename,
                        "doc_type": row.doc_type,
                        "category": row.category,
                    })
                
                return {
                    "results": search_results,
                    "total_found": len(search_results),
                    "query": query
                }
                
        except Exception as e:
            logger.error(f"âŒ ì§ì ‘ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return {"results": [], "total_found": 0, "error": str(e)}
    
    async def convert_to_pdf_if_needed(self, file_path: Path) -> Path:
        """í•„ìš”ì‹œ PDFë¡œ ë³€í™˜ (DOCX ë“±)"""
        if file_path.suffix.lower() == '.pdf':
            return file_path
        
        logger.info(f"ğŸ”„ PDF ë³€í™˜ ì¤‘: {file_path.name}")
        
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                with open(file_path, "rb") as f:
                    files = {
                        "file": (file_path.name, f, "application/octet-stream")
                    }
                    
                    response = await client.post(
                        f"{self.doc_converter_url}/convert",
                        files=files
                    )
                
                if response.status_code == 200:
                    # PDF íŒŒì¼ì„ ì„ì‹œ ì €ì¥
                    pdf_path = PROCESSED_DIR / f"{file_path.stem}_converted.pdf"
                    with open(pdf_path, "wb") as f:
                        f.write(response.content)
                    
                    logger.info(f"âœ… PDF ë³€í™˜ ì™„ë£Œ: {pdf_path.name}")
                    return pdf_path
                else:
                    raise Exception(f"ë³€í™˜ ì‹¤íŒ¨: HTTP {response.status_code}")
                    
        except Exception as e:
            logger.error(f"âŒ PDF ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
            raise
    
    async def analyze_document_file(self, file_path: Path) -> ProcessingResult:
        """ë¬¸ì„œ íŒŒì¼ì„ ë¶„ì„"""
        start_time = time.time()
        doc_type = self.determine_doc_type(file_path)
        relative_path = file_path.relative_to(DOCS_ROOT)
        folder_path = str(relative_path.parent)
        
        try:
            logger.info(f"ğŸ“„ ë¬¸ì„œ ë¶„ì„ ì‹œì‘: {file_path.name} (íƒ€ì…: {doc_type})")
            
            # PDFë¡œ ë³€í™˜ (í•„ìš”ì‹œ)
            pdf_file_path = await self.convert_to_pdf_if_needed(file_path)
            
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                with open(pdf_file_path, "rb") as f:
                    files = {
                        "file": (pdf_file_path.name, f, "application/pdf")
                    }
                    data = {"smart_pipeline": True}
                    
                    response = await client.post(
                        f"{self.doc_parser_url}/analyze",
                        files=files,
                        data=data
                    )
                
                if response.status_code == 200:
                    result = response.json()
                    processing_time = time.time() - start_time
                    
                    return ProcessingResult(
                        filename=file_path.name,
                        doc_type=doc_type,
                        folder_path=folder_path,
                        success=True,
                        processing_time=processing_time,
                        page_count=result.get("page_count"),
                        chunk_count=len(result.get("chunks", [])),
                        doc_parser_result=result
                    )
                else:
                    error_msg = f"Doc Parser ì˜¤ë¥˜ (HTTP {response.status_code}): {response.text}"
                    logger.error(error_msg)
                    return ProcessingResult(
                        filename=file_path.name,
                        doc_type=doc_type,
                        folder_path=folder_path,
                        success=False,
                        processing_time=time.time() - start_time,
                        error_message=error_msg
                    )
            
            # ì„ì‹œ PDF íŒŒì¼ ì •ë¦¬
            if pdf_file_path != file_path and pdf_file_path.exists():
                pdf_file_path.unlink()
                    
        except Exception as e:
            error_msg = f"ë¬¸ì„œ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
            logger.error(error_msg)
            return ProcessingResult(
                filename=file_path.name,
                doc_type=doc_type,
                folder_path=folder_path,
                success=False,
                processing_time=time.time() - start_time,
                error_message=error_msg
            )
    
    async def save_to_database(self, parsed_result: Dict, source_file: Path, doc_type: str) -> bool:
        """íŒŒì‹±ëœ ê²°ê³¼ë¥¼ ì§ì ‘ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (S3 ì—…ë¡œë“œ ì—†ìŒ)"""
        try:
            logger.info(f"ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹œì‘: {source_file.name} (íƒ€ì…: {doc_type})")
            
            # ë¡œì»¬ì—ì„œ ì§ì ‘ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            from datetime import datetime
            from sqlmodel.ext.asyncio.session import AsyncSession
            from sqlalchemy.ext.asyncio import create_async_engine
            from src.models import Document, Chunk
            from src.aws.embedding_service import TitanEmbeddingService
            from src.documents.chunking import HierarchicalChunker
            import uuid
            
            # ì§ì ‘ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ URL ê°€ì ¸ì˜¤ê¸°
            database_url = os.getenv("DATABASE_URL") or f"postgresql+asyncpg://{os.getenv('DATABASE_USER', 'postgres')}:{os.getenv('DATABASE_PASSWORD', 'postgres')}@{os.getenv('DATABASE_HOST', '127.0.0.1')}:{os.getenv('DATABASE_PORT', '5434')}/{os.getenv('DATABASE_NAME', 'smartclm-poc')}"
            async_engine = create_async_engine(database_url, echo=False)
            
            async with AsyncSession(async_engine) as session:
                try:
                    # 1. Document ë©”íƒ€ë°ì´í„° êµ¬ì„±
                    filename = source_file.name
                    title = parsed_result.get("title", filename)
                    page_count = parsed_result.get("page_count", 0)
                    markdown_content = parsed_result.get("markdown_content", "")
                    html_content = parsed_result.get("html_content", "")
                    
                    # íŒŒì¼ í¬ê¸° ê³„ì‚°
                    file_size = source_file.stat().st_size if source_file.exists() else 0
                    
                    # docs/ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ ê³„ì‚°
                    relative_path = source_file.relative_to(DOCS_ROOT)
                    
                    # 2. Document ìƒì„± (ì‹¤ì œ íŒŒì¼ ê²½ë¡œ ì •ë³´ ì‚¬ìš©)
                    document = Document(
                        room_id=None,  # ì „ì—­ ë¬¸ì„œ
                        doc_type=doc_type,
                        category=None,
                        processing_status="processing",
                        filename=filename,
                        version=None,
                        s3_bucket="local",  # ë¡œì»¬ ì €ì¥ í‘œì‹œ
                        s3_key=str(relative_path),  # docs/ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ
                        pdf_s3_bucket="local",
                        pdf_s3_key=f"processed/{doc_type}/{filename}",  # ì²˜ë¦¬ëœ ê²°ê³¼ ê²½ë¡œ
                        file_size=file_size,
                        pdf_file_size=file_size,  # PDFì™€ ì›ë³¸ì´ ê°™ë‹¤ê³  ê°€ì •
                        page_count=page_count,
                        document_metadata={
                            "source": "rag_pipeline",
                            "local_processing": True,
                            "original_path": str(source_file),
                            "processing_time": parsed_result.get("processing_time", 0),
                            "chunks_count": len(parsed_result.get("chunks", [])),
                        },
                        auto_tags=[doc_type],
                        html_content=html_content,
                        markdown_content=markdown_content,
                    )
                    
                    # 3. Document ì €ì¥
                    session.add(document)
                    await session.commit()
                    await session.refresh(document)
                    document_id = document.id  # IDë¥¼ ë¯¸ë¦¬ ì €ì¥
                    logger.info(f"ğŸ“„ Document ì €ì¥ ì™„ë£Œ: ID {document_id}")
                    
                    # 4. ì²­í‚¹ ë° ì„ë² ë”© ì²˜ë¦¬
                    if markdown_content:
                        logger.info(f"âœ‚ï¸ ì²­í‚¹ ë° ì„ë² ë”© ì‹œì‘: {filename}")
                        
                        chunker = HierarchicalChunker()
                        embedding_service = TitanEmbeddingService()
                        
                        # ë§ˆí¬ë‹¤ìš´ ì²­í‚¹
                        chunking_result = chunker.chunk_markdown(
                            markdown_content=markdown_content,
                            filename=filename,
                        )
                        
                        # ë²¡í„°ìš© ì²­í¬ ìƒì„±
                        vector_ready_chunks = chunker.create_vector_ready_chunks(chunking_result)
                        
                        # ì„ë² ë”© ìƒì„±
                        embedded_chunks = await embedding_service.embed_chunked_documents(vector_ready_chunks)
                        
                        # ì²­í¬ ì €ì¥
                        chunk_objects = []
                        for i, chunk_data in enumerate(embedded_chunks):
                            if chunk_data.get("content", "").strip():
                                chunk = Chunk(
                                    document_id=document.id or 0,
                                    content=chunk_data.get("content", ""),
                                    chunk_index=i,
                                    header_1=chunk_data.get("headers", {}).get("header_1"),
                                    header_2=chunk_data.get("headers", {}).get("header_2"),
                                    header_3=chunk_data.get("headers", {}).get("header_3"),
                                    header_4=chunk_data.get("headers", {}).get("header_4"),
                                    parent_id=chunk_data.get("parent_id"),
                                    child_id=chunk_data.get("child_id"),
                                    chunk_type=chunk_data.get("chunk_type", "child"),
                                    embedding=chunk_data.get("embedding"),
                                    word_count=len(chunk_data.get("content", "").split()),
                                    char_count=len(chunk_data.get("content", "")),
                                    chunk_metadata=chunk_data.get("metadata", {}),
                                    auto_tags=chunk_data.get("auto_tags", []),
                                )
                                chunk_objects.append(chunk)
                        
                        # ë°°ì¹˜ë¡œ ì²­í¬ ì €ì¥
                        for chunk in chunk_objects:
                            session.add(chunk)
                        
                        await session.commit()
                        logger.info(f"âœ… ì²­í¬ ì €ì¥ ì™„ë£Œ: {len(chunk_objects)}ê°œ")
                    
                    # 5. ì²˜ë¦¬ ì™„ë£Œ ìƒíƒœë¡œ ë³€ê²½
                    document.processing_status = "completed"
                    session.add(document)
                    await session.commit()
                    
                    logger.info(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ: {filename} (Document ID: {document_id})")
                    return True
                    
                except Exception as e:
                    await session.rollback()
                    raise e
                    
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {str(e)}")
            import traceback
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return False


class RAGPipeline:
    """RAG íŒŒì´í”„ë¼ì¸ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.client = ExternalServiceClient()
        self.results: List[ProcessingResult] = []
    
    def find_all_documents(self) -> List[Path]:
        """ê·¼ê±° ìë£Œ í´ë”ì—ì„œë§Œ ë¬¸ì„œ íŒŒì¼ ì°¾ê¸°"""
        supported_extensions = {'.pdf', '.docx', '.doc', '.hwp', '.xlsx', '.xls', '.pptx', '.ppt'}
        document_files = []
        
        # ê·¼ê±° ìë£Œ í´ë”ë§Œ ê²€ìƒ‰
        base_folder = DOCS_ROOT / "ê·¼ê±° ìë£Œ"
        if base_folder.exists():
            for ext in supported_extensions:
                # ëª¨ë“  í•˜ìœ„ í´ë” ê²€ìƒ‰ (ë²•ë ¹, ì²´ê²°ê³„ì•½, í‘œì¤€ê³„ì•½ì„œ)
                document_files.extend(base_folder.glob(f"**/*{ext}"))
        
        return sorted(set(document_files))  # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    
    async def step1_store_reference_materials(self, force: bool = False):
        """Step 1: ê·¼ê±°ìë£Œ ì €ì¥"""
        logger.info("ğŸš€ Step 1: ê·¼ê±°ìë£Œ ì €ì¥ ì‹œì‘")
        
        # ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬
        health_status = await self.client.health_check_all()
        if not all(health_status.values()):
            unhealthy_services = [k for k, v in health_status.items() if not v]
            logger.error(f"âŒ ë‹¤ìŒ ì„œë¹„ìŠ¤ë“¤ì´ ë¹„ì •ìƒì…ë‹ˆë‹¤: {unhealthy_services}")
            return False
        
        # ë¬¸ì„œ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        document_files = self.find_all_documents()
        if not document_files:
            logger.warning("âš ï¸ docs í´ë”ì— ì§€ì›ë˜ëŠ” ë¬¸ì„œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        logger.info(f"ğŸ“ ë°œê²¬ëœ ë¬¸ì„œ íŒŒì¼: {len(document_files)}ê°œ")
        
        # ë¬¸ì„œ íƒ€ì…ë³„ ë¶„ë¥˜ ì¶œë ¥
        type_counts = {}
        for file_path in document_files:
            doc_type = self.client.determine_doc_type(file_path)
            type_counts[doc_type] = type_counts.get(doc_type, 0) + 1
        
        logger.info("ğŸ“Š ë¬¸ì„œ íƒ€ì…ë³„ ë¶„ë¥˜:")
        for doc_type, count in type_counts.items():
            logger.info(f"  - {doc_type}: {count}ê°œ")
        
        # ê° ë¬¸ì„œ íŒŒì¼ ì²˜ë¦¬
        for i, file_path in enumerate(document_files, 1):
            doc_type = self.client.determine_doc_type(file_path)
            logger.info(f"ğŸ“„ ì²˜ë¦¬ ì¤‘ ({i}/{len(document_files)}): {file_path.name} ({doc_type})")
            
            # 1. JSON íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸ (ë¬¸ì„œ ë¶„ì„ ê±´ë„ˆë›°ê¸°ìš©)
            doc_type_folder = PROCESSED_DIR / doc_type
            result_file = doc_type_folder / f"{file_path.stem}_parsed.json"
            old_format_file = PROCESSED_DIR / f"{file_path.stem}_{doc_type}_parsed.json"
            
            if not force and (result_file.exists() or old_format_file.exists()):
                # ê¸°ì¡´ JSON íŒŒì¼ ë¡œë“œ
                json_file = result_file if result_file.exists() else old_format_file
                logger.info(f"ğŸ“„ ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ì‚¬ìš©: {json_file.name}")
                
                with open(json_file, 'r', encoding='utf-8') as f:
                    doc_parser_result = json.load(f)
                
                result = ProcessingResult(
                    filename=file_path.name,
                    doc_type=doc_type,
                    folder_path=str(file_path.parent),
                    success=True,
                    processing_time=0,
                    page_count=doc_parser_result.get("page_count"),
                    chunk_count=len(doc_parser_result.get("chunks", [])),
                    doc_parser_result=doc_parser_result
                )
            else:
                # ìƒˆë¡œ ë¬¸ì„œ ë¶„ì„
                logger.info(f"ğŸ” ë¬¸ì„œ ë¶„ì„ ì‹œì‘: {file_path.name}")
                result = await self.client.analyze_document_file(file_path)
            
            self.results.append(result)
            
            if result.success:
                # 2. íƒ€ì…ë³„ í´ë” ìƒì„±
                import shutil
                doc_type_folder = PROCESSED_DIR / doc_type
                doc_type_folder.mkdir(exist_ok=True)
                
                # 3. ë¶„ì„ ê²°ê³¼ ì €ì¥ (íƒ€ì…ë³„ í´ë”ì— JSON ì €ì¥)
                result_file = doc_type_folder / f"{file_path.stem}_parsed.json"
                with open(result_file, 'w', encoding='utf-8') as f:
                    json.dump(result.doc_parser_result, f, ensure_ascii=False, indent=2)
                logger.info(f"ğŸ“„ JSON ì €ì¥ ì™„ë£Œ: {result_file}")
                
                # 4. ì›ë³¸ íŒŒì¼ì„ íƒ€ì…ë³„ í´ë”ë¡œ ë³µì‚¬
                copied_file = doc_type_folder / file_path.name
                shutil.copy2(file_path, copied_file)
                logger.info(f"ğŸ“ íŒŒì¼ ë³µì‚¬ ì™„ë£Œ: {copied_file}")
                
                # 5. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ (ì¤‘ë³µ ì²´í¬)
                # DBì— ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸
                db_already_exists = False
                if not force:
                    try:
                        from sqlmodel.ext.asyncio.session import AsyncSession
                        from sqlalchemy.ext.asyncio import create_async_engine
                        from src.models import Document
                        from sqlmodel import select
                        
                        # ì§ì ‘ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ URL ê°€ì ¸ì˜¤ê¸°
                        database_url = os.getenv("DATABASE_URL") or f"postgresql+asyncpg://{os.getenv('DATABASE_USER', 'postgres')}:{os.getenv('DATABASE_PASSWORD', 'postgres')}@{os.getenv('DATABASE_HOST', 'localhost')}:{os.getenv('DATABASE_PORT', '5434')}/{os.getenv('DATABASE_NAME', 'smartclm-poc')}"
                        async_engine = create_async_engine(database_url, echo=False)
                        
                        async with AsyncSession(async_engine) as session:
                            # ì™„ë£Œëœ ë¬¸ì„œë§Œ ìŠ¤í‚µ, processing ìƒíƒœëŠ” ì¬ì²˜ë¦¬
                            query = select(Document).where(
                                Document.filename == file_path.name,
                                Document.doc_type == doc_type,
                                Document.processing_status == "completed"
                            )
                            result_doc = await session.exec(query)
                            existing_doc = result_doc.first()
                            
                            if existing_doc:
                                logger.info(f"ğŸ’¾ DB ì €ì¥ ê±´ë„ˆë›°ê¸°: {file_path.name} (ì´ë¯¸ ì €ì¥ë¨: ID {existing_doc.id})")
                                db_already_exists = True
                    except Exception as e:
                        logger.warning(f"âš ï¸ DB ì¤‘ë³µ í™•ì¸ ì‹¤íŒ¨: {file_path.name} - {str(e)}")
                        logger.info(f"ğŸ’¾ ì¤‘ë³µ í™•ì¸ ì‹¤íŒ¨ë¡œ ì¸í•´ ì €ì¥ì„ ì‹œë„í•©ë‹ˆë‹¤: {file_path.name}")
                
                if not db_already_exists:
                    db_success = await self.client.save_to_database(
                        result.doc_parser_result, 
                        file_path,  # ì „ì²´ íŒŒì¼ ê²½ë¡œ ì „ë‹¬
                        doc_type
                    )
                    
                    if db_success:
                        logger.info(f"âœ… ì™„ë£Œ: {file_path.name} ({result.processing_time:.2f}ì´ˆ)")
                    else:
                        logger.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {file_path.name}")
                else:
                    logger.info(f"âœ… ì™„ë£Œ: {file_path.name} ({result.processing_time:.2f}ì´ˆ) - DBëŠ” ê¸°ì¡´ ì‚¬ìš©")
            else:
                logger.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {file_path.name} - {result.error_message}")
            
            # ì²˜ë¦¬ ê°„ ì ì‹œ ëŒ€ê¸° (ì‹œìŠ¤í…œ ë¶€í•˜ ë°©ì§€)
            await asyncio.sleep(1)
        
        # ê²°ê³¼ ìš”ì•½ ì €ì¥
        await self._save_processing_summary()
        
        return True
    
    async def _ensure_contract_in_db(self, document_path: Path) -> Optional[int]:
        """ê³„ì•½ì„œê°€ DBì— ì—†ìœ¼ë©´ ì €ì¥í•˜ê³ , ìˆìœ¼ë©´ ê¸°ì¡´ ID ë°˜í™˜"""
        try:
            from sqlmodel.ext.asyncio.session import AsyncSession
            from sqlalchemy.ext.asyncio import create_async_engine
            from src.models import Document
            from sqlmodel import select
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
            database_url = os.getenv("DATABASE_URL") or f"postgresql+asyncpg://{os.getenv('DATABASE_USER', 'postgres')}:{os.getenv('DATABASE_PASSWORD', 'postgres')}@{os.getenv('DATABASE_HOST', 'localhost')}:{os.getenv('DATABASE_PORT', '5434')}/{os.getenv('DATABASE_NAME', 'smartclm-poc')}"
            async_engine = create_async_engine(database_url, echo=False)
            
            async with AsyncSession(async_engine) as session:
                # 1. ê¸°ì¡´ ë¬¸ì„œ í™•ì¸
                query = select(Document).where(
                    Document.filename == document_path.name,
                    Document.doc_type == "contract",
                    Document.processing_status == "completed"
                )
                result = await session.exec(query)
                existing_doc = result.first()
                
                if existing_doc:
                    logger.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ê³„ì•½ì„œ ì‚­ì œ í›„ ì¬ìƒì„±: {document_path.name} (ID: {existing_doc.id})")
                    # ê¸°ì¡´ ì²­í¬ë“¤ ì‚­ì œ
                    from sqlmodel import delete
                    from src.models import Chunk
                    chunk_delete_stmt = delete(Chunk).where(Chunk.document_id == existing_doc.id)
                    await session.exec(chunk_delete_stmt)
                    
                    # ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ
                    doc_delete_stmt = delete(Document).where(Document.id == existing_doc.id)
                    await session.exec(doc_delete_stmt)
                    await session.commit()
                    logger.info(f"âœ… ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
                
                # 2. ìƒˆë¡œ ë¶„ì„ ë° ì €ì¥
                logger.info(f"ğŸ” ê³„ì•½ì„œ ë¶„ì„ ë° DB ì €ì¥: {document_path.name}")
                doc_data = await self.client.analyze_document_file(document_path)
                
                if not doc_data.success:
                    logger.error(f"âŒ ë¬¸ì„œ ë¶„ì„ ì‹¤íŒ¨: {doc_data.error_message}")
                    return None
                
                logger.info(f"âœ… ë¬¸ì„œ ë¶„ì„ ì™„ë£Œ ({doc_data.processing_time:.2f}ì´ˆ)")
                
                # 3. DB ì €ì¥
                db_success = await self.client.save_to_database(
                    doc_data.doc_parser_result,
                    document_path,
                    "contract"  # contract íƒ€ì…ìœ¼ë¡œ ì €ì¥
                )
                
                if not db_success:
                    logger.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {document_path.name}")
                    return None
                
                # 4. ì €ì¥ëœ ë¬¸ì„œ ID ì¡°íšŒ
                result2 = await session.exec(query)
                new_doc = result2.first()
                
                if new_doc:
                    logger.info(f"âœ… ìƒˆ ê³„ì•½ì„œ ì €ì¥ ì™„ë£Œ: ID {new_doc.id}")
                    return new_doc.id
                
                return None
                
        except Exception as e:
            logger.error(f"âŒ ê³„ì•½ì„œ DB ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return None
    
    async def _get_document_from_db(self, document_id: int) -> Optional[Dict]:
        """DBì—ì„œ ë¬¸ì„œ ì •ë³´ ì¡°íšŒ"""
        try:
            from sqlmodel.ext.asyncio.session import AsyncSession
            from sqlalchemy.ext.asyncio import create_async_engine
            from src.models import Document
            from sqlmodel import select
            
            database_url = os.getenv("DATABASE_URL") or f"postgresql+asyncpg://{os.getenv('DATABASE_USER', 'postgres')}:{os.getenv('DATABASE_PASSWORD', 'postgres')}@{os.getenv('DATABASE_HOST', 'localhost')}:{os.getenv('DATABASE_PORT', '5434')}/{os.getenv('DATABASE_NAME', 'smartclm-poc')}"
            async_engine = create_async_engine(database_url, echo=False)
            
            async with AsyncSession(async_engine) as session:
                query = select(Document).where(Document.id == document_id)
                result = await session.exec(query)
                document = result.first()
                
                if document:
                    return {
                        "id": document.id,
                        "filename": document.filename,
                        "doc_type": document.doc_type,
                        "markdown_content": document.markdown_content,
                        "page_count": document.page_count
                    }
                return None
                
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return None
    
    async def _get_document_chunks_from_db(self, document_id: int) -> List[Dict]:
        """DBì—ì„œ ë¬¸ì„œì˜ ì²­í¬ë“¤ ì¡°íšŒ"""
        try:
            from sqlmodel.ext.asyncio.session import AsyncSession
            from sqlalchemy.ext.asyncio import create_async_engine
            from src.models import Chunk
            from sqlmodel import select
            
            database_url = os.getenv("DATABASE_URL") or f"postgresql+asyncpg://{os.getenv('DATABASE_USER', 'postgres')}:{os.getenv('DATABASE_PASSWORD', 'postgres')}@{os.getenv('DATABASE_HOST', 'localhost')}:{os.getenv('DATABASE_PORT', '5434')}/{os.getenv('DATABASE_NAME', 'smartclm-poc')}"
            async_engine = create_async_engine(database_url, echo=False)
            
            async with AsyncSession(async_engine) as session:
                query = select(Chunk).where(
                    Chunk.document_id == document_id,
                    Chunk.chunk_type == "parent"
                ).order_by(Chunk.chunk_index)
                
                result = await session.exec(query)
                chunks = result.all()
                
                # Chunk ëª¨ë¸ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                chunk_list = []
                for chunk in chunks:
                    chunk_dict = {
                        "chunk_index": chunk.chunk_index,
                        "chunk_type": chunk.chunk_type,
                        "content": chunk.content,
                        "header_1": chunk.header_1,
                        "header_2": chunk.header_2,
                        "header_3": chunk.header_3,
                        "chunk_metadata": chunk.chunk_metadata or {}
                    }
                    chunk_list.append(chunk_dict)
                
                logger.info(f"ğŸ“Š DBì—ì„œ {len(chunk_list)}ê°œ parent ì²­í¬ ì¡°íšŒ")
                return chunk_list
                
        except Exception as e:
            logger.error(f"âŒ ì²­í¬ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return []
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ”— ì²´ì¸ ë¶„ì„ ë©”ì†Œë“œë“¤ (Chain 1, 2, 3)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def _chain1_identify_violations(self, document_name: str, all_chunks: List[Dict], document_id: int) -> List[Dict]:
        """Chain 1: ì „ì²´ ê³„ì•½ì„œë¥¼ ìŠ¤ìº”í•˜ì—¬ ìœ„ë²• ì¡°í•­ë“¤ì„ ì‹ë³„"""
        try:
            # ì „ì²´ ê³„ì•½ì„œ êµ¬ì¡° ìƒì„± (ì›ë³¸ ì¡°í•­ ë²ˆí˜¸ ë³´ì¡´)
            contract_structure = []
            for i, chunk in enumerate(all_chunks, 1):
                title = chunk.get("header_1", f"ì¡°í•­ {i}")
                content = chunk.get("content", "")[:200] + "..." if len(chunk.get("content", "")) > 200 else chunk.get("content", "")
                

                # titleì´ ì´ë¯¸ "ì œXì¡°" í˜•íƒœì¸ì§€ í™•ì¸
                if title.startswith("ì œ") and "ì¡°" in title:
                    # ì´ë¯¸ ì¡°í•­ ë²ˆí˜¸ê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    contract_structure.append(f"{title}: {content}")
                else:
                    # ì¡°í•­ ë²ˆí˜¸ê°€ ì—†ìœ¼ë©´ ìˆœì„œëŒ€ë¡œ ì¶”ê°€
                    contract_structure.append(f"ì œ{i}ì¡° {title}: {content}")
            
            full_contract = "\n\n".join(contract_structure)
            
            # Chain 1 í”„ë¡¬í”„íŠ¸
            chain1_prompt = f"""# ğŸ“‹ (ì£¼)ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ ê´€ì  ê³„ì•½ì„œ ìœ„í—˜ ì¡°í•­ ì‹ë³„

**ê³„ì•½ì„œëª…:** {document_name}
**ì´ ì¡°í•­ìˆ˜:** {len(all_chunks)}ê°œ
**ê²€í†  ê´€ì :** **(ì£¼)ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ ì…ì¥ì—ì„œ ë¶ˆë¦¬í•œ ì¡°í•­ ì¤‘ì‹¬ ë¶„ì„**

**ì „ì²´ ê³„ì•½ì„œ êµ¬ì¡°:**
```
{full_contract}
```

## ğŸ¯ ë¶„ì„ ìš”êµ¬ì‚¬í•­

ì´ ê³„ì•½ì„œë¥¼ **(ì£¼)ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ ì…ì¥**ì—ì„œ ê²€í† í•˜ì—¬ **ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ì—ê²Œ ë¶ˆë¦¬í•˜ê±°ë‚˜ ìœ„í—˜í•œ ì¡°í•­ë“¤**ì„ ëª¨ë‘ ì°¾ì•„ì£¼ì„¸ìš”.

### ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ ê´€ì  ìƒì„¸ ê²€í†  ê¸°ì¤€:

**ğŸ” ë¹„ë°€ì •ë³´ ê´€ë ¨ ìœ„í—˜:**
1. **ì •ë³´ ë³´í˜¸ ë²”ìœ„ì˜ ê³¼ë„í•œ ì¶•ì†Œ**: "ë¹„ë°€" í‘œì‹œ ëˆ„ë½ ì‹œ í•µì‹¬ì •ë³´ë„ ë³´í˜¸ë°›ì§€ ëª»í•˜ëŠ” ì¡°í•­
2. **ì¼ë°©ì  ë¹„ë°€ìœ ì§€ ì˜ë¬´**: ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ì—ê²Œë§Œ ê³¼ë„í•œ ë¹„ë°€ìœ ì§€ ì˜ë¬´ ë¶€ê³¼
3. **ë¹„ë°€ì •ë³´ ì •ì˜ì˜ ë¶ˆê· í˜•**: ìƒëŒ€ë°© ì •ë³´ëŠ” ë„“ê²Œ, ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ ì •ë³´ëŠ” ì¢ê²Œ ì •ì˜

**âš–ï¸ ì±…ì„ ë° ë°°ìƒ ê´€ë ¨ ìœ„í—˜:**
4. **ê³¼ë„í•œ ì†í•´ë°°ìƒ**: ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ì—ê²Œë§Œ ê³¼ì¤‘í•œ ë°°ìƒì±…ì„ ë¶€ê³¼
5. **ë°°ìƒ í•œë„ì˜ ë¶ˆê· í˜•**: ìƒëŒ€ë°©ì€ ë¬´ì œí•œ, ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ëŠ” ì œí•œëœ ë°°ìƒ
6. **ì…ì¦ì±…ì„ì˜ ì „ê°€**: ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ê°€ ë¬´ê³¼ì‹¤ì„ ì…ì¦í•´ì•¼ í•˜ëŠ” ì¡°í•­

**ğŸšª ê³„ì•½ í•´ì§€ ë° ê¶Œë¦¬ ê´€ë ¨ ìœ„í—˜:**
7. **ì¼ë°©ì  í•´ì§€ê¶Œ**: ìƒëŒ€ë°©ì—ê²Œë§Œ í•´ì§€ê¶Œì„ ë¶€ì—¬í•˜ëŠ” ë¶ˆí‰ë“± ì¡°í•­
8. **ê¶Œë¦¬ í–‰ì‚¬ ì œí•œ**: ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ì˜ ì •ë‹¹í•œ ê¶Œë¦¬(ê°€ì²˜ë¶„, ì†í•´ë°°ìƒì²­êµ¬ ë“±) ì œí•œ
9. **í†µì§€ ì˜ë¬´ì˜ ë¶ˆê· í˜•**: ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ì—ê²Œë§Œ ê¹Œë‹¤ë¡œìš´ í†µì§€ ì˜ë¬´ ë¶€ê³¼

**ğŸ“‹ ì ˆì°¨ ë° ê¸°íƒ€ ìœ„í—˜:**
10. **ê´€í•  ë²•ì›ì˜ ë¶ˆë¦¬í•¨**: ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ì—ê²Œ ë¶ˆë¦¬í•œ ì›ê±°ë¦¬ ê´€í•  ì§€ì •
11. **ê³„ì•½ ê¸°ê°„ì˜ ë¶ˆê· í˜•**: ì˜ë¬´ëŠ” ê¸¸ê²Œ, ê¶Œë¦¬ëŠ” ì§§ê²Œ ì„¤ì •
12. **ë²•ì  ìœ„í—˜**: ê°•í–‰ë²•ê·œ ìœ„ë°˜ìœ¼ë¡œ ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ê°€ ë¶ˆì´ìµì„ ë°›ì„ ìˆ˜ ìˆëŠ” ì¡°í•­

**âš ï¸ íŠ¹íˆ ì£¼ì˜ê¹Šê²Œ ê²€í† í•  ì¡°í•­:**
- ë¹„ë°€ì •ë³´ ì •ì˜ì—ì„œ "í‘œì‹œ" ìš”êµ¬ì‚¬í•­ (ì‹¤ë¬´ìƒ ëˆ„ë½ ìœ„í—˜ ë†’ìŒ)
- ì†í•´ë°°ìƒ ì¡°í•­ì˜ ê¸ˆì•¡ ì œí•œ ë° ë©´ì±… ì¡°í•­
- ê³„ì•½ ìœ„ë°˜ ì‹œ êµ¬ì œìˆ˜ë‹¨ ì œí•œ ì¡°í•­
- ì¤€ê±°ë²• ë° ê´€í•  ì¡°í•­

## ğŸ“ ì¶œë ¥ í˜•ì‹ (JSON)

ìœ„ë²• ì¡°í•­ì´ ìˆë‹¤ë©´ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”:

```json
{{
  "violations_found": true,
  "total_violations": 3,
  "violations": [
    {{
      "clause_number": "ì œ3ì¡°",
      "clause_title": "ë¹„ë°€ì •ë³´ì˜ ì •ì˜",
      "risk_type": "ì •ë³´_ë³´í˜¸_ë²”ìœ„ì˜_ê³¼ë„í•œ_ì¶•ì†Œ",
      "risk_level": "ë†’ìŒ",
      "brief_reason": "ë¹„ë°€ í‘œì‹œ ëˆ„ë½ ì‹œ í•µì‹¬ì •ë³´ë„ ë³´í˜¸ë°›ì§€ ëª»í•´ ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ì—ê²Œ ë§¤ìš° ìœ„í—˜"
    }},
    {{
      "clause_number": "ì œ9ì¡°", 
      "clause_title": "ì†í•´ë°°ìƒ",
      "risk_type": "ê³¼ë„í•œ_ì†í•´ë°°ìƒ_ë°_ê¶Œë¦¬ì œí•œ",
      "risk_level": "ë†’ìŒ",
      "brief_reason": "ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ë§Œ ë°°ìƒ í•œë„ ì œí•œë˜ê³  ê°€ì²˜ë¶„ ì²­êµ¬ê¶Œë„ ë°•íƒˆë‹¹í•¨"
    }},
    {{
      "clause_number": "ì œ8ì¡°",
      "clause_title": "ê³„ì•½ê¸°ê°„",
      "risk_type": "ê³„ì•½_ê¸°ê°„ì˜_ë¶ˆê· í˜•",
      "risk_level": "ì¤‘ê°„", 
      "brief_reason": "ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ì˜ ì˜ë¬´ëŠ” 3ë…„+1ë…„, ìƒëŒ€ë°© ì˜ë¬´ëŠ” ê³„ì•½ ì¢…ë£Œì™€ í•¨ê»˜ ì†Œë©¸"
    }}
  ]
}}
```

ìœ„ë²• ì¡°í•­ì´ ì—†ë‹¤ë©´:
```json
{{
  "violations_found": false,
  "total_violations": 0,
  "violations": []
}}
```

**ì¤‘ìš”**: ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ê³ , ì¶”ê°€ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”."""

            # AI í˜¸ì¶œ
            from src.aws.bedrock_service import BedrockService
            bedrock_service = BedrockService()
            
            response = bedrock_service.invoke_model(
                prompt=chain1_prompt,
                max_tokens=2000,
                temperature=0.0
            )
            
            # JSON íŒŒì‹±
            response_text = response.get("text", "") if isinstance(response, dict) else str(response)
            
            import json
            import re
            
            # JSON ì¶”ì¶œ (```json íƒœê·¸ ì œê±°)
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # JSON íƒœê·¸ ì—†ì´ ì§ì ‘ JSONì¸ ê²½ìš°
                json_str = response_text.strip()
            
            try:
                result = json.loads(json_str)
                violations = result.get("violations", [])
                
                # chunk_index ì¶”ê°€ (ì¡°í•­ ë²ˆí˜¸ì—ì„œ ì¶”ì¶œ)
                for violation in violations:
                    clause_num = violation.get("clause_number", "")
                    # "ì œ3ì¡°" -> 3 ì¶”ì¶œ
                    import re
                    match = re.search(r'ì œ(\d+)ì¡°', clause_num)
                    if match:
                        violation["chunk_index"] = int(match.group(1)) - 1  # 0-based
                    else:
                        violation["chunk_index"] = 0
                
                logger.info(f"ğŸ” Chain 1 ê²°ê³¼: {len(violations)}ê°œ ìœ„ë²• ì¡°í•­ í›„ë³´ ì‹ë³„")
                return violations
                
            except json.JSONDecodeError as e:
                logger.error(f"âŒ Chain 1 JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                logger.error(f"ì‘ë‹µ í…ìŠ¤íŠ¸: {response_text[:500]}...")
                return []
                
        except Exception as e:
            logger.error(f"âŒ Chain 1 ì‹¤íŒ¨: {str(e)}")
            return []
    
    async def _chain2_search_related_laws(self, violation_candidates: List[Dict]) -> List[Dict]:
        """Chain 2: ê° ìœ„ë²• ì¡°í•­ë³„ë¡œ ê´€ë ¨ ë²•ë ¹ì„ ë³‘ë ¬ ê²€ìƒ‰"""
        violations_with_laws = []
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ íƒœìŠ¤í¬ ìƒì„±
        search_tasks = []
        for violation in violation_candidates:
            search_query = f"{violation.get('clause_title', '')} {violation.get('brief_reason', '')}"
            task = self._search_laws_for_violation(violation, search_query)
            search_tasks.append(task)
        
        # ë³‘ë ¬ ì‹¤í–‰
        if search_tasks:
            results = await asyncio.gather(*search_tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, Exception):
                    logger.error(f"âŒ ë²•ë ¹ ê²€ìƒ‰ ì‹¤íŒ¨: {result}")
                else:
                    violations_with_laws.append(result)
        
        logger.info(f"ğŸ” Chain 2 ê²°ê³¼: {len(violations_with_laws)}ê°œ ì¡°í•­ì˜ ë²•ë ¹ ê²€ìƒ‰ ì™„ë£Œ")
        return violations_with_laws
    
    async def _search_laws_for_violation(self, violation: Dict, search_query: str) -> Dict:
        """ê°œë³„ ìœ„ë²• ì¡°í•­ì— ëŒ€í•œ ë²•ë ¹ ê²€ìƒ‰"""
        try:
            # ê´€ë ¨ ë²•ë ¹ ê²€ìƒ‰
            legal_docs = await self.client.search_documents_direct(
                query=search_query,
                top_k=3,
                doc_types=["law"]
            )
            
            violation_with_laws = violation.copy()
            violation_with_laws["related_laws"] = legal_docs.get("results", [])
            violation_with_laws["laws_found"] = len(legal_docs.get("results", []))
            
            logger.info(f"  ğŸ“– {violation.get('clause_title', '')}: {len(legal_docs.get('results', []))}ê°œ ê´€ë ¨ ë²•ë ¹ ë°œê²¬")
            return violation_with_laws
            
        except Exception as e:
            logger.error(f"âŒ ë²•ë ¹ ê²€ìƒ‰ ì‹¤íŒ¨ ({violation.get('clause_title', '')}): {e}")
            violation_with_laws = violation.copy()
            violation_with_laws["related_laws"] = []
            violation_with_laws["laws_found"] = 0
            return violation_with_laws
    
    async def _chain3_detailed_analysis(self, violations_with_laws: List[Dict], document_name: str) -> Dict:
        """Chain 3: ìœ„ë²• ì¡°í•­ê³¼ ë²•ë ¹ ê·¼ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ìƒì„¸ ë¶„ì„"""
        try:
            if not violations_with_laws:
                return {"violations": []}
            
            final_violations = []
            
            for violation_data in violations_with_laws:
                try:
                    # ìƒì„¸ ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
                    clause_title = violation_data.get("clause_title", "")
                    clause_number = violation_data.get("clause_number", "")
                    risk_type = violation_data.get("risk_type", "")
                    brief_reason = violation_data.get("brief_reason", "")
                    related_laws = violation_data.get("related_laws", [])
                    
                    # ê´€ë ¨ ë²•ë ¹ í…ìŠ¤íŠ¸ êµ¬ì„±
                    laws_text = ""
                    if related_laws:
                        law_descriptions = []
                        for i, law in enumerate(related_laws, 1):
                            filename = law.get('filename', '').replace('.pdf', '')
                            content = law.get('content', '')[:300] + "..." if len(law.get('content', '')) > 300 else law.get('content', '')
                            similarity = f"(ìœ ì‚¬ë„: {law.get('similarity_score', 0):.3f})"
                            law_descriptions.append(f"{i}. {filename} {similarity}\n{content}")
                        laws_text = "\n\n".join(law_descriptions)
                    else:
                        laws_text = "ê´€ë ¨ ë²•ë ¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    
                    chain3_prompt = f"""# ğŸ›ï¸ (ì£¼)ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ ê´€ì  ê³„ì•½ì„œ ì¡°í•­ ìƒì„¸ ë¶„ì„

**ë¶„ì„ ëŒ€ìƒ ì¡°í•­:** {clause_number} {clause_title}
**ê²€í†  ê´€ì :** (ì£¼)ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ ì…ì¥ì—ì„œì˜ ìœ„í—˜ì„± í‰ê°€
**ì˜ˆë¹„ ìœ„í—˜ ìœ í˜•:** {risk_type}
**ì˜ˆë¹„ íŒë‹¨:** {brief_reason}

## ğŸ“š ê´€ë ¨ ë²•ë ¹ ê·¼ê±°

{laws_text}

## ğŸ¯ ìƒì„¸ ë¶„ì„ ìš”êµ¬ì‚¬í•­

ìœ„ ì¡°í•­ê³¼ ê´€ë ¨ ë²•ë ¹ì„ ë°”íƒ•ìœ¼ë¡œ **(ì£¼)ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ ì…ì¥ì—ì„œ** ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìƒì„¸ ë¶„ì„í•´ì£¼ì„¸ìš”:

### ì¶œë ¥ í˜•ì‹ (JSON):
```json
{{
  "ì¡°í•­_ìœ„ì¹˜": "{clause_number} ({clause_title})",
  "ë¦¬ìŠ¤í¬_ìœ í˜•": "ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ ê´€ì ì˜ êµ¬ì²´ì ì¸ ìœ„í—˜ ìœ í˜• (ì˜ˆ: ì •ë³´ ë³´í˜¸ ë²”ìœ„ì˜ ê³¼ë„í•œ ì¶•ì†Œ, ê³¼ë„í•œ_ë°°ìƒì±…ì„, ë¶ˆë¦¬í•œ_í•´ì§€ì¡°ê±´)",
  "íŒë‹¨_ê·¼ê±°": "ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ì—ê²Œ ì–´ë–¤ ë¶ˆì´ìµì´ ìˆëŠ”ì§€ ê´€ë ¨ ë²•ë ¹ê³¼ í•¨ê»˜ êµ¬ì²´ì  ì œì‹œ (ì˜ˆ: ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ì—ê²Œë§Œ ë¯¼ë²• ì œ398ì¡° ìœ„ë°˜ ìˆ˜ì¤€ì˜ ê³¼ë„í•œ ë°°ìƒì±…ì„ ë¶€ê³¼)",
  "ì˜ê²¬": "ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ ì…ì¥ì—ì„œì˜ êµ¬ì²´ì ì¸ ê°œì„  ë°©ì•ˆ (ì˜ˆ: ìƒí˜¸ ë°°ìƒì±…ì„ìœ¼ë¡œ ë³€ê²½í•˜ê±°ë‚˜ ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ ë°°ìƒ í•œë„ ì„¤ì • í•„ìš”)",
  "ê´€ë ¨_ë²•ë ¹": ["êµ¬ì²´ì ì¸ ë²•ë ¹ ì¡°í•­ëª…"]
}}
```

**ì¤‘ìš”**: 
1. ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥
2. **(ì£¼)ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ ì…ì¥**ì—ì„œ ë¶ˆë¦¬í•œ ì ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„
3. ê´€ë ¨ ë²•ë ¹ì´ ìˆë‹¤ë©´ êµ¬ì²´ì ì¸ ì¡°í•­ëª… ëª…ì‹œ  
4. ë¹„ì—ìŠ¤ì§€íŒŒíŠ¸ë„ˆìŠ¤ì—ê²Œ ì‹¤ë¬´ì ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ” ê°œì„ ë°©ì•ˆ ì œì‹œ
5. ì¶”ê°€ ì„¤ëª… ì—†ì´ JSONë§Œ ì¶œë ¥"""

                    # AI í˜¸ì¶œ
                    from src.aws.bedrock_service import BedrockService
                    bedrock_service = BedrockService()
                    
                    response = bedrock_service.invoke_model(
                        prompt=chain3_prompt,
                        max_tokens=1500,
                        temperature=0.0
                    )
                    
                    # JSON íŒŒì‹±
                    response_text = response.get("text", "") if isinstance(response, dict) else str(response)
                    
                    import json
                    import re
                    
                    # JSON ì¶”ì¶œ
                    json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
                    if json_match:
                        json_str = json_match.group(1)
                    else:
                        json_str = response_text.strip()
                    
                    try:
                        detailed_analysis = json.loads(json_str)
                        final_violations.append(detailed_analysis)
                        logger.info(f"  âœ… {clause_title} ìƒì„¸ ë¶„ì„ ì™„ë£Œ")
                        
                    except json.JSONDecodeError as e:
                        logger.error(f"âŒ Chain 3 JSON íŒŒì‹± ì‹¤íŒ¨ ({clause_title}): {e}")
                        # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í˜•ì‹ìœ¼ë¡œ ëŒ€ì²´
                        final_violations.append({
                            "ì¡°í•­_ìœ„ì¹˜": f"{clause_number} ({clause_title})",
                            "ë¦¬ìŠ¤í¬_ìœ í˜•": risk_type,
                            "íŒë‹¨_ê·¼ê±°": brief_reason,
                            "ì˜ê²¬": "ìƒì„¸ ë¶„ì„ ì‹¤íŒ¨ë¡œ ì¸í•´ ê¸°ë³¸ ì •ë³´ë§Œ ì œê³µ",
                            "ê´€ë ¨_ë²•ë ¹": [law.get('filename', '').replace('.pdf', '') for law in related_laws[:2]]
                        })
                
                except Exception as e:
                    logger.error(f"âŒ ê°œë³„ ì¡°í•­ ë¶„ì„ ì‹¤íŒ¨ ({violation_data.get('clause_title', '')}): {e}")
                    continue
            
            return {
                "contract_analysis": {
                    "document_name": document_name,
                    "total_violations": len(final_violations),
                    "analysis_method": "3_chain_analysis",
                    "analysis_date": datetime.now().isoformat()
                },
                "violations": final_violations
            }
            
        except Exception as e:
            logger.error(f"âŒ Chain 3 ì‹¤íŒ¨: {str(e)}")
            return {"violations": []}
    
    async def _save_chain_analysis_results(self, document_name: str, analysis_result: Dict, performance_stats: Dict):
        """ì²´ì¸ ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        try:
            # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ê²°ê³¼ íŒŒì¼ëª…
            result_filename = f"chain_analysis_{document_name.replace('.docx', '')}_{timestamp}.json"
            result_file = RESULTS_DIR / result_filename
            
            # ìµœì¢… ê²°ê³¼ êµ¬ì„±
            final_result = {
                "metadata": {
                    "timestamp": datetime.now().isoformat(),
                    "document_name": document_name,
                    "analysis_method": "3_chain_analysis",
                    "performance": performance_stats
                },
                "summary": {
                    "violations_found": len(analysis_result.get("violations", [])),
                    "total_analysis_time": performance_stats.get("total_time", 0),
                    "efficiency_gain": f"ê¸°ì¡´ ì¡°í•­ë³„ ë¶„ì„ ëŒ€ë¹„ {performance_stats.get('total_clauses', 0) * 30 - performance_stats.get('total_time', 0):.1f}ì´ˆ ë‹¨ì¶•"
                },
                "analysis_result": analysis_result
            }
            
            # JSON íŒŒì¼ ì €ì¥
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(final_result, f, ensure_ascii=False, indent=2)
            
            # ë¡œê·¸ ì¶œë ¥
            logger.info("=" * 50)
            logger.info("ğŸ“ ì²´ì¸ ë¶„ì„ ê²°ê³¼ ìš”ì•½")
            logger.info("=" * 50)
            logger.info(f"ğŸ“„ ë¬¸ì„œëª…: {document_name}")
            logger.info(f"ğŸ” ìœ„ë²• ì¡°í•­ ë°œê²¬: {len(analysis_result.get('violations', []))}ê°œ")
            logger.info(f"â±ï¸ ì „ì²´ ë¶„ì„ ì‹œê°„: {performance_stats.get('total_time', 0):.2f}ì´ˆ")
            logger.info(f"   - Chain 1 (ìœ„ë²• ì¡°í•­ ì‹ë³„): {performance_stats.get('chain1_time', 0):.2f}ì´ˆ")
            logger.info(f"   - Chain 2 (ë²•ë ¹ ê²€ìƒ‰): {performance_stats.get('chain2_time', 0):.2f}ì´ˆ")  
            logger.info(f"   - Chain 3 (ìƒì„¸ ë¶„ì„): {performance_stats.get('chain3_time', 0):.2f}ì´ˆ")
            
            # ê°œë³„ ìœ„ë²• ì¡°í•­ ìš”ì•½
            if analysis_result.get("violations"):
                logger.info(f"ğŸ“Š ë°œê²¬ëœ ìœ„ë²• ì¡°í•­:")
                for i, violation in enumerate(analysis_result["violations"], 1):
                    logger.info(f"   {i}. {violation.get('ì¡°í•­_ìœ„ì¹˜', '')} - {violation.get('ë¦¬ìŠ¤í¬_ìœ í˜•', '')}")
            
            logger.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {result_file}")
            logger.info("=" * 50)
            
        except Exception as e:
            logger.error(f"âŒ ì²´ì¸ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
     
    async def _save_processing_summary(self):
        """ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì €ì¥"""
        # íƒ€ì…ë³„ í†µê³„
        type_stats = {}
        for result in self.results:
            doc_type = result.doc_type
            if doc_type not in type_stats:
                type_stats[doc_type] = {"total": 0, "success": 0, "failed": 0}
            
            type_stats[doc_type]["total"] += 1
            if result.success:
                type_stats[doc_type]["success"] += 1
            else:
                type_stats[doc_type]["failed"] += 1
        
        summary = {
            "total_files": len(self.results),
            "successful": len([r for r in self.results if r.success]),
            "failed": len([r for r in self.results if not r.success]),
            "total_processing_time": sum(r.processing_time for r in self.results),
            "type_statistics": type_stats,
            "details": [
                {
                    "filename": r.filename,
                    "doc_type": r.doc_type,
                    "folder_path": r.folder_path,
                    "success": r.success,
                    "processing_time": r.processing_time,
                    "page_count": r.page_count,
                    "chunk_count": r.chunk_count,
                    "error": r.error_message
                }
                for r in self.results
            ]
        }
        
        summary_file = RESULTS_DIR / "processing_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“Š ì²˜ë¦¬ ìš”ì•½:")
        logger.info(f"  - ì „ì²´: {summary['total_files']}ê°œ")
        logger.info(f"  - ì„±ê³µ: {summary['successful']}ê°œ")
        logger.info(f"  - ì‹¤íŒ¨: {summary['failed']}ê°œ")
        logger.info(f"  - ì´ ì²˜ë¦¬ ì‹œê°„: {summary['total_processing_time']:.2f}ì´ˆ")
        
        logger.info(f"ğŸ“‹ íƒ€ì…ë³„ í†µê³„:")
        for doc_type, stats in type_stats.items():
            logger.info(f"  - {doc_type}: {stats['success']}/{stats['total']} ì„±ê³µ")
        
        logger.info(f"ğŸ’¾ ìš”ì•½ ì €ì¥: {summary_file}")
    
    async def _save_search_test_results(self, search_results: List[Dict]):
        """ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥"""
        # ê²°ê³¼ ìš”ì•½ ê³„ì‚°
        total_queries = len(search_results)
        successful_queries = len([r for r in search_results if r.get("success", False)])
        failed_queries = total_queries - successful_queries
        
        # ê²€ìƒ‰ ì‹œê°„ í†µê³„
        search_times = [r.get("search_time", 0) for r in search_results if r.get("success")]
        avg_search_time = sum(search_times) / len(search_times) if search_times else 0
        
        # ê²°ê³¼ ìˆ˜ í†µê³„
        result_counts = [r.get("total_results", 0) for r in search_results if r.get("success")]
        avg_results = sum(result_counts) / len(result_counts) if result_counts else 0
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì„±ê³µë¥ 
        category_stats = {}
        for result in search_results:
            category = result.get("category", "unknown")
            if category not in category_stats:
                category_stats[category] = {"total": 0, "success": 0}
            category_stats[category]["total"] += 1
            if result.get("success", False):
                category_stats[category]["success"] += 1
        
        summary = {
            "timestamp": datetime.now().isoformat(),
            "test_summary": {
                "total_queries": total_queries,
                "successful_queries": successful_queries,
                "failed_queries": failed_queries,
                "success_rate": successful_queries / total_queries if total_queries > 0 else 0,
                "avg_search_time_seconds": round(avg_search_time, 3),
                "avg_results_per_query": round(avg_results, 1)
            },
            "category_performance": {
                category: {
                    "success_rate": stats["success"] / stats["total"] if stats["total"] > 0 else 0,
                    "success_count": stats["success"],
                    "total_count": stats["total"]
                }
                for category, stats in category_stats.items()
            },
            "detailed_results": search_results
        }
        
        # ê²°ê³¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = RESULTS_DIR / f"search_test_results_{timestamp}.json"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        # ë¡œê·¸ ì¶œë ¥
        logger.info("=" * 50)
        logger.info("ğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        logger.info("=" * 50)
        logger.info(f"ğŸ“Š ì „ì²´ ì¿¼ë¦¬: {total_queries}ê°œ")
        logger.info(f"âœ… ì„±ê³µ: {successful_queries}ê°œ ({successful_queries/total_queries*100:.1f}%)")
        logger.info(f"âŒ ì‹¤íŒ¨: {failed_queries}ê°œ")
        logger.info(f"â±ï¸ í‰ê·  ê²€ìƒ‰ ì‹œê°„: {avg_search_time:.3f}ì´ˆ")
        logger.info(f"ğŸ“„ í‰ê·  ê²°ê³¼ ìˆ˜: {avg_results:.1f}ê°œ")
        
        logger.info("\nğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ì„±ê³µë¥ :")
        for category, stats in category_stats.items():
            success_rate = stats["success"] / stats["total"] * 100
            logger.info(f"  - {category}: {stats['success']}/{stats['total']} ({success_rate:.1f}%)")
        
        logger.info(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥: {results_file}")
    
    async def _create_manual_sections(self, doc_parser_result: Dict) -> List[Dict]:
        """í•œêµ­ì–´ ì¡°í•­ íŒ¨í„´ì„ ì¸ì‹í•˜ì—¬ ì¡°í•­ë³„ë¡œ ë¶„í•  (ë…ë¦½ í•¨ìˆ˜)"""
        try:
            import re
            
            # ë§ˆí¬ë‹¤ìš´ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
            markdown_content = doc_parser_result.get("markdown_content", "")
            if not markdown_content:
                logger.error("âŒ ë§ˆí¬ë‹¤ìš´ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            logger.info(f"ğŸ“„ ì¡°í•­ ë¶„í•  ì‹œì‘ (ì´ {len(markdown_content)} ë¬¸ì)")
            
            # í•œêµ­ì–´ ì¡°í•­ íŒ¨í„´ ì¸ì‹ ë° ë¶„í• 
            lines = markdown_content.split('\n')
            sections = []
            current_section = {"header_1": "ì„œë¬¸", "content": ""}
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # í—¤ë”© ë˜ëŠ” ì¡°í•­ íŒ¨í„´ ë°œê²¬ ì‹œ ìƒˆë¡œìš´ ì„¹ì…˜ ì‹œì‘
                is_new_section = False
                header_text = ""
                
                if line.startswith('#'):
                    # ë§ˆí¬ë‹¤ìš´ í—¤ë”©
                    is_new_section = True
                    header_text = line.lstrip('#').strip()
                elif re.match(r'^ì œ\s*\d+\s*ì¡°', line):
                    # "ì œXì¡°" ë˜ëŠ” "ì œ X ì¡°" íŒ¨í„´ - ì›ë³¸ ì¡°í•­ ë²ˆí˜¸ ë³´ì¡´
                    is_new_section = True
                    header_text = line  # ì›ë³¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    logger.info(f"ğŸ” ì¡°í•­ íŒ¨í„´ ë°œê²¬: {line}")
                elif line.startswith('Article') or line.startswith('ARTICLE'):
                    # ì˜ë¬¸ ì¡°í•­
                    is_new_section = True
                    header_text = line
                elif any(line.startswith(pattern) for pattern in ['1.', '2.', '3.', '4.', '5.', '6.', '7.', '8.', '9.']):
                    # ë²ˆí˜¸ ë§¤ê¸´ ì¡°í•­ (1., 2., 3. ë“±)
                    if len(line) > 10 and not current_section["content"]:
                        is_new_section = True
                        header_text = line
                
                if is_new_section:
                    # ì´ì „ ì„¹ì…˜ ì €ì¥
                    if current_section["content"].strip():
                        current_section["chunk_type"] = "parent"
                        sections.append(current_section)
                    
                    # ìƒˆ ì„¹ì…˜ ì‹œì‘ - ì›ë³¸ ì¡°í•­ ë²ˆí˜¸ ë³´ì¡´
                    current_section = {
                        "header_1": header_text,  # ì›ë³¸ ì¡°í•­ ë²ˆí˜¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        "content": "",
                        "chunk_type": "parent"
                    }
                else:
                    # ë‚´ìš© ì¶”ê°€
                    if current_section["content"]:
                        current_section["content"] += "\n"
                    current_section["content"] += line
            
            # ë§ˆì§€ë§‰ ì„¹ì…˜ ì €ì¥
            if current_section["content"].strip():
                current_section["chunk_type"] = "parent"
                sections.append(current_section)
            
            # í—¤ë”©ì´ ì—†ìœ¼ë©´ ë¬¸ë‹¨ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
            if not sections:
                logger.info("ğŸ“„ í—¤ë”©ì´ ì—†ì–´ ë¬¸ë‹¨ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.")
                paragraphs = [p.strip() for p in markdown_content.split('\n\n') if p.strip()]
                
                for i, paragraph in enumerate(paragraphs[:10], 1):  # ìµœëŒ€ 10ê°œ ë¬¸ë‹¨
                    if len(paragraph) > 50:  # ë„ˆë¬´ ì§§ì€ ë¬¸ë‹¨ ì œì™¸
                        sections.append({
                            "header_1": f"ì¡°í•­ {i}",
                            "content": paragraph,
                            "chunk_type": "parent"
                        })
            
            logger.info(f"âœ… {len(sections)}ê°œ ì¡°í•­ìœ¼ë¡œ ë¶„í•  ì™„ë£Œ")
            
            # ê° ì„¹ì…˜ ë¯¸ë¦¬ë³´ê¸°
            for i, section in enumerate(sections[:3], 1):
                preview = section["content"][:100] + "..." if len(section["content"]) > 100 else section["content"]
                logger.info(f"  {i}. {section['header_1']}: {preview}")
                
            return sections
            
        except Exception as e:
            logger.error(f"âŒ ì¡°í•­ ë¶„í•  ì‹¤íŒ¨: {str(e)}")
            return []
    
    async def _perform_contract_review(self, section_title: str, section_content: str, document_name: str, all_sections: List[Dict] = None, current_section_index: int = 1, document_id: Optional[int] = None) -> Dict:
        """ê³„ì•½ì„œ ì¡°í•­ ê²€í†  ìˆ˜í–‰"""
        try:
            # 1. ê´€ë ¨ ë²•ë ¹ ê²€ìƒ‰
            search_query = f"{section_title} {section_content[:200]}"  # ì¡°í•­ ì œëª©ê³¼ ë‚´ìš© ì¼ë¶€ë¡œ ê²€ìƒ‰
            
            legal_docs = await self.client.search_documents_direct(
                query=search_query,
                top_k=3,
                doc_types=["law"]  # ë²•ë ¹ë§Œ ê²€ìƒ‰
            )
            
            # 2. ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ ê´€ë ¨ ì¡°í•­ ê²€ìƒ‰
            contract_context = ""
            if all_sections:
                # ì „ì²´ êµ¬ì¡° ê°œìš” (ì œëª©ë§Œ)
                contract_overview = []
                for idx, section in enumerate(all_sections, 1):
                    title = section.get("header_1", f"ì¡°í•­ {idx}")
                    marker = " â† [í˜„ì¬ ê²€í†  ì¤‘]" if idx == current_section_index else ""
                    contract_overview.append(f"{idx}. {title}{marker}")
                
                # í˜„ì¬ ì¡°í•­ê³¼ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ì¡°í•­ë“¤ ê²€ìƒ‰ (document_id ê¸°ë°˜)
                similar_clauses = []
                if document_id:
                    try:
                        from src.aws.embedding_service import TitanEmbeddingService
                        from sqlmodel.ext.asyncio.session import AsyncSession
                        from sqlalchemy.ext.asyncio import create_async_engine
                        from sqlalchemy import text
                        
                        # í˜„ì¬ ì¡°í•­ ì„ë² ë”© ìƒì„±
                        embedding_service = TitanEmbeddingService()
                        current_clause_embedding = await embedding_service.create_single_embedding(section_content)
                        
                        # ê°™ì€ ë¬¸ì„œ ë‚´ì—ì„œ ìœ ì‚¬í•œ ì¡°í•­ ê²€ìƒ‰ (í˜„ì¬ ì¡°í•­ ì œì™¸)
                        database_url = os.getenv("DATABASE_URL") or f"postgresql+asyncpg://{os.getenv('DATABASE_USER', 'postgres')}:{os.getenv('DATABASE_PASSWORD', 'postgres')}@{os.getenv('DATABASE_HOST', '127.0.0.1')}:{os.getenv('DATABASE_PORT', '5434')}/{os.getenv('DATABASE_NAME', 'smartclm-poc')}"
                        async_engine = create_async_engine(database_url, echo=False)
                        
                        async with AsyncSession(async_engine) as session:
                            # ë²¡í„° ê²€ìƒ‰ ì¿¼ë¦¬ (document_id ê¸°ë°˜, í˜„ì¬ ì¡°í•­ ì œì™¸)
                            vector_str = "[" + ",".join(map(str, current_clause_embedding)) + "]"
                            similarity_query = """
                            SELECT 
                                c.header_1,
                                c.content,
                                c.chunk_index,
                                (1 - (c.embedding <=> :query_embedding)) as similarity_score
                            FROM chunks c
                            WHERE c.embedding IS NOT NULL
                            AND c.chunk_type = 'parent'
                            AND c.document_id = :document_id
                            AND c.chunk_index != :current_index
                            ORDER BY similarity_score DESC
                            LIMIT 3
                            """
                            
                            connection = await session.connection()
                            result = await connection.execute(text(similarity_query), {
                                "query_embedding": vector_str,
                                "document_id": document_id,
                                "current_index": current_section_index - 1  # 0-based
                            })
                            similar_clauses = result.fetchall()
                            
                        # ê´€ë ¨ ì¡°í•­ë“¤ í¬ë§·
                        related_sections = []
                        for clause in similar_clauses:
                            similarity = clause.similarity_score
                            clause_num = clause.chunk_index + 1  # 1-basedë¡œ ë³€í™˜
                            title = clause.header_1 or f"ì¡°í•­ {clause_num}"
                            content = clause.content[:300] + "..." if len(clause.content) > 300 else clause.content
                            
                            related_sections.append(f"""**ì¡°í•­ {clause_num}: {title}** (ìœ ì‚¬ë„: {similarity:.3f})
{content}""")
                        
                        logger.info(f"ğŸ”— ê´€ë ¨ ì¡°í•­ ê²€ìƒ‰ ì™„ë£Œ: {len(similar_clauses)}ê°œ ì¡°í•­ ë°œê²¬")
                        
                    except Exception as e:
                        logger.warning(f"âš ï¸ ê´€ë ¨ ì¡°í•­ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}, ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ëŒ€ì²´")
                        # ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´
                        similar_clauses = []
                        window_size = 2
                        start_idx = max(0, current_section_index - 1 - window_size)
                        end_idx = min(len(all_sections), current_section_index + window_size)
                        
                        related_sections = []
                        for idx in range(start_idx, end_idx):
                            if idx != current_section_index - 1:  # í˜„ì¬ ì¡°í•­ ì œì™¸
                                section = all_sections[idx]
                                section_num = idx + 1
                                title = section.get("header_1", f"ì¡°í•­ {section_num}")
                                content = section.get("content", "")[:300] + "..." if len(section.get("content", "")) > 300 else section.get("content", "")
                                
                                related_sections.append(f"""**ì¡°í•­ {section_num}: {title}** (ì¸ì ‘ ì¡°í•­)
{content}""")
                
                else:
                    # document_idê°€ ì—†ëŠ” ê²½ìš° ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì‚¬ìš©
                    logger.info("ğŸ”— document_id ì—†ìŒ, ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ ì‚¬ìš©")
                    similar_clauses = []
                    window_size = 2
                    start_idx = max(0, current_section_index - 1 - window_size)
                    end_idx = min(len(all_sections), current_section_index + window_size)
                    
                    related_sections = []
                    for idx in range(start_idx, end_idx):
                        if idx != current_section_index - 1:  # í˜„ì¬ ì¡°í•­ ì œì™¸
                            section = all_sections[idx]
                            section_num = idx + 1
                            title = section.get("header_1", f"ì¡°í•­ {section_num}")
                            content = section.get("content", "")[:300] + "..." if len(section.get("content", "")) > 300 else section.get("content", "")
                            
                            related_sections.append(f"""**ì¡°í•­ {section_num}: {title}** (ì¸ì ‘ ì¡°í•­)
{content}""")
                
                contract_context = f"""
**ê³„ì•½ì„œ ì „ì²´ êµ¬ì¡° ({document_name}):**
{chr(10).join(contract_overview)}

**ê²€í†  ë²”ìœ„:**
- ì´ {len(all_sections)}ê°œ ì¡°í•­ ì¤‘ í˜„ì¬ ì¡°í•­: {current_section_index}/{len(all_sections)}
- ê³„ì•½ ìœ í˜•: ë¹„ë°€ìœ ì§€ê³„ì•½ì„œ (NDA)

**ğŸ”— ê³„ì•½ì„œ ë‚´ ê´€ë ¨ ì¡°í•­ (ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜):**

{chr(10).join(related_sections) if related_sections else "ê´€ë ¨ ì¡°í•­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
"""

            # 3. ê°œì„ ëœ ê²€í†  í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt_template = """# ğŸ›ï¸ ì „ë¬¸ ê³„ì•½ì„œ ì¡°í•­ ë²•ì  ê²€í†  ë° ìœ„í—˜ ë¶„ì„

ë‹¹ì‹ ì€ ê³„ì•½ë²•, ë¯¼ë²•, ìƒë²•, ê°œì¸ì •ë³´ë³´í˜¸ë²• ë“±ì— ì •í†µí•œ ë³€í˜¸ì‚¬ë¡œì„œ ê³„ì•½ì„œ ì¡°í•­ì˜ ë²•ì  ìœ„í—˜ì„ ë¶„ì„í•˜ê³  ì‹¤ë¬´ì  ê°œì„ ë°©ì•ˆì„ ì œì‹œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

{contract_context}

## ğŸ“‹ í˜„ì¬ ê²€í†  ëŒ€ìƒ ì¡°í•­

**ì¡°í•­ ì œëª©:** {section_title}
**ì¡°í•­ ë‚´ìš©:**
```
{section_content}
```

{legal_references}

## ğŸ” ìƒì„¸ ë²•ì  ê²€í†  ìš”êµ¬ì‚¬í•­

### 1. ë²•ì  ìœ íš¨ì„± ë¶„ì„
- í•´ë‹¹ ì¡°í•­ì˜ ë²•ì  êµ¬ì†ë ¥ ì—¬ë¶€
- ë¯¼ë²•, ìƒë²• ë“± ê¸°ë³¸ë²•ê³¼ì˜ í•©ì¹˜ì„±
- ê°•í–‰ë²•ê·œ ìœ„ë°˜ ê°€ëŠ¥ì„± ê²€í† 
- ê³µì„œì–‘ì† ìœ„ë°˜ ì—¬ë¶€

### 2. ìœ„í—˜ìš”ì†Œ ë° ë¶„ìŸ ê°€ëŠ¥ì„±
- ì¡°í•­ í•´ì„ìƒ ëª¨í˜¸í•œ ë¶€ë¶„
- ì¼ë°© ë‹¹ì‚¬ìì—ê²Œ ê³¼ë„í•˜ê²Œ ë¶ˆë¦¬í•œ ì¡°í•­
- ë¶ˆê³µì •ì•½ê´€ í•´ë‹¹ ê°€ëŠ¥ì„±
- ì‹¤ì œ ë¶„ìŸ ë°œìƒ ì‹œ ì˜ˆìƒ ìŸì 

### 3. ì „ì²´ ê³„ì•½ì„œì™€ì˜ ì¼ê´€ì„±
- ë‹¤ë¥¸ ì¡°í•­ê³¼ì˜ ìƒì¶© ì—¬ë¶€
- ê³„ì•½ì„œ ì „ì²´ ëª©ì ê³¼ì˜ ì •í•©ì„±
- ì¡°í•­ ê°„ ì—°ê³„ì„± ë° ë³´ì™„ì„±

### 4. ì—…ê³„ í‘œì¤€ ë° ëª¨ë²”ê´€í–‰ ë¹„êµ
- ë™ì¢… ì—…ê³„ í‘œì¤€ ê³„ì•½ì„œì™€ì˜ ë¹„êµ
- ì¼ë°˜ì  ìƒê´€ë¡€ì™€ì˜ ë¶€í•©ì„±
- ë²•ì› íŒë¡€ ë™í–¥ê³¼ì˜ ì¼ì¹˜ì„±

## ğŸ“ ë¶„ì„ ê²°ê³¼ ì‘ì„± í˜•ì‹

### âš–ï¸ ë²•ì  ìœ íš¨ì„±
[ì¡°í•­ì˜ ë²•ì  íš¨ë ¥, ê°•í–‰ë²•ê·œ ìœ„ë°˜ ì—¬ë¶€, ê³µì„œì–‘ì† ì í•©ì„± ë“±]

### âš ï¸ ìœ„í—˜ìš”ì†Œ
[êµ¬ì²´ì  ìœ„í—˜ ì‚¬í•­ì„ ë²ˆí˜¸ë¥¼ ë§¤ê²¨ ë‚˜ì—´]
1. 
2. 
3. 

### ğŸ“– ê´€ë ¨ ë²•ë ¹ ìœ„ë°˜ ê²€í† 
[í•´ë‹¹ ë²•ë ¹ ì¡°í•­ê³¼ ìœ„ë°˜ ê°€ëŠ¥ì„±, ì œì¬ ìˆ˜ì¤€]

### ğŸ”§ ê°œì„  ê¶Œê³ ì‚¬í•­
[êµ¬ì²´ì ì´ê³  ì‹¤ë¬´ì ì¸ ì¡°í•­ ê°œì„  ë°©ì•ˆ]
1. 
2. 
3. 

### ğŸ“Š ì¢…í•© ìœ„í—˜ë„ í‰ê°€
**ìœ„í—˜ë„:** [ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ/ì•ˆì „]

### ğŸ¯ í•µì‹¬ ê¶Œê³ ì‚¬í•­
[ê°€ì¥ ìš°ì„ ì ìœ¼ë¡œ ê°œì„ í•´ì•¼ í•  ì‚¬í•­ 3ê°€ì§€]

### ğŸ“‹ ë²•ì  ê·¼ê±° ìš”ì•½
[ì¸ìš©ëœ ë²•ë ¹ ë° ì¡°í•­ì˜ í•µì‹¬ ë‚´ìš©]

ê³„ì•½ì„œ ì „ì²´ì˜ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ í•´ë‹¹ ì¡°í•­ì´ ê³„ì•½ ëª©ì  ë‹¬ì„±ì— ì í•©í•œì§€, ë‹¹ì‚¬ì ê°„ ê¶Œë¦¬ì˜ë¬´ê°€ ê· í˜•ìˆê²Œ ë°°ë¶„ë˜ì—ˆëŠ”ì§€ ì¢…í•©ì ìœ¼ë¡œ ê²€í† í•´ì£¼ì„¸ìš”."""

            # 4. ë²•ë ¹ ê·¼ê±° í…ìŠ¤íŠ¸ êµ¬ì„±
            legal_references = "## âŒ ê´€ë ¨ ë²•ë ¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nê²€ìƒ‰ëœ ë²•ë ¹ì´ ì—†ì–´ ì¼ë°˜ì ì¸ ë²•ë¦¬ì— ë”°ë¼ ê²€í† ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤."
            if legal_docs.get("results"):
                references = []
                for i, doc in enumerate(legal_docs["results"], 1):
                    filename = doc['filename'].replace('.pdf', '')
                    content = doc['content'][:500] + "..." if len(doc['content']) > 500 else doc['content']
                    similarity = f"(ìœ ì‚¬ë„: {doc.get('similarity_score', 0):.3f})"
                    references.append(f"### ğŸ“– {i}. {filename} {similarity}\n```\n{content}\n```")
                legal_references = "## ğŸ“š ê´€ë ¨ ë²•ë ¹ ë° íŒë¡€ ê·¼ê±° (ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰)\n\n" + "\n\n".join(references)
            else:
                legal_references = "## ğŸ“š ê´€ë ¨ ë²•ë ¹ ë° íŒë¡€ ê·¼ê±° (ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰)\n\nâŒ ê´€ë ¨ ë²•ë ¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nê²€ìƒ‰ëœ ë²•ë ¹ì´ ì—†ì–´ ì¼ë°˜ì ì¸ ë²•ë¦¬ì— ë”°ë¼ ê²€í† ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤."
            
            # 5. ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸
            full_prompt = prompt_template.format(
                contract_context=contract_context,
                section_title=section_title,
                section_content=section_content,
                legal_references=legal_references
            )
            
            # 6. LLM í˜¸ì¶œ (ë¹„ìŠ¤íŠ¸ë¦¬ë°) - ë” ìƒì„¸í•œ ë¶„ì„ì„ ìœ„í•´ í† í° ìˆ˜ ì¦ê°€
            from src.aws.bedrock_service import BedrockService
            bedrock_service = BedrockService()
            
            # í”„ë¡¬í”„íŠ¸ í¬ê¸° ì¸¡ì •
            prompt_chars = len(full_prompt)
            estimated_input_tokens = prompt_chars // 4  # ëŒ€ëµì ì¸ í† í° ì¶”ì • (1í† í° â‰ˆ 4ê¸€ì)
            
            logger.info(f"ğŸ¤– AI ì¡°í•­ ê²€í†  ì‹œì‘: {section_title}")
            logger.info(f"   ğŸ“ í”„ë¡¬í”„íŠ¸ í¬ê¸°: {prompt_chars:,}ì (â‰ˆ{estimated_input_tokens:,} í† í°)")
            
            response = bedrock_service.invoke_model(
                prompt=full_prompt,
                max_tokens=4000,  # ë” ìƒì„¸í•œ ë¶„ì„ì„ ìœ„í•´ ì¦ê°€
                temperature=0.0
            )
            
            # í† í° ì‚¬ìš©ëŸ‰ ìƒì„¸ ë¡œê·¸
            token_usage = response.get("usage", {})
            processing_time = response.get("timing", {}).get("processing_time_ms", 0)
            input_tokens = token_usage.get("input_tokens", 0)
            output_tokens = token_usage.get("output_tokens", 0)
            total_tokens = token_usage.get("total_tokens", 0)
            
            logger.info(f"âœ… AI ê²€í†  ì™„ë£Œ: {section_title} ({processing_time}ms)")
            logger.info(f"   ğŸ“Š í† í° ì‚¬ìš©ëŸ‰: ì…ë ¥ {input_tokens:,} + ì¶œë ¥ {output_tokens:,} = ì´ {total_tokens:,}")
            
            # 7. ê²°ê³¼ êµ¬ì„±
            review_text = response.get("text", "") if isinstance(response, dict) else str(response)
            
            return {
                "section_title": section_title,
                "section_content": section_content[:500] + "..." if len(section_content) > 500 else section_content,
                "legal_references_count": len(legal_docs.get("results", [])),
                "legal_references": legal_docs.get("results", []),
                "review_analysis": review_text,
                "contract_context_provided": all_sections is not None,
                "total_sections_in_contract": len(all_sections) if all_sections else 0,
                "current_section_index": current_section_index,
                "token_usage": token_usage,
                "processing_time_ms": processing_time,
                "prompt_size": {
                    "characters": prompt_chars,
                    "estimated_tokens": estimated_input_tokens
                },
                "optimization_info": {
                    "related_sections_count": len(related_sections) if 'related_sections' in locals() else 0,
                    "total_sections_available": len(all_sections) if all_sections else 0,
                    "search_method": "semantic_similarity" if document_id and similar_clauses else "sliding_window_fallback",
                    "semantic_similarity_count": len(similar_clauses) if 'similar_clauses' in locals() else 0,
                    "document_id_provided": document_id is not None
                },
                "success": True
            }
            
        except Exception as e:
            logger.error(f"âŒ ê³„ì•½ì„œ ê²€í†  ìˆ˜í–‰ ì‹¤íŒ¨: {str(e)}")
            return {
                "section_title": section_title,
                "success": False,
                "error": str(e)
            }
    
    async def _save_contract_review_results(self, document_name: str, review_results: List[Dict]):
        """ê³„ì•½ì„œ ê²€í†  ê²°ê³¼ ì €ì¥"""
        # ê²°ê³¼ ìš”ì•½ ê³„ì‚°
        total_sections = len(review_results)
        successful_reviews = len([r for r in review_results if r.get("success", False)])
        failed_reviews = total_sections - successful_reviews
        
        # ê²€í†  ì‹œê°„ í†µê³„
        review_times = [r.get("processing_time_ms", 0) for r in review_results if r.get("success")]
        avg_review_time = sum(review_times) / len(review_times) if review_times else 0
        
        # ë²•ë ¹ ì°¸ì¡° í†µê³„
        legal_ref_counts = [r.get("legal_references_count", 0) for r in review_results if r.get("success")]
        avg_legal_refs = sum(legal_ref_counts) / len(legal_ref_counts) if legal_ref_counts else 0
        
        # í† í° ì‚¬ìš©ëŸ‰ í†µê³„
        successful_results = [r for r in review_results if r.get("success", False)]
        if successful_results:
            total_input_tokens = sum(r.get("token_usage", {}).get("input_tokens", 0) for r in successful_results)
            total_output_tokens = sum(r.get("token_usage", {}).get("output_tokens", 0) for r in successful_results)
            total_tokens = sum(r.get("token_usage", {}).get("total_tokens", 0) for r in successful_results)
            avg_input_tokens = total_input_tokens / len(successful_results)
            avg_output_tokens = total_output_tokens / len(successful_results)
            avg_total_tokens = total_tokens / len(successful_results)
            
            # í”„ë¡¬í”„íŠ¸ í¬ê¸° í†µê³„
            prompt_sizes = [r.get("prompt_size", {}).get("characters", 0) for r in successful_results]
            avg_prompt_size = sum(prompt_sizes) / len(prompt_sizes) if prompt_sizes else 0
            
            # ê´€ë ¨ ì¡°í•­ íš¨ìœ¨ì„± í†µê³„
            related_counts = [r.get("optimization_info", {}).get("related_sections_count", 0) for r in successful_results]
            avg_related_sections = sum(related_counts) / len(related_counts) if related_counts else 0
            
            # ê²€ìƒ‰ ë°©ë²• í†µê³„
            search_methods = [r.get("optimization_info", {}).get("search_method", "unknown") for r in successful_results]
            semantic_count = search_methods.count("semantic_similarity")
            fallback_count = search_methods.count("sliding_window_fallback")
        else:
            total_input_tokens = total_output_tokens = total_tokens = 0
            avg_input_tokens = avg_output_tokens = avg_total_tokens = 0
            avg_prompt_size = avg_related_sections = 0
            semantic_count = fallback_count = 0
        
        summary = {
            "timestamp": datetime.now().isoformat(),
            "document_name": document_name,
            "review_summary": {
                "total_sections": total_sections,
                "successful_reviews": successful_reviews,
                "failed_reviews": failed_reviews,
                "success_rate": successful_reviews / total_sections if total_sections > 0 else 0,
                "avg_review_time_ms": round(avg_review_time, 1),
                "avg_legal_references": round(avg_legal_refs, 1),
                "token_usage": {
                    "total_input_tokens": total_input_tokens,
                    "total_output_tokens": total_output_tokens,
                    "total_tokens": total_tokens,
                    "avg_input_tokens": round(avg_input_tokens, 1),
                    "avg_output_tokens": round(avg_output_tokens, 1),
                    "avg_total_tokens": round(avg_total_tokens, 1)
                },
                "optimization": {
                    "avg_prompt_size_chars": round(avg_prompt_size, 1),
                    "avg_related_sections": round(avg_related_sections, 1),
                    "semantic_search_count": semantic_count,
                    "fallback_count": fallback_count,
                    "optimization_method": "semantic_similarity"
                }
            },
            "detailed_results": review_results
        }
        
        # ê²°ê³¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = RESULTS_DIR / f"contract_review_results_{timestamp}.json"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        # ë¡œê·¸ ì¶œë ¥
        logger.info("=" * 50)
        logger.info("ğŸ“ ê³„ì•½ì„œ ê²€í†  ê²°ê³¼ ìš”ì•½ (ì˜ë¯¸ì  ìœ ì‚¬ë„ ìµœì í™”)")
        logger.info("=" * 50)
        logger.info(f"ğŸ“„ ë¬¸ì„œëª…: {document_name}")
        logger.info(f"ğŸ“Š ì´ ì¡°í•­: {total_sections}ê°œ")
        logger.info(f"âœ… ì„±ê³µ: {successful_reviews}ê°œ ({successful_reviews/total_sections*100:.1f}%)")
        logger.info(f"âŒ ì‹¤íŒ¨: {failed_reviews}ê°œ")
        logger.info(f"â±ï¸ í‰ê·  ê²€í†  ì‹œê°„: {avg_review_time:.1f}ms")
        logger.info(f"ğŸ“– í‰ê·  ë²•ë ¹ ì°¸ì¡°: {avg_legal_refs:.1f}ê°œ")
        logger.info(f"ğŸ¯ í† í° ì‚¬ìš©ëŸ‰:")
        logger.info(f"   - ì´ ì…ë ¥: {total_input_tokens:,} (í‰ê· : {avg_input_tokens:.1f})")
        logger.info(f"   - ì´ ì¶œë ¥: {total_output_tokens:,} (í‰ê· : {avg_output_tokens:.1f})")
        logger.info(f"   - ì´ ì‚¬ìš©: {total_tokens:,} (í‰ê· : {avg_total_tokens:.1f})")
        logger.info(f"ğŸ“ ìµœì í™” íš¨ê³¼:")
        logger.info(f"   - í‰ê·  í”„ë¡¬í”„íŠ¸ í¬ê¸°: {avg_prompt_size:,.0f}ì")
        logger.info(f"   - í‰ê·  ê´€ë ¨ ì¡°í•­ ìˆ˜: {avg_related_sections:.1f}ê°œ")
        logger.info(f"   - ì˜ë¯¸ì  ê²€ìƒ‰ ì„±ê³µ: {semantic_count}ê°œ")
        logger.info(f"   - ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ëŒ€ì²´: {fallback_count}ê°œ")
        
        # ì„±ê³µí•œ ê²€í†  ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
        if successful_reviews > 0:
            logger.info(f"\nğŸ“‹ ê²€í†  ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:")
            for result in review_results:
                if result.get("success"):
                    analysis = result.get("review_analysis", "")
                    # ìœ„í—˜ë„ ì¶”ì¶œ ì‹œë„
                    risk_level = "ë¶„ì„ ì¤‘"
                    if "ìœ„í—˜ë„:" in analysis:
                        risk_line = [line for line in analysis.split('\n') if 'ìœ„í—˜ë„:' in line]
                        if risk_line:
                            risk_level = risk_line[0].split('ìœ„í—˜ë„:')[-1].strip()
                    
                    logger.info(f"  - {result['section_title']}: ìœ„í—˜ë„ {risk_level}")
        
        logger.info(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥: {results_file}")
    
    async def step2_test_search(self):
        """Step 2: ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ” Step 2: ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì •ì˜
        test_queries = [
            # ë²•ë ¹ ê´€ë ¨ ì¿¼ë¦¬
            {
                "query": "ê°œì¸ì •ë³´ë³´í˜¸ë²•ì˜ ì£¼ìš” ë‚´ìš©ì€?",
                "category": "law",
                "expected_doc_type": "law"
            },
            {
                "query": "ì „ììƒê±°ë˜ ì†Œë¹„ìë³´í˜¸ì— ê´€í•œ ë²•ë¥ ",
                "category": "law", 
                "expected_doc_type": "law"
            },
            # ê³„ì•½ì„œ ê´€ë ¨ ì¿¼ë¦¬
            {
                "query": "ê³„ì•½í•´ì§€ ì¡°ê±´ê³¼ ì ˆì°¨",
                "category": "contract",
                "expected_doc_type": ["executed_contract", "standard_contract"]
            },
            {
                "query": "ì†í•´ë°°ìƒ ì±…ì„ ë²”ìœ„",
                "category": "contract",
                "expected_doc_type": ["executed_contract", "standard_contract"]
            },
            {
                "query": "ê³„ì•½ ê¸°ê°„ ë° ê°±ì‹  ì¡°ê±´",
                "category": "contract", 
                "expected_doc_type": ["executed_contract", "standard_contract"]
            },
            # ì¼ë°˜ì ì¸ ì§ˆë¬¸
            {
                "query": "ê³„ì•½ì„œ ì‘ì„± ì‹œ ì£¼ì˜ì‚¬í•­",
                "category": "general",
                "expected_doc_type": "any"
            }
        ]
        
        search_results = []
        
        # ê° ì¿¼ë¦¬ì— ëŒ€í•´ ê²€ìƒ‰ ìˆ˜í–‰
        for i, test_case in enumerate(test_queries, 1):
            logger.info(f"ğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ {i}/{len(test_queries)}: {test_case['query']}")
            
            try:
                # ì§ì ‘ ê²€ìƒ‰ í˜¸ì¶œ
                start_time = time.time()
                result = await self.client.search_documents_direct(
                    query=test_case["query"],
                    top_k=5,
                    doc_types=None  # ëª¨ë“  ë¬¸ì„œ íƒ€ì…ì—ì„œ ê²€ìƒ‰
                )
                search_time = time.time() - start_time
                
                # ê²°ê³¼ ë¶„ì„
                search_result = {
                    "query": test_case["query"],
                    "category": test_case["category"],
                    "expected_doc_type": test_case["expected_doc_type"],
                    "search_time": search_time,
                    "total_results": len(result.get("results", [])),
                    "results": result.get("results", []),
                    "success": True
                }
                
                # ê²°ê³¼ í’ˆì§ˆ í‰ê°€
                if result.get("results"):
                    # ë¬¸ì„œ íƒ€ì… ë¶„í¬ í™•ì¸
                    doc_types = [r.get("doc_type") for r in result["results"]]
                    search_result["doc_type_distribution"] = {
                        doc_type: doc_types.count(doc_type) 
                        for doc_type in set(doc_types) if doc_type
                    }
                    
                    # í‰ê·  ì ìˆ˜ ê³„ì‚°
                    scores = [r.get("similarity_score", 0) for r in result["results"]]
                    search_result["avg_similarity_score"] = sum(scores) / len(scores) if scores else 0
                    search_result["max_similarity_score"] = max(scores) if scores else 0
                    search_result["min_similarity_score"] = min(scores) if scores else 0
                
                logger.info(f"  âœ… ê²€ìƒ‰ ì™„ë£Œ: {search_result['total_results']}ê°œ ê²°ê³¼, {search_time:.2f}ì´ˆ")
                if search_result.get("doc_type_distribution"):
                    for doc_type, count in search_result["doc_type_distribution"].items():
                        logger.info(f"    - {doc_type}: {count}ê°œ")
                
            except Exception as e:
                logger.error(f"  âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
                search_result = {
                    "query": test_case["query"],
                    "category": test_case["category"],
                    "expected_doc_type": test_case["expected_doc_type"],
                    "error": str(e),
                    "success": False
                }
            
            search_results.append(search_result)
            
            # ìš”ì²­ ê°„ ì ì‹œ ëŒ€ê¸°
            await asyncio.sleep(0.5)
        
        # ê²°ê³¼ ìš”ì•½ ë° ì €ì¥
        await self._save_search_test_results(search_results)
        
        logger.info("ğŸ‰ Step 2 ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
    
    async def step3_contract_review_test(self):
        """Step 3: ê³„ì•½ì„œ ê²€í†  ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ (3ë‹¨ê³„ ì²´ì¸ ë°©ì‹)"""
        logger.info("ğŸ“ Step 3: ê³„ì•½ì„œ ê²€í†  ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘ (ì²´ì¸ ë°©ì‹)")
        
        # 1. í…ŒìŠ¤íŠ¸ ëŒ€ìƒ ë¬¸ì„œ ì„ íƒ
        test_document_path = DOCS_ROOT / "1. NDA" / "ì´ˆì•ˆ_ë¹„ë°€ìœ ì§€ê³„ì•½ì„œ.docx"
        
        if not test_document_path.exists():
            logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤: {test_document_path}")
            return False
        
        logger.info(f"ğŸ“„ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ: {test_document_path.name}")
        
        # 2. ê³„ì•½ì„œë¥¼ DBì— ì €ì¥ (ì¤‘ë³µ ì²´í¬ í¬í•¨)
        logger.info("ğŸ’¾ ê³„ì•½ì„œ DB ì €ì¥ í™•ì¸/ì²˜ë¦¬ ì¤‘...")
        document_id = await self._ensure_contract_in_db(test_document_path)
        
        if not document_id:
            logger.error("âŒ ê³„ì•½ì„œ DB ì €ì¥ ì‹¤íŒ¨")
            return False
        
        logger.info(f"âœ… ê³„ì•½ì„œ DB ì €ì¥ ì™„ë£Œ (Document ID: {document_id})")
        
        # 3. DBì—ì„œ ê³„ì•½ì„œ ì •ë³´ ì¡°íšŒ
        document_info = await self._get_document_from_db(document_id)
        if not document_info:
            logger.error("âŒ DBì—ì„œ ê³„ì•½ì„œ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨")
            return False
        
        # 4. ê³„ì•½ì„œ ì¡°í•­ ì¡°íšŒ
        chunks = await self._get_document_chunks_from_db(document_id)
        parent_chunks = [c for c in chunks if c.get("chunk_type") == "parent"]
        
        logger.info(f"ğŸ” DBì—ì„œ ì¡°íšŒí•œ ì²­í¬ ìˆ˜: {len(chunks)}")
        logger.info(f"ğŸ” Parent ì²­í¬ ìˆ˜: {len(parent_chunks)}")
        
        if not parent_chunks:
            logger.error("âŒ ë¶„ì„í•  ì¡°í•­ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        logger.info(f"ğŸ“Š ì´ {len(parent_chunks)}ê°œ ì¡°í•­ ë°œê²¬")
        
        try:
            # === 3ë‹¨ê³„ ì²´ì¸ ë¶„ì„ ì‹œì‘ ===
            
            # Chain 1: ì „ì²´ ê³„ì•½ì„œ ìŠ¤ìº” â†’ ìœ„ë²• ì¡°í•­ ì‹ë³„
            logger.info("ğŸ” Chain 1: ì „ì²´ ê³„ì•½ì„œ ìœ„ë²• ì¡°í•­ ì‹ë³„ ì¤‘...")
            start_time = time.time()
            
            violation_candidates = await self._chain1_identify_violations(
                document_name=test_document_path.name,
                all_chunks=parent_chunks,
                document_id=document_id
            )
            
            chain1_time = time.time() - start_time
            logger.info(f"âœ… Chain 1 ì™„ë£Œ: {len(violation_candidates)}ê°œ ìœ„ë²• ì¡°í•­ í›„ë³´ ë°œê²¬ ({chain1_time:.2f}ì´ˆ)")
            
            if not violation_candidates:
                logger.info("âœ… ìœ„ë²• ì¡°í•­ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                await self._save_chain_analysis_results(test_document_path.name, {"violations": []}, {
                    "chain1_time": chain1_time,
                    "chain2_time": 0,
                    "chain3_time": 0,
                    "total_time": chain1_time,
                    "violations_found": 0,
                    "total_clauses": len(parent_chunks)
                })
                return True
            
            # Chain 2: ê° ìœ„ë²• ì¡°í•­ë³„ ê´€ë ¨ ë²•ë ¹ ê²€ìƒ‰ (ë³‘ë ¬ ì²˜ë¦¬)
            logger.info("ğŸ” Chain 2: ìœ„ë²• ì¡°í•­ë³„ ê´€ë ¨ ë²•ë ¹ ê²€ìƒ‰ ì¤‘...")
            start_time = time.time()
            
            violations_with_laws = await self._chain2_search_related_laws(violation_candidates)
            
            chain2_time = time.time() - start_time
            logger.info(f"âœ… Chain 2 ì™„ë£Œ: {len(violations_with_laws)}ê°œ ì¡°í•­ì˜ ë²•ë ¹ ê²€ìƒ‰ ì™„ë£Œ ({chain2_time:.2f}ì´ˆ)")
            
            # Chain 3: ìµœì¢… ìƒì„¸ ë¶„ì„
            logger.info("ğŸ” Chain 3: ìµœì¢… ìƒì„¸ ë¶„ì„ ì¤‘...")
            start_time = time.time()
            
            final_analysis = await self._chain3_detailed_analysis(
                violations_with_laws=violations_with_laws,
                document_name=test_document_path.name
            )
            
            chain3_time = time.time() - start_time
            logger.info(f"âœ… Chain 3 ì™„ë£Œ: ìµœì¢… ë¶„ì„ ì™„ë£Œ ({chain3_time:.2f}ì´ˆ)")
            
            # ê²°ê³¼ ì €ì¥
            await self._save_chain_analysis_results(test_document_path.name, final_analysis, {
                "chain1_time": chain1_time,
                "chain2_time": chain2_time, 
                "chain3_time": chain3_time,
                "total_time": chain1_time + chain2_time + chain3_time,
                "violations_found": len(final_analysis.get("violations", [])),
                "total_clauses": len(parent_chunks)
            })
            
            logger.info("ğŸ‰ Step 3 ê³„ì•½ì„œ ê²€í†  í…ŒìŠ¤íŠ¸ ì™„ë£Œ (ì²´ì¸ ë°©ì‹)!")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì²´ì¸ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return False
    
    async def step4_streamlit_visualization(self):
        """Step 4: Streamlit ê²°ê³¼ ì‹œê°í™”"""
        logger.info("ğŸ“ˆ Step 4: Streamlit ê²°ê³¼ ì‹œê°í™” ì‹œì‘")
        # TODO: Streamlit ì‹œê°í™” êµ¬í˜„
        pass
    
    async def _split_contract_into_sections(self, markdown_content: str) -> List[Dict]:
        """ê³„ì•½ì„œë¥¼ ì¡°í•­ë³„ë¡œ ë¶„í• """
        try:
            logger.info(f"ğŸ“„ ê³„ì•½ì„œ ë¶„í•  ì‹œì‘ (ì´ {len(markdown_content)} ë¬¸ì)")
            
            # ë§ˆí¬ë‹¤ìš´ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
            preview_lines = markdown_content.split('\n')[:10]
            logger.info("ğŸ“‹ ë§ˆí¬ë‹¤ìš´ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:")
            for i, line in enumerate(preview_lines, 1):
                if line.strip():
                    logger.info(f"  {i:2d}: {line[:80]}{'...' if len(line) > 80 else ''}")
            
            sections = []
            
            # ë°©ë²• 1: í—¤ë”©(#, ##) ë° ì¡°í•­ íŒ¨í„´ìœ¼ë¡œ ë¶„í• 
            lines = markdown_content.split('\n')
            current_section = {"header_1": "ì„œë¬¸", "content": ""}
            section_count = 0
            
            import re
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # í—¤ë”© ë˜ëŠ” ì¡°í•­ íŒ¨í„´ ë°œê²¬ ì‹œ ìƒˆë¡œìš´ ì„¹ì…˜ ì‹œì‘
                is_new_section = False
                header_text = ""
                
                if line.startswith('#'):
                    # ë§ˆí¬ë‹¤ìš´ í—¤ë”©
                    is_new_section = True
                    header_text = line.lstrip('#').strip()
                elif re.match(r'^ì œ\s*\d+\s*ì¡°', line):
                    # "ì œXì¡°" ë˜ëŠ” "ì œ X ì¡°" íŒ¨í„´ (ê´„í˜¸ ì—†ì´ë„ ì¸ì‹)
                    is_new_section = True
                    header_text = line
                elif line.startswith('Article') or line.startswith('ARTICLE'):
                    # ì˜ë¬¸ ì¡°í•­
                    is_new_section = True
                    header_text = line
                elif any(line.startswith(pattern) for pattern in ['1.', '2.', '3.', '4.', '5.', '6.', '7.', '8.', '9.']):
                    # ë²ˆí˜¸ ë§¤ê¸´ ì¡°í•­ (1., 2., 3. ë“±)
                    if len(line) > 10 and not current_section["content"]:  # ë„ˆë¬´ ì§§ì§€ ì•Šê³  ìƒˆ ì„¹ì…˜ì¼ ë•Œë§Œ
                        is_new_section = True
                        header_text = line
                
                if is_new_section:
                    # ì´ì „ ì„¹ì…˜ ì €ì¥
                    if current_section["content"].strip():
                        current_section["chunk_type"] = "parent"
                        sections.append(current_section)
                        section_count += 1
                    
                    # ìƒˆ ì„¹ì…˜ ì‹œì‘ - ì›ë³¸ ì¡°í•­ ë²ˆí˜¸ ë³´ì¡´
                    current_section = {
                        "header_1": header_text,  # ì›ë³¸ ì¡°í•­ ë²ˆí˜¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        "content": "",
                        "chunk_type": "parent"
                    }
                else:
                    # ë‚´ìš© ì¶”ê°€
                    if current_section["content"]:
                        current_section["content"] += "\n"
                    current_section["content"] += line
            
            # ë§ˆì§€ë§‰ ì„¹ì…˜ ì €ì¥
            if current_section["content"].strip():
                sections.append(current_section)
                section_count += 1
            
            # í—¤ë”©ì´ ì—†ìœ¼ë©´ ë¬¸ë‹¨ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
            if not sections:
                logger.info("ğŸ“„ í—¤ë”©ì´ ì—†ì–´ ë¬¸ë‹¨ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.")
                paragraphs = [p.strip() for p in markdown_content.split('\n\n') if p.strip()]
                
                for i, paragraph in enumerate(paragraphs[:10], 1):  # ìµœëŒ€ 10ê°œ ë¬¸ë‹¨
                    if len(paragraph) > 50:  # ë„ˆë¬´ ì§§ì€ ë¬¸ë‹¨ ì œì™¸
                        sections.append({
                            "header_1": f"ì¡°í•­ {i}",
                            "content": paragraph,
                            "chunk_type": "parent"
                        })
            
            logger.info(f"âœ… {len(sections)}ê°œ ì¡°í•­ìœ¼ë¡œ ë¶„í•  ì™„ë£Œ")
            
            # ê° ì„¹ì…˜ ë¯¸ë¦¬ë³´ê¸°
            for i, section in enumerate(sections[:3], 1):
                preview = section["content"][:100] + "..." if len(section["content"]) > 100 else section["content"]
                logger.info(f"  {i}. {section['header_1']}: {preview}")
                
            return sections
            
        except Exception as e:
            logger.error(f"âŒ ê³„ì•½ì„œ ë¶„í•  ì‹¤íŒ¨: {str(e)}")
            return []


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Smart CLM RAG íŒŒì´í”„ë¼ì¸")
    parser.add_argument(
        "--step", 
        type=int, 
        choices=[1, 2, 3, 4], 
        default=1,
        help="ì‹¤í–‰í•  ë‹¨ê³„ (1: ê·¼ê±°ìë£Œ ì €ì¥, 2: ê²€ìƒ‰ í…ŒìŠ¤íŠ¸, 3: ê³„ì•½ì„œ ê²€í†  í…ŒìŠ¤íŠ¸, 4: Streamlit ì‹œê°í™”)"
    )
    parser.add_argument(
        "--force", 
        action="store_true",
        help="ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ë„ ê°•ì œë¡œ ë‹¤ì‹œ ì²˜ë¦¬"
    )
    
    args = parser.parse_args()
    
    pipeline = RAGPipeline()
    
    try:
        if args.step == 1:
            success = await pipeline.step1_store_reference_materials(force=args.force)
            if success:
                logger.info("ğŸ‰ Step 1 ì™„ë£Œ!")
            else:
                logger.error("âŒ Step 1 ì‹¤íŒ¨!")
                sys.exit(1)
        
        elif args.step == 2:
            await pipeline.step2_test_search()
        
        elif args.step == 3:
            await pipeline.step3_contract_review_test()
        
        elif args.step == 4:
            await pipeline.step4_streamlit_visualization()
    
    except KeyboardInterrupt:
        logger.info("â¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(0)
    
    except Exception as e:
        logger.error(f"ğŸ’¥ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main()) 